{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "popularity_node_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qblUM5xVnXwI"
      },
      "source": [
        "# Predicting Business Popularity\n",
        "##### Pytorch Geometric implementation with NetworkX graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYqsoeYGnUQ0",
        "outputId": "47c5e232-d427-4a0f-fce4-e21c7bc2e19e"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-ABYVKzIBey"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import networkx as nx\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj04yyTQnyVy",
        "outputId": "d685af7d-7ab1-4f24-8dc6-e0c1d6fac263"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-SbFnpP4UW5"
      },
      "source": [
        "## Creating the network graph datasets\n",
        "\n",
        "We use NetworkX to read and parse the graph into train/validate/test sets. We load these into DataLoader to use mini-batching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqrP8jo8nyeF"
      },
      "source": [
        "G = nx.read_gpickle(\"./drive/MyDrive/Colab Notebooks/restaurants_MA.gpickle\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJdPl2F6Jiz7"
      },
      "source": [
        "H = Graph(G)\n",
        "dataset = GraphDataset(graphs=[H], task='node')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl7KGLQPg5Nz",
        "outputId": "86357ac3-259f-4361-f7e6-096c01685274"
      },
      "source": [
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(), batch_size=16)\n",
        "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(), batch_size=16)\n",
        "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(), batch_size=16)\n",
        "\n",
        "num_node_features = dataset_train.num_node_features\n",
        "num_classes = dataset_train.num_node_labels\n",
        "print(f\"There are {num_node_features} features and {num_classes} labels.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 59 features and 3 labels.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRbKKXJA9Rgu"
      },
      "source": [
        "## Training a Multi-layer Perception Network (MLP)\n",
        "\n",
        "In theory, we should be able to infer the popularity category, without taking any relational information into account.\n",
        "\n",
        "Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCG2l3CeovKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4fb636-d214-4ba1-955c-dd748f5910f2"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(MLP, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = nn.Linear(num_node_features, hidden_channels)\n",
        "        self.lin2 = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=59, out_features=16, bias=True)\n",
            "  (lin2): Linear(in_features=16, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3f2Gdnj_kys"
      },
      "source": [
        "Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity and [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout).\n",
        "Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding (`hidden_channels=16`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes.\n",
        "\n",
        "Let's train our simple MLP by following a similar procedure as described in [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8).\n",
        "We again make use of the **cross entropy loss** and **Adam optimizer**.\n",
        "This time, we also define a **`test` function** to evaluate how well our final model performs on the test node set (which labels have not been observed during training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pzvM8E_O6aW",
        "outputId": "f4bc1f57-fc61-407d-f75b-7f60934bb770"
      },
      "source": [
        "print(dataset.graphs[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph(G=[], edge_index=[2, 81210], edge_label_index=[2, 81210], node_feature=[6192, 59], node_label=[6192], node_label_index=[6192], task=[])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1RXBrSGMRUq",
        "outputId": "b174551d-9e00-4ca7-de99-fd008906249a"
      },
      "source": [
        "x = dataset.graphs[0].node_feature\n",
        "y = dataset.graphs[0].node_label\n",
        "\n",
        "print(x.size())\n",
        "print(y.size())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6192, 59])\n",
            "torch.Size([6192])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49RWYzgAPPNo",
        "outputId": "2f478de9-d42d-4e61-c854-700a534e24dd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "mask = np.random.rand(1, y.size()[0])\n",
        "\n",
        "train_mask = (mask < 0.9)[0]\n",
        "test_mask = (mask >= 0.9)[0]\n",
        "\n",
        "print(train_mask)\n",
        "print(test_mask)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True False  True ...  True  True  True]\n",
            "[False  True False ... False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "qD6iAX_e9bz4",
        "outputId": "a8cff778-0742-4e0a-8d3e-f16eb6d22ce8"
      },
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(x)  # Perform a single forward pass.\n",
        "      loss = criterion(out[train_mask], y[train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(x)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[test_mask] == y[test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 300):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 1.2184\n",
            "Epoch: 002, Loss: 1.1271\n",
            "Epoch: 003, Loss: 1.1017\n",
            "Epoch: 004, Loss: 1.0858\n",
            "Epoch: 005, Loss: 1.0706\n",
            "Epoch: 006, Loss: 1.0646\n",
            "Epoch: 007, Loss: 1.0548\n",
            "Epoch: 008, Loss: 1.0483\n",
            "Epoch: 009, Loss: 1.0416\n",
            "Epoch: 010, Loss: 1.0404\n",
            "Epoch: 011, Loss: 1.0328\n",
            "Epoch: 012, Loss: 1.0328\n",
            "Epoch: 013, Loss: 1.0246\n",
            "Epoch: 014, Loss: 1.0208\n",
            "Epoch: 015, Loss: 1.0127\n",
            "Epoch: 016, Loss: 1.0089\n",
            "Epoch: 017, Loss: 1.0035\n",
            "Epoch: 018, Loss: 0.9968\n",
            "Epoch: 019, Loss: 0.9965\n",
            "Epoch: 020, Loss: 0.9867\n",
            "Epoch: 021, Loss: 0.9880\n",
            "Epoch: 022, Loss: 0.9814\n",
            "Epoch: 023, Loss: 0.9812\n",
            "Epoch: 024, Loss: 0.9698\n",
            "Epoch: 025, Loss: 0.9698\n",
            "Epoch: 026, Loss: 0.9642\n",
            "Epoch: 027, Loss: 0.9612\n",
            "Epoch: 028, Loss: 0.9639\n",
            "Epoch: 029, Loss: 0.9534\n",
            "Epoch: 030, Loss: 0.9516\n",
            "Epoch: 031, Loss: 0.9497\n",
            "Epoch: 032, Loss: 0.9527\n",
            "Epoch: 033, Loss: 0.9506\n",
            "Epoch: 034, Loss: 0.9454\n",
            "Epoch: 035, Loss: 0.9455\n",
            "Epoch: 036, Loss: 0.9475\n",
            "Epoch: 037, Loss: 0.9429\n",
            "Epoch: 038, Loss: 0.9345\n",
            "Epoch: 039, Loss: 0.9432\n",
            "Epoch: 040, Loss: 0.9315\n",
            "Epoch: 041, Loss: 0.9340\n",
            "Epoch: 042, Loss: 0.9320\n",
            "Epoch: 043, Loss: 0.9315\n",
            "Epoch: 044, Loss: 0.9240\n",
            "Epoch: 045, Loss: 0.9303\n",
            "Epoch: 046, Loss: 0.9194\n",
            "Epoch: 047, Loss: 0.9259\n",
            "Epoch: 048, Loss: 0.9241\n",
            "Epoch: 049, Loss: 0.9157\n",
            "Epoch: 050, Loss: 0.9224\n",
            "Epoch: 051, Loss: 0.9218\n",
            "Epoch: 052, Loss: 0.9210\n",
            "Epoch: 053, Loss: 0.9193\n",
            "Epoch: 054, Loss: 0.9156\n",
            "Epoch: 055, Loss: 0.9208\n",
            "Epoch: 056, Loss: 0.9127\n",
            "Epoch: 057, Loss: 0.9140\n",
            "Epoch: 058, Loss: 0.9175\n",
            "Epoch: 059, Loss: 0.9033\n",
            "Epoch: 060, Loss: 0.9108\n",
            "Epoch: 061, Loss: 0.9170\n",
            "Epoch: 062, Loss: 0.9102\n",
            "Epoch: 063, Loss: 0.9134\n",
            "Epoch: 064, Loss: 0.9133\n",
            "Epoch: 065, Loss: 0.9075\n",
            "Epoch: 066, Loss: 0.9072\n",
            "Epoch: 067, Loss: 0.9082\n",
            "Epoch: 068, Loss: 0.9031\n",
            "Epoch: 069, Loss: 0.9072\n",
            "Epoch: 070, Loss: 0.9044\n",
            "Epoch: 071, Loss: 0.9035\n",
            "Epoch: 072, Loss: 0.8967\n",
            "Epoch: 073, Loss: 0.9053\n",
            "Epoch: 074, Loss: 0.9058\n",
            "Epoch: 075, Loss: 0.9060\n",
            "Epoch: 076, Loss: 0.9022\n",
            "Epoch: 077, Loss: 0.8971\n",
            "Epoch: 078, Loss: 0.9054\n",
            "Epoch: 079, Loss: 0.9006\n",
            "Epoch: 080, Loss: 0.8905\n",
            "Epoch: 081, Loss: 0.9005\n",
            "Epoch: 082, Loss: 0.8998\n",
            "Epoch: 083, Loss: 0.8932\n",
            "Epoch: 084, Loss: 0.8922\n",
            "Epoch: 085, Loss: 0.8945\n",
            "Epoch: 086, Loss: 0.8997\n",
            "Epoch: 087, Loss: 0.8978\n",
            "Epoch: 088, Loss: 0.8985\n",
            "Epoch: 089, Loss: 0.8939\n",
            "Epoch: 090, Loss: 0.8913\n",
            "Epoch: 091, Loss: 0.8960\n",
            "Epoch: 092, Loss: 0.8980\n",
            "Epoch: 093, Loss: 0.8962\n",
            "Epoch: 094, Loss: 0.8952\n",
            "Epoch: 095, Loss: 0.8943\n",
            "Epoch: 096, Loss: 0.8954\n",
            "Epoch: 097, Loss: 0.8948\n",
            "Epoch: 098, Loss: 0.8923\n",
            "Epoch: 099, Loss: 0.8849\n",
            "Epoch: 100, Loss: 0.8864\n",
            "Epoch: 101, Loss: 0.8913\n",
            "Epoch: 102, Loss: 0.8887\n",
            "Epoch: 103, Loss: 0.8903\n",
            "Epoch: 104, Loss: 0.8894\n",
            "Epoch: 105, Loss: 0.8945\n",
            "Epoch: 106, Loss: 0.8848\n",
            "Epoch: 107, Loss: 0.8899\n",
            "Epoch: 108, Loss: 0.8927\n",
            "Epoch: 109, Loss: 0.8972\n",
            "Epoch: 110, Loss: 0.8912\n",
            "Epoch: 111, Loss: 0.8910\n",
            "Epoch: 112, Loss: 0.8875\n",
            "Epoch: 113, Loss: 0.8906\n",
            "Epoch: 114, Loss: 0.8903\n",
            "Epoch: 115, Loss: 0.8838\n",
            "Epoch: 116, Loss: 0.8916\n",
            "Epoch: 117, Loss: 0.8934\n",
            "Epoch: 118, Loss: 0.8863\n",
            "Epoch: 119, Loss: 0.8910\n",
            "Epoch: 120, Loss: 0.8853\n",
            "Epoch: 121, Loss: 0.8866\n",
            "Epoch: 122, Loss: 0.8849\n",
            "Epoch: 123, Loss: 0.8809\n",
            "Epoch: 124, Loss: 0.8880\n",
            "Epoch: 125, Loss: 0.8872\n",
            "Epoch: 126, Loss: 0.8894\n",
            "Epoch: 127, Loss: 0.8839\n",
            "Epoch: 128, Loss: 0.8846\n",
            "Epoch: 129, Loss: 0.8873\n",
            "Epoch: 130, Loss: 0.8816\n",
            "Epoch: 131, Loss: 0.8846\n",
            "Epoch: 132, Loss: 0.8935\n",
            "Epoch: 133, Loss: 0.8847\n",
            "Epoch: 134, Loss: 0.8945\n",
            "Epoch: 135, Loss: 0.8842\n",
            "Epoch: 136, Loss: 0.8845\n",
            "Epoch: 137, Loss: 0.8832\n",
            "Epoch: 138, Loss: 0.8849\n",
            "Epoch: 139, Loss: 0.8782\n",
            "Epoch: 140, Loss: 0.8833\n",
            "Epoch: 141, Loss: 0.8814\n",
            "Epoch: 142, Loss: 0.8816\n",
            "Epoch: 143, Loss: 0.8807\n",
            "Epoch: 144, Loss: 0.8829\n",
            "Epoch: 145, Loss: 0.8806\n",
            "Epoch: 146, Loss: 0.8808\n",
            "Epoch: 147, Loss: 0.8798\n",
            "Epoch: 148, Loss: 0.8799\n",
            "Epoch: 149, Loss: 0.8895\n",
            "Epoch: 150, Loss: 0.8878\n",
            "Epoch: 151, Loss: 0.8817\n",
            "Epoch: 152, Loss: 0.8765\n",
            "Epoch: 153, Loss: 0.8788\n",
            "Epoch: 154, Loss: 0.8825\n",
            "Epoch: 155, Loss: 0.8824\n",
            "Epoch: 156, Loss: 0.8779\n",
            "Epoch: 157, Loss: 0.8751\n",
            "Epoch: 158, Loss: 0.8831\n",
            "Epoch: 159, Loss: 0.8858\n",
            "Epoch: 160, Loss: 0.8798\n",
            "Epoch: 161, Loss: 0.8836\n",
            "Epoch: 162, Loss: 0.8841\n",
            "Epoch: 163, Loss: 0.8817\n",
            "Epoch: 164, Loss: 0.8802\n",
            "Epoch: 165, Loss: 0.8745\n",
            "Epoch: 166, Loss: 0.8764\n",
            "Epoch: 167, Loss: 0.8806\n",
            "Epoch: 168, Loss: 0.8815\n",
            "Epoch: 169, Loss: 0.8739\n",
            "Epoch: 170, Loss: 0.8791\n",
            "Epoch: 171, Loss: 0.8748\n",
            "Epoch: 172, Loss: 0.8823\n",
            "Epoch: 173, Loss: 0.8760\n",
            "Epoch: 174, Loss: 0.8802\n",
            "Epoch: 175, Loss: 0.8773\n",
            "Epoch: 176, Loss: 0.8858\n",
            "Epoch: 177, Loss: 0.8832\n",
            "Epoch: 178, Loss: 0.8760\n",
            "Epoch: 179, Loss: 0.8799\n",
            "Epoch: 180, Loss: 0.8794\n",
            "Epoch: 181, Loss: 0.8794\n",
            "Epoch: 182, Loss: 0.8761\n",
            "Epoch: 183, Loss: 0.8755\n",
            "Epoch: 184, Loss: 0.8810\n",
            "Epoch: 185, Loss: 0.8792\n",
            "Epoch: 186, Loss: 0.8804\n",
            "Epoch: 187, Loss: 0.8788\n",
            "Epoch: 188, Loss: 0.8753\n",
            "Epoch: 189, Loss: 0.8796\n",
            "Epoch: 190, Loss: 0.8783\n",
            "Epoch: 191, Loss: 0.8763\n",
            "Epoch: 192, Loss: 0.8727\n",
            "Epoch: 193, Loss: 0.8781\n",
            "Epoch: 194, Loss: 0.8797\n",
            "Epoch: 195, Loss: 0.8746\n",
            "Epoch: 196, Loss: 0.8746\n",
            "Epoch: 197, Loss: 0.8758\n",
            "Epoch: 198, Loss: 0.8782\n",
            "Epoch: 199, Loss: 0.8807\n",
            "Epoch: 200, Loss: 0.8775\n",
            "Epoch: 201, Loss: 0.8734\n",
            "Epoch: 202, Loss: 0.8804\n",
            "Epoch: 203, Loss: 0.8817\n",
            "Epoch: 204, Loss: 0.8762\n",
            "Epoch: 205, Loss: 0.8762\n",
            "Epoch: 206, Loss: 0.8783\n",
            "Epoch: 207, Loss: 0.8731\n",
            "Epoch: 208, Loss: 0.8708\n",
            "Epoch: 209, Loss: 0.8805\n",
            "Epoch: 210, Loss: 0.8794\n",
            "Epoch: 211, Loss: 0.8720\n",
            "Epoch: 212, Loss: 0.8666\n",
            "Epoch: 213, Loss: 0.8752\n",
            "Epoch: 214, Loss: 0.8791\n",
            "Epoch: 215, Loss: 0.8729\n",
            "Epoch: 216, Loss: 0.8764\n",
            "Epoch: 217, Loss: 0.8736\n",
            "Epoch: 218, Loss: 0.8715\n",
            "Epoch: 219, Loss: 0.8747\n",
            "Epoch: 220, Loss: 0.8721\n",
            "Epoch: 221, Loss: 0.8690\n",
            "Epoch: 222, Loss: 0.8737\n",
            "Epoch: 223, Loss: 0.8731\n",
            "Epoch: 224, Loss: 0.8735\n",
            "Epoch: 225, Loss: 0.8657\n",
            "Epoch: 226, Loss: 0.8681\n",
            "Epoch: 227, Loss: 0.8702\n",
            "Epoch: 228, Loss: 0.8708\n",
            "Epoch: 229, Loss: 0.8686\n",
            "Epoch: 230, Loss: 0.8699\n",
            "Epoch: 231, Loss: 0.8747\n",
            "Epoch: 232, Loss: 0.8701\n",
            "Epoch: 233, Loss: 0.8708\n",
            "Epoch: 234, Loss: 0.8671\n",
            "Epoch: 235, Loss: 0.8712\n",
            "Epoch: 236, Loss: 0.8745\n",
            "Epoch: 237, Loss: 0.8716\n",
            "Epoch: 238, Loss: 0.8685\n",
            "Epoch: 239, Loss: 0.8709\n",
            "Epoch: 240, Loss: 0.8709\n",
            "Epoch: 241, Loss: 0.8688\n",
            "Epoch: 242, Loss: 0.8705\n",
            "Epoch: 243, Loss: 0.8751\n",
            "Epoch: 244, Loss: 0.8700\n",
            "Epoch: 245, Loss: 0.8707\n",
            "Epoch: 246, Loss: 0.8684\n",
            "Epoch: 247, Loss: 0.8716\n",
            "Epoch: 248, Loss: 0.8653\n",
            "Epoch: 249, Loss: 0.8693\n",
            "Epoch: 250, Loss: 0.8735\n",
            "Epoch: 251, Loss: 0.8764\n",
            "Epoch: 252, Loss: 0.8718\n",
            "Epoch: 253, Loss: 0.8760\n",
            "Epoch: 254, Loss: 0.8767\n",
            "Epoch: 255, Loss: 0.8725\n",
            "Epoch: 256, Loss: 0.8669\n",
            "Epoch: 257, Loss: 0.8739\n",
            "Epoch: 258, Loss: 0.8734\n",
            "Epoch: 259, Loss: 0.8753\n",
            "Epoch: 260, Loss: 0.8698\n",
            "Epoch: 261, Loss: 0.8655\n",
            "Epoch: 262, Loss: 0.8692\n",
            "Epoch: 263, Loss: 0.8703\n",
            "Epoch: 264, Loss: 0.8674\n",
            "Epoch: 265, Loss: 0.8661\n",
            "Epoch: 266, Loss: 0.8653\n",
            "Epoch: 267, Loss: 0.8685\n",
            "Epoch: 268, Loss: 0.8704\n",
            "Epoch: 269, Loss: 0.8701\n",
            "Epoch: 270, Loss: 0.8701\n",
            "Epoch: 271, Loss: 0.8695\n",
            "Epoch: 272, Loss: 0.8670\n",
            "Epoch: 273, Loss: 0.8705\n",
            "Epoch: 274, Loss: 0.8712\n",
            "Epoch: 275, Loss: 0.8645\n",
            "Epoch: 276, Loss: 0.8668\n",
            "Epoch: 277, Loss: 0.8665\n",
            "Epoch: 278, Loss: 0.8707\n",
            "Epoch: 279, Loss: 0.8676\n",
            "Epoch: 280, Loss: 0.8640\n",
            "Epoch: 281, Loss: 0.8672\n",
            "Epoch: 282, Loss: 0.8761\n",
            "Epoch: 283, Loss: 0.8697\n",
            "Epoch: 284, Loss: 0.8680\n",
            "Epoch: 285, Loss: 0.8629\n",
            "Epoch: 286, Loss: 0.8704\n",
            "Epoch: 287, Loss: 0.8755\n",
            "Epoch: 288, Loss: 0.8618\n",
            "Epoch: 289, Loss: 0.8625\n",
            "Epoch: 290, Loss: 0.8641\n",
            "Epoch: 291, Loss: 0.8666\n",
            "Epoch: 292, Loss: 0.8695\n",
            "Epoch: 293, Loss: 0.8703\n",
            "Epoch: 294, Loss: 0.8625\n",
            "Epoch: 295, Loss: 0.8634\n",
            "Epoch: 296, Loss: 0.8739\n",
            "Epoch: 297, Loss: 0.8686\n",
            "Epoch: 298, Loss: 0.8666\n",
            "Epoch: 299, Loss: 0.8725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsnhGADw_v8N"
      },
      "source": [
        "After training the model, we can call the `test` function to see how well our model performs on unseen labels.\n",
        "Here, we are interested in the accuracy of the model, *i.e.*, the ratio of correctly classified nodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wId7_-jW_ehn",
        "outputId": "88859e20-7578-4f7c-ac19-f48a1b79ffe2"
      },
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yzxxbjcACGM"
      },
      "source": [
        "## Training a Graph Neural Network (GNN)\n",
        "\n",
        "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCLoB6j6_wt8"
      },
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, args):\n",
        "        super(GNN, self).__init__()\n",
        "        self.num_layers = args[\"num_layers\"]\n",
        "\n",
        "        conv_model = self.build_conv_model(args[\"model\"])\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_size, hidden_size))\n",
        "\n",
        "        for l in range(self.num_layers - 1):\n",
        "            self.convs.append(conv_model(hidden_size, hidden_size))\n",
        "        self.post_mp = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.node_feature, data.edge_index, data.batch\n",
        "\n",
        "        for i in range(len(self.convs) - 1):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.leaky_relu(x)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GCN':\n",
        "            return pyg_nn.GCNConv\n",
        "        elif model_type == 'GAT':\n",
        "            return pyg_nn.GATConv\n",
        "        elif model_type == \"GraphSage\":\n",
        "            return pyg_nn.SAGEConv\n",
        "        elif model_type == \"TransformerConv\":\n",
        "            return pyg_nn.TransformerConv\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Model {} unavailable, please add it to GNN.build_conv_model.\".format(model_type))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYPyeWfCCugm"
      },
      "source": [
        "def train_gnn(train_loader, val_loader, test_loader, args, num_node_features, num_classes,\n",
        "          device=\"cpu\"):\n",
        "    model = GNN(num_node_features, args['hidden_size'], num_classes, args).to(device)\n",
        "    print(model)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=5e-4)\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for epoch in range(args['epochs']):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.node_label\n",
        "            loss = model.loss(pred[batch.node_label_index], label)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc = test_gnn(train_loader, model, device)\n",
        "        val_acc = test_gnn(val_loader, model, device)\n",
        "        test_acc = test_gnn(test_loader, model, device)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Train: {train_acc:.4f}, Validation: {val_acc:.4f}. Test: {test_acc:.4f}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        losses.append(round(total_loss, 4))\n",
        "        accuracies.append(test_acc)\n",
        "\n",
        "    return (model, losses, accuracies)\n",
        "\n",
        "def test_gnn(loader, model, device='cuda'):\n",
        "    model.eval()\n",
        "    for batch in loader:\n",
        "        batch.to(device)\n",
        "        logits = model(batch)\n",
        "        pred = logits[batch.node_label_index].max(1)[1]\n",
        "        acc = pred.eq(batch.node_label).sum().item()\n",
        "        total = batch.node_label_index.shape[0]\n",
        "        acc /= total\n",
        "    return acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "iRg7AX6nEjc2",
        "outputId": "7c0b2d96-d58c-4b0c-99cf-3eebcc6f889d"
      },
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 250})'''))\n",
        "\n",
        "args = {\n",
        "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"hidden_size\" : 48,\n",
        "    \"epochs\" : 300,\n",
        "    \"lr\" : 0.001,\n",
        "    \"num_layers\": 2,\n",
        "    \"model\": \"GraphSage\" # [GraphSage, GAT, GCN, TransformerConv]\n",
        "}\n",
        "\n",
        "gnn_model, losses, accuracies = train_gnn(train_loader, val_loader, test_loader, args, num_node_features, num_classes, args[\"device\"])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 250})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "GNN(\n",
            "  (convs): ModuleList(\n",
            "    (0): SAGEConv(59, 48)\n",
            "    (1): SAGEConv(48, 48)\n",
            "  )\n",
            "  (post_mp): Linear(in_features=48, out_features=3, bias=True)\n",
            ")\n",
            "Epoch 1: Train: 0.0143, Validation: 0.0194. Test: 0.0081, Loss: 4.7936\n",
            "Epoch 2: Train: 0.0188, Validation: 0.0226. Test: 0.0177, Loss: 4.6089\n",
            "Epoch 3: Train: 0.0220, Validation: 0.0242. Test: 0.0226, Loss: 4.4292\n",
            "Epoch 4: Train: 0.0267, Validation: 0.0291. Test: 0.0274, Loss: 4.2545\n",
            "Epoch 5: Train: 0.0380, Validation: 0.0372. Test: 0.0339, Loss: 4.0849\n",
            "Epoch 6: Train: 0.0529, Validation: 0.0533. Test: 0.0468, Loss: 3.9201\n",
            "Epoch 7: Train: 0.0715, Validation: 0.0743. Test: 0.0645, Loss: 3.7601\n",
            "Epoch 8: Train: 0.1104, Validation: 0.1099. Test: 0.1032, Loss: 3.6050\n",
            "Epoch 9: Train: 0.1641, Validation: 0.1712. Test: 0.1742, Loss: 3.4552\n",
            "Epoch 10: Train: 0.2374, Validation: 0.2423. Test: 0.2452, Loss: 3.3117\n",
            "Epoch 11: Train: 0.2966, Validation: 0.3037. Test: 0.3161, Loss: 3.1768\n",
            "Epoch 12: Train: 0.3525, Validation: 0.3522. Test: 0.3613, Loss: 3.0530\n",
            "Epoch 13: Train: 0.3559, Validation: 0.3522. Test: 0.3645, Loss: 2.9418\n",
            "Epoch 14: Train: 0.3584, Validation: 0.3570. Test: 0.3758, Loss: 2.8419\n",
            "Epoch 15: Train: 0.3604, Validation: 0.3635. Test: 0.3758, Loss: 2.7500\n",
            "Epoch 16: Train: 0.3612, Validation: 0.3651. Test: 0.3726, Loss: 2.6634\n",
            "Epoch 17: Train: 0.3616, Validation: 0.3667. Test: 0.3774, Loss: 2.5802\n",
            "Epoch 18: Train: 0.3622, Validation: 0.3651. Test: 0.3790, Loss: 2.4992\n",
            "Epoch 19: Train: 0.3642, Validation: 0.3651. Test: 0.3790, Loss: 2.4199\n",
            "Epoch 20: Train: 0.3636, Validation: 0.3667. Test: 0.3790, Loss: 2.3416\n",
            "Epoch 21: Train: 0.3644, Validation: 0.3635. Test: 0.3806, Loss: 2.2643\n",
            "Epoch 22: Train: 0.3656, Validation: 0.3635. Test: 0.3806, Loss: 2.1879\n",
            "Epoch 23: Train: 0.3673, Validation: 0.3619. Test: 0.3806, Loss: 2.1126\n",
            "Epoch 24: Train: 0.3681, Validation: 0.3603. Test: 0.3806, Loss: 2.0387\n",
            "Epoch 25: Train: 0.3709, Validation: 0.3651. Test: 0.3806, Loss: 1.9667\n",
            "Epoch 26: Train: 0.3733, Validation: 0.3651. Test: 0.3806, Loss: 1.8971\n",
            "Epoch 27: Train: 0.3765, Validation: 0.3635. Test: 0.3855, Loss: 1.8304\n",
            "Epoch 28: Train: 0.3763, Validation: 0.3538. Test: 0.3839, Loss: 1.7672\n",
            "Epoch 29: Train: 0.3846, Validation: 0.3651. Test: 0.3871, Loss: 1.7080\n",
            "Epoch 30: Train: 0.3969, Validation: 0.3683. Test: 0.4065, Loss: 1.6531\n",
            "Epoch 31: Train: 0.4048, Validation: 0.3716. Test: 0.4097, Loss: 1.6027\n",
            "Epoch 32: Train: 0.4165, Validation: 0.3861. Test: 0.4177, Loss: 1.5567\n",
            "Epoch 33: Train: 0.4212, Validation: 0.3926. Test: 0.4161, Loss: 1.5148\n",
            "Epoch 34: Train: 0.4304, Validation: 0.3813. Test: 0.4210, Loss: 1.4766\n",
            "Epoch 35: Train: 0.4329, Validation: 0.3748. Test: 0.4258, Loss: 1.4418\n",
            "Epoch 36: Train: 0.4357, Validation: 0.3877. Test: 0.4210, Loss: 1.4102\n",
            "Epoch 37: Train: 0.4379, Validation: 0.3942. Test: 0.4290, Loss: 1.3814\n",
            "Epoch 38: Train: 0.4418, Validation: 0.4039. Test: 0.4323, Loss: 1.3553\n",
            "Epoch 39: Train: 0.4470, Validation: 0.4136. Test: 0.4371, Loss: 1.3319\n",
            "Epoch 40: Train: 0.4520, Validation: 0.4265. Test: 0.4484, Loss: 1.3110\n",
            "Epoch 41: Train: 0.4537, Validation: 0.4297. Test: 0.4548, Loss: 1.2923\n",
            "Epoch 42: Train: 0.4551, Validation: 0.4265. Test: 0.4613, Loss: 1.2757\n",
            "Epoch 43: Train: 0.4553, Validation: 0.4330. Test: 0.4661, Loss: 1.2607\n",
            "Epoch 44: Train: 0.4575, Validation: 0.4346. Test: 0.4694, Loss: 1.2473\n",
            "Epoch 45: Train: 0.4559, Validation: 0.4443. Test: 0.4742, Loss: 1.2351\n",
            "Epoch 46: Train: 0.4549, Validation: 0.4443. Test: 0.4839, Loss: 1.2240\n",
            "Epoch 47: Train: 0.4599, Validation: 0.4313. Test: 0.4823, Loss: 1.2139\n",
            "Epoch 48: Train: 0.4611, Validation: 0.4233. Test: 0.4855, Loss: 1.2045\n",
            "Epoch 49: Train: 0.4656, Validation: 0.4233. Test: 0.4839, Loss: 1.1958\n",
            "Epoch 50: Train: 0.4684, Validation: 0.4281. Test: 0.4774, Loss: 1.1876\n",
            "Epoch 51: Train: 0.4732, Validation: 0.4330. Test: 0.4742, Loss: 1.1799\n",
            "Epoch 52: Train: 0.4716, Validation: 0.4378. Test: 0.4919, Loss: 1.1727\n",
            "Epoch 53: Train: 0.4741, Validation: 0.4394. Test: 0.4903, Loss: 1.1660\n",
            "Epoch 54: Train: 0.4741, Validation: 0.4426. Test: 0.5048, Loss: 1.1599\n",
            "Epoch 55: Train: 0.4749, Validation: 0.4426. Test: 0.5113, Loss: 1.1542\n",
            "Epoch 56: Train: 0.4749, Validation: 0.4394. Test: 0.5081, Loss: 1.1489\n",
            "Epoch 57: Train: 0.4771, Validation: 0.4410. Test: 0.5097, Loss: 1.1439\n",
            "Epoch 58: Train: 0.4791, Validation: 0.4491. Test: 0.5081, Loss: 1.1390\n",
            "Epoch 59: Train: 0.4864, Validation: 0.4540. Test: 0.5129, Loss: 1.1342\n",
            "Epoch 60: Train: 0.4896, Validation: 0.4556. Test: 0.5097, Loss: 1.1295\n",
            "Epoch 61: Train: 0.4912, Validation: 0.4572. Test: 0.5145, Loss: 1.1250\n",
            "Epoch 62: Train: 0.4924, Validation: 0.4604. Test: 0.5161, Loss: 1.1208\n",
            "Epoch 63: Train: 0.4928, Validation: 0.4653. Test: 0.5194, Loss: 1.1168\n",
            "Epoch 64: Train: 0.4949, Validation: 0.4717. Test: 0.5194, Loss: 1.1130\n",
            "Epoch 65: Train: 0.4961, Validation: 0.4717. Test: 0.5242, Loss: 1.1092\n",
            "Epoch 66: Train: 0.4981, Validation: 0.4750. Test: 0.5226, Loss: 1.1054\n",
            "Epoch 67: Train: 0.4981, Validation: 0.4733. Test: 0.5161, Loss: 1.1017\n",
            "Epoch 68: Train: 0.5001, Validation: 0.4733. Test: 0.5161, Loss: 1.0980\n",
            "Epoch 69: Train: 0.4997, Validation: 0.4733. Test: 0.5145, Loss: 1.0944\n",
            "Epoch 70: Train: 0.5015, Validation: 0.4733. Test: 0.5129, Loss: 1.0909\n",
            "Epoch 71: Train: 0.5027, Validation: 0.4733. Test: 0.5145, Loss: 1.0875\n",
            "Epoch 72: Train: 0.5045, Validation: 0.4750. Test: 0.5145, Loss: 1.0842\n",
            "Epoch 73: Train: 0.5054, Validation: 0.4766. Test: 0.5177, Loss: 1.0810\n",
            "Epoch 74: Train: 0.5054, Validation: 0.4782. Test: 0.5210, Loss: 1.0778\n",
            "Epoch 75: Train: 0.5060, Validation: 0.4766. Test: 0.5210, Loss: 1.0747\n",
            "Epoch 76: Train: 0.5070, Validation: 0.4782. Test: 0.5210, Loss: 1.0717\n",
            "Epoch 77: Train: 0.5106, Validation: 0.4830. Test: 0.5226, Loss: 1.0687\n",
            "Epoch 78: Train: 0.5102, Validation: 0.4830. Test: 0.5323, Loss: 1.0659\n",
            "Epoch 79: Train: 0.5124, Validation: 0.4814. Test: 0.5339, Loss: 1.0631\n",
            "Epoch 80: Train: 0.5142, Validation: 0.4814. Test: 0.5339, Loss: 1.0603\n",
            "Epoch 81: Train: 0.5169, Validation: 0.4814. Test: 0.5355, Loss: 1.0576\n",
            "Epoch 82: Train: 0.5181, Validation: 0.4814. Test: 0.5355, Loss: 1.0550\n",
            "Epoch 83: Train: 0.5189, Validation: 0.4830. Test: 0.5323, Loss: 1.0524\n",
            "Epoch 84: Train: 0.5193, Validation: 0.4847. Test: 0.5323, Loss: 1.0498\n",
            "Epoch 85: Train: 0.5197, Validation: 0.4863. Test: 0.5323, Loss: 1.0473\n",
            "Epoch 86: Train: 0.5195, Validation: 0.4847. Test: 0.5339, Loss: 1.0448\n",
            "Epoch 87: Train: 0.5193, Validation: 0.4847. Test: 0.5323, Loss: 1.0424\n",
            "Epoch 88: Train: 0.5207, Validation: 0.4863. Test: 0.5306, Loss: 1.0399\n",
            "Epoch 89: Train: 0.5217, Validation: 0.4863. Test: 0.5355, Loss: 1.0375\n",
            "Epoch 90: Train: 0.5209, Validation: 0.4895. Test: 0.5355, Loss: 1.0352\n",
            "Epoch 91: Train: 0.5207, Validation: 0.4927. Test: 0.5355, Loss: 1.0329\n",
            "Epoch 92: Train: 0.5209, Validation: 0.4943. Test: 0.5371, Loss: 1.0306\n",
            "Epoch 93: Train: 0.5207, Validation: 0.4943. Test: 0.5355, Loss: 1.0283\n",
            "Epoch 94: Train: 0.5221, Validation: 0.4927. Test: 0.5355, Loss: 1.0261\n",
            "Epoch 95: Train: 0.5217, Validation: 0.4927. Test: 0.5371, Loss: 1.0239\n",
            "Epoch 96: Train: 0.5219, Validation: 0.4911. Test: 0.5371, Loss: 1.0218\n",
            "Epoch 97: Train: 0.5223, Validation: 0.4911. Test: 0.5387, Loss: 1.0197\n",
            "Epoch 98: Train: 0.5239, Validation: 0.4911. Test: 0.5403, Loss: 1.0176\n",
            "Epoch 99: Train: 0.5237, Validation: 0.4943. Test: 0.5435, Loss: 1.0155\n",
            "Epoch 100: Train: 0.5243, Validation: 0.4943. Test: 0.5387, Loss: 1.0135\n",
            "Epoch 101: Train: 0.5249, Validation: 0.4927. Test: 0.5387, Loss: 1.0115\n",
            "Epoch 102: Train: 0.5243, Validation: 0.4960. Test: 0.5387, Loss: 1.0096\n",
            "Epoch 103: Train: 0.5247, Validation: 0.4960. Test: 0.5355, Loss: 1.0077\n",
            "Epoch 104: Train: 0.5243, Validation: 0.4976. Test: 0.5355, Loss: 1.0058\n",
            "Epoch 105: Train: 0.5243, Validation: 0.4992. Test: 0.5355, Loss: 1.0039\n",
            "Epoch 106: Train: 0.5249, Validation: 0.4992. Test: 0.5371, Loss: 1.0021\n",
            "Epoch 107: Train: 0.5257, Validation: 0.5008. Test: 0.5403, Loss: 1.0003\n",
            "Epoch 108: Train: 0.5261, Validation: 0.5024. Test: 0.5419, Loss: 0.9985\n",
            "Epoch 109: Train: 0.5274, Validation: 0.5040. Test: 0.5435, Loss: 0.9967\n",
            "Epoch 110: Train: 0.5268, Validation: 0.5040. Test: 0.5387, Loss: 0.9950\n",
            "Epoch 111: Train: 0.5280, Validation: 0.5057. Test: 0.5371, Loss: 0.9933\n",
            "Epoch 112: Train: 0.5276, Validation: 0.5040. Test: 0.5355, Loss: 0.9916\n",
            "Epoch 113: Train: 0.5282, Validation: 0.5073. Test: 0.5306, Loss: 0.9899\n",
            "Epoch 114: Train: 0.5294, Validation: 0.5105. Test: 0.5306, Loss: 0.9883\n",
            "Epoch 115: Train: 0.5302, Validation: 0.5105. Test: 0.5306, Loss: 0.9866\n",
            "Epoch 116: Train: 0.5312, Validation: 0.5105. Test: 0.5306, Loss: 0.9850\n",
            "Epoch 117: Train: 0.5312, Validation: 0.5121. Test: 0.5323, Loss: 0.9835\n",
            "Epoch 118: Train: 0.5334, Validation: 0.5089. Test: 0.5371, Loss: 0.9819\n",
            "Epoch 119: Train: 0.5338, Validation: 0.5105. Test: 0.5387, Loss: 0.9803\n",
            "Epoch 120: Train: 0.5360, Validation: 0.5137. Test: 0.5371, Loss: 0.9788\n",
            "Epoch 121: Train: 0.5364, Validation: 0.5137. Test: 0.5371, Loss: 0.9773\n",
            "Epoch 122: Train: 0.5373, Validation: 0.5202. Test: 0.5371, Loss: 0.9758\n",
            "Epoch 123: Train: 0.5370, Validation: 0.5218. Test: 0.5371, Loss: 0.9743\n",
            "Epoch 124: Train: 0.5383, Validation: 0.5202. Test: 0.5371, Loss: 0.9729\n",
            "Epoch 125: Train: 0.5389, Validation: 0.5202. Test: 0.5355, Loss: 0.9714\n",
            "Epoch 126: Train: 0.5391, Validation: 0.5218. Test: 0.5339, Loss: 0.9700\n",
            "Epoch 127: Train: 0.5387, Validation: 0.5234. Test: 0.5339, Loss: 0.9685\n",
            "Epoch 128: Train: 0.5401, Validation: 0.5186. Test: 0.5355, Loss: 0.9671\n",
            "Epoch 129: Train: 0.5411, Validation: 0.5202. Test: 0.5323, Loss: 0.9656\n",
            "Epoch 130: Train: 0.5415, Validation: 0.5186. Test: 0.5323, Loss: 0.9642\n",
            "Epoch 131: Train: 0.5417, Validation: 0.5170. Test: 0.5339, Loss: 0.9628\n",
            "Epoch 132: Train: 0.5445, Validation: 0.5170. Test: 0.5387, Loss: 0.9614\n",
            "Epoch 133: Train: 0.5482, Validation: 0.5137. Test: 0.5403, Loss: 0.9601\n",
            "Epoch 134: Train: 0.5492, Validation: 0.5202. Test: 0.5435, Loss: 0.9587\n",
            "Epoch 135: Train: 0.5496, Validation: 0.5218. Test: 0.5435, Loss: 0.9575\n",
            "Epoch 136: Train: 0.5504, Validation: 0.5250. Test: 0.5403, Loss: 0.9562\n",
            "Epoch 137: Train: 0.5498, Validation: 0.5267. Test: 0.5419, Loss: 0.9549\n",
            "Epoch 138: Train: 0.5514, Validation: 0.5283. Test: 0.5419, Loss: 0.9536\n",
            "Epoch 139: Train: 0.5522, Validation: 0.5283. Test: 0.5419, Loss: 0.9524\n",
            "Epoch 140: Train: 0.5526, Validation: 0.5283. Test: 0.5435, Loss: 0.9511\n",
            "Epoch 141: Train: 0.5526, Validation: 0.5267. Test: 0.5435, Loss: 0.9499\n",
            "Epoch 142: Train: 0.5552, Validation: 0.5299. Test: 0.5452, Loss: 0.9487\n",
            "Epoch 143: Train: 0.5554, Validation: 0.5299. Test: 0.5452, Loss: 0.9475\n",
            "Epoch 144: Train: 0.5546, Validation: 0.5315. Test: 0.5452, Loss: 0.9463\n",
            "Epoch 145: Train: 0.5552, Validation: 0.5347. Test: 0.5484, Loss: 0.9451\n",
            "Epoch 146: Train: 0.5584, Validation: 0.5331. Test: 0.5484, Loss: 0.9440\n",
            "Epoch 147: Train: 0.5595, Validation: 0.5331. Test: 0.5484, Loss: 0.9428\n",
            "Epoch 148: Train: 0.5595, Validation: 0.5315. Test: 0.5468, Loss: 0.9417\n",
            "Epoch 149: Train: 0.5605, Validation: 0.5347. Test: 0.5468, Loss: 0.9406\n",
            "Epoch 150: Train: 0.5617, Validation: 0.5331. Test: 0.5484, Loss: 0.9395\n",
            "Epoch 151: Train: 0.5627, Validation: 0.5347. Test: 0.5516, Loss: 0.9384\n",
            "Epoch 152: Train: 0.5641, Validation: 0.5347. Test: 0.5516, Loss: 0.9373\n",
            "Epoch 153: Train: 0.5643, Validation: 0.5347. Test: 0.5548, Loss: 0.9363\n",
            "Epoch 154: Train: 0.5665, Validation: 0.5363. Test: 0.5581, Loss: 0.9352\n",
            "Epoch 155: Train: 0.5685, Validation: 0.5363. Test: 0.5613, Loss: 0.9341\n",
            "Epoch 156: Train: 0.5704, Validation: 0.5363. Test: 0.5613, Loss: 0.9331\n",
            "Epoch 157: Train: 0.5710, Validation: 0.5396. Test: 0.5629, Loss: 0.9320\n",
            "Epoch 158: Train: 0.5726, Validation: 0.5444. Test: 0.5629, Loss: 0.9310\n",
            "Epoch 159: Train: 0.5736, Validation: 0.5444. Test: 0.5645, Loss: 0.9299\n",
            "Epoch 160: Train: 0.5738, Validation: 0.5444. Test: 0.5645, Loss: 0.9289\n",
            "Epoch 161: Train: 0.5738, Validation: 0.5460. Test: 0.5613, Loss: 0.9279\n",
            "Epoch 162: Train: 0.5722, Validation: 0.5460. Test: 0.5694, Loss: 0.9270\n",
            "Epoch 163: Train: 0.5722, Validation: 0.5493. Test: 0.5645, Loss: 0.9260\n",
            "Epoch 164: Train: 0.5732, Validation: 0.5525. Test: 0.5661, Loss: 0.9250\n",
            "Epoch 165: Train: 0.5742, Validation: 0.5541. Test: 0.5677, Loss: 0.9241\n",
            "Epoch 166: Train: 0.5742, Validation: 0.5541. Test: 0.5677, Loss: 0.9231\n",
            "Epoch 167: Train: 0.5740, Validation: 0.5541. Test: 0.5677, Loss: 0.9221\n",
            "Epoch 168: Train: 0.5742, Validation: 0.5541. Test: 0.5694, Loss: 0.9212\n",
            "Epoch 169: Train: 0.5742, Validation: 0.5541. Test: 0.5710, Loss: 0.9203\n",
            "Epoch 170: Train: 0.5738, Validation: 0.5525. Test: 0.5677, Loss: 0.9194\n",
            "Epoch 171: Train: 0.5746, Validation: 0.5509. Test: 0.5677, Loss: 0.9185\n",
            "Epoch 172: Train: 0.5750, Validation: 0.5509. Test: 0.5694, Loss: 0.9176\n",
            "Epoch 173: Train: 0.5752, Validation: 0.5509. Test: 0.5710, Loss: 0.9167\n",
            "Epoch 174: Train: 0.5758, Validation: 0.5525. Test: 0.5726, Loss: 0.9158\n",
            "Epoch 175: Train: 0.5760, Validation: 0.5557. Test: 0.5742, Loss: 0.9149\n",
            "Epoch 176: Train: 0.5776, Validation: 0.5590. Test: 0.5758, Loss: 0.9141\n",
            "Epoch 177: Train: 0.5780, Validation: 0.5590. Test: 0.5774, Loss: 0.9132\n",
            "Epoch 178: Train: 0.5780, Validation: 0.5590. Test: 0.5790, Loss: 0.9124\n",
            "Epoch 179: Train: 0.5788, Validation: 0.5606. Test: 0.5806, Loss: 0.9115\n",
            "Epoch 180: Train: 0.5784, Validation: 0.5622. Test: 0.5806, Loss: 0.9107\n",
            "Epoch 181: Train: 0.5788, Validation: 0.5638. Test: 0.5790, Loss: 0.9099\n",
            "Epoch 182: Train: 0.5790, Validation: 0.5622. Test: 0.5790, Loss: 0.9091\n",
            "Epoch 183: Train: 0.5801, Validation: 0.5606. Test: 0.5790, Loss: 0.9083\n",
            "Epoch 184: Train: 0.5801, Validation: 0.5606. Test: 0.5806, Loss: 0.9075\n",
            "Epoch 185: Train: 0.5807, Validation: 0.5606. Test: 0.5839, Loss: 0.9067\n",
            "Epoch 186: Train: 0.5807, Validation: 0.5606. Test: 0.5839, Loss: 0.9059\n",
            "Epoch 187: Train: 0.5811, Validation: 0.5622. Test: 0.5855, Loss: 0.9052\n",
            "Epoch 188: Train: 0.5799, Validation: 0.5606. Test: 0.5903, Loss: 0.9044\n",
            "Epoch 189: Train: 0.5796, Validation: 0.5622. Test: 0.5919, Loss: 0.9036\n",
            "Epoch 190: Train: 0.5799, Validation: 0.5606. Test: 0.5919, Loss: 0.9029\n",
            "Epoch 191: Train: 0.5799, Validation: 0.5606. Test: 0.5935, Loss: 0.9021\n",
            "Epoch 192: Train: 0.5796, Validation: 0.5574. Test: 0.5935, Loss: 0.9014\n",
            "Epoch 193: Train: 0.5799, Validation: 0.5590. Test: 0.5935, Loss: 0.9006\n",
            "Epoch 194: Train: 0.5801, Validation: 0.5606. Test: 0.5935, Loss: 0.8998\n",
            "Epoch 195: Train: 0.5799, Validation: 0.5606. Test: 0.5935, Loss: 0.8991\n",
            "Epoch 196: Train: 0.5803, Validation: 0.5590. Test: 0.5935, Loss: 0.8984\n",
            "Epoch 197: Train: 0.5807, Validation: 0.5590. Test: 0.5935, Loss: 0.8976\n",
            "Epoch 198: Train: 0.5807, Validation: 0.5590. Test: 0.5935, Loss: 0.8969\n",
            "Epoch 199: Train: 0.5807, Validation: 0.5606. Test: 0.5935, Loss: 0.8962\n",
            "Epoch 200: Train: 0.5805, Validation: 0.5606. Test: 0.5935, Loss: 0.8955\n",
            "Epoch 201: Train: 0.5805, Validation: 0.5606. Test: 0.5935, Loss: 0.8948\n",
            "Epoch 202: Train: 0.5811, Validation: 0.5638. Test: 0.5935, Loss: 0.8941\n",
            "Epoch 203: Train: 0.5811, Validation: 0.5638. Test: 0.5952, Loss: 0.8935\n",
            "Epoch 204: Train: 0.5805, Validation: 0.5670. Test: 0.5968, Loss: 0.8928\n",
            "Epoch 205: Train: 0.5807, Validation: 0.5687. Test: 0.5968, Loss: 0.8921\n",
            "Epoch 206: Train: 0.5807, Validation: 0.5687. Test: 0.5968, Loss: 0.8915\n",
            "Epoch 207: Train: 0.5813, Validation: 0.5687. Test: 0.5968, Loss: 0.8908\n",
            "Epoch 208: Train: 0.5815, Validation: 0.5670. Test: 0.5968, Loss: 0.8902\n",
            "Epoch 209: Train: 0.5819, Validation: 0.5654. Test: 0.5968, Loss: 0.8895\n",
            "Epoch 210: Train: 0.5813, Validation: 0.5670. Test: 0.5968, Loss: 0.8889\n",
            "Epoch 211: Train: 0.5813, Validation: 0.5654. Test: 0.5952, Loss: 0.8883\n",
            "Epoch 212: Train: 0.5819, Validation: 0.5670. Test: 0.5935, Loss: 0.8876\n",
            "Epoch 213: Train: 0.5817, Validation: 0.5638. Test: 0.5935, Loss: 0.8870\n",
            "Epoch 214: Train: 0.5815, Validation: 0.5638. Test: 0.5935, Loss: 0.8864\n",
            "Epoch 215: Train: 0.5829, Validation: 0.5654. Test: 0.5935, Loss: 0.8858\n",
            "Epoch 216: Train: 0.5829, Validation: 0.5654. Test: 0.5935, Loss: 0.8852\n",
            "Epoch 217: Train: 0.5825, Validation: 0.5654. Test: 0.5935, Loss: 0.8846\n",
            "Epoch 218: Train: 0.5821, Validation: 0.5654. Test: 0.5935, Loss: 0.8840\n",
            "Epoch 219: Train: 0.5825, Validation: 0.5654. Test: 0.5919, Loss: 0.8834\n",
            "Epoch 220: Train: 0.5827, Validation: 0.5654. Test: 0.5935, Loss: 0.8828\n",
            "Epoch 221: Train: 0.5829, Validation: 0.5654. Test: 0.5952, Loss: 0.8823\n",
            "Epoch 222: Train: 0.5833, Validation: 0.5654. Test: 0.5952, Loss: 0.8817\n",
            "Epoch 223: Train: 0.5829, Validation: 0.5638. Test: 0.5952, Loss: 0.8811\n",
            "Epoch 224: Train: 0.5833, Validation: 0.5622. Test: 0.5952, Loss: 0.8806\n",
            "Epoch 225: Train: 0.5837, Validation: 0.5622. Test: 0.5952, Loss: 0.8800\n",
            "Epoch 226: Train: 0.5839, Validation: 0.5622. Test: 0.5952, Loss: 0.8794\n",
            "Epoch 227: Train: 0.5835, Validation: 0.5638. Test: 0.5952, Loss: 0.8789\n",
            "Epoch 228: Train: 0.5841, Validation: 0.5654. Test: 0.5952, Loss: 0.8783\n",
            "Epoch 229: Train: 0.5845, Validation: 0.5670. Test: 0.5968, Loss: 0.8778\n",
            "Epoch 230: Train: 0.5847, Validation: 0.5654. Test: 0.5968, Loss: 0.8773\n",
            "Epoch 231: Train: 0.5845, Validation: 0.5654. Test: 0.5968, Loss: 0.8767\n",
            "Epoch 232: Train: 0.5849, Validation: 0.5654. Test: 0.5968, Loss: 0.8762\n",
            "Epoch 233: Train: 0.5855, Validation: 0.5654. Test: 0.5968, Loss: 0.8757\n",
            "Epoch 234: Train: 0.5857, Validation: 0.5654. Test: 0.5952, Loss: 0.8752\n",
            "Epoch 235: Train: 0.5857, Validation: 0.5654. Test: 0.5952, Loss: 0.8746\n",
            "Epoch 236: Train: 0.5865, Validation: 0.5654. Test: 0.5952, Loss: 0.8741\n",
            "Epoch 237: Train: 0.5865, Validation: 0.5687. Test: 0.5968, Loss: 0.8736\n",
            "Epoch 238: Train: 0.5859, Validation: 0.5703. Test: 0.5935, Loss: 0.8731\n",
            "Epoch 239: Train: 0.5869, Validation: 0.5703. Test: 0.5919, Loss: 0.8726\n",
            "Epoch 240: Train: 0.5871, Validation: 0.5703. Test: 0.5919, Loss: 0.8721\n",
            "Epoch 241: Train: 0.5873, Validation: 0.5703. Test: 0.5903, Loss: 0.8716\n",
            "Epoch 242: Train: 0.5877, Validation: 0.5719. Test: 0.5903, Loss: 0.8711\n",
            "Epoch 243: Train: 0.5883, Validation: 0.5719. Test: 0.5887, Loss: 0.8707\n",
            "Epoch 244: Train: 0.5881, Validation: 0.5719. Test: 0.5887, Loss: 0.8702\n",
            "Epoch 245: Train: 0.5887, Validation: 0.5719. Test: 0.5903, Loss: 0.8697\n",
            "Epoch 246: Train: 0.5883, Validation: 0.5687. Test: 0.5903, Loss: 0.8692\n",
            "Epoch 247: Train: 0.5893, Validation: 0.5687. Test: 0.5903, Loss: 0.8688\n",
            "Epoch 248: Train: 0.5891, Validation: 0.5703. Test: 0.5919, Loss: 0.8683\n",
            "Epoch 249: Train: 0.5889, Validation: 0.5703. Test: 0.5919, Loss: 0.8678\n",
            "Epoch 250: Train: 0.5891, Validation: 0.5703. Test: 0.5935, Loss: 0.8674\n",
            "Epoch 251: Train: 0.5885, Validation: 0.5703. Test: 0.5935, Loss: 0.8669\n",
            "Epoch 252: Train: 0.5885, Validation: 0.5687. Test: 0.5935, Loss: 0.8665\n",
            "Epoch 253: Train: 0.5885, Validation: 0.5687. Test: 0.5935, Loss: 0.8660\n",
            "Epoch 254: Train: 0.5893, Validation: 0.5670. Test: 0.5952, Loss: 0.8656\n",
            "Epoch 255: Train: 0.5891, Validation: 0.5670. Test: 0.5952, Loss: 0.8651\n",
            "Epoch 256: Train: 0.5897, Validation: 0.5687. Test: 0.5952, Loss: 0.8647\n",
            "Epoch 257: Train: 0.5906, Validation: 0.5687. Test: 0.5952, Loss: 0.8643\n",
            "Epoch 258: Train: 0.5912, Validation: 0.5687. Test: 0.5952, Loss: 0.8638\n",
            "Epoch 259: Train: 0.5916, Validation: 0.5703. Test: 0.5935, Loss: 0.8634\n",
            "Epoch 260: Train: 0.5932, Validation: 0.5687. Test: 0.5952, Loss: 0.8630\n",
            "Epoch 261: Train: 0.5938, Validation: 0.5703. Test: 0.5952, Loss: 0.8626\n",
            "Epoch 262: Train: 0.5940, Validation: 0.5703. Test: 0.5952, Loss: 0.8621\n",
            "Epoch 263: Train: 0.5948, Validation: 0.5687. Test: 0.5952, Loss: 0.8617\n",
            "Epoch 264: Train: 0.5950, Validation: 0.5703. Test: 0.5952, Loss: 0.8613\n",
            "Epoch 265: Train: 0.5944, Validation: 0.5719. Test: 0.5952, Loss: 0.8609\n",
            "Epoch 266: Train: 0.5956, Validation: 0.5719. Test: 0.5952, Loss: 0.8605\n",
            "Epoch 267: Train: 0.5960, Validation: 0.5719. Test: 0.5952, Loss: 0.8601\n",
            "Epoch 268: Train: 0.5956, Validation: 0.5719. Test: 0.5952, Loss: 0.8597\n",
            "Epoch 269: Train: 0.5958, Validation: 0.5719. Test: 0.5952, Loss: 0.8593\n",
            "Epoch 270: Train: 0.5958, Validation: 0.5751. Test: 0.5952, Loss: 0.8589\n",
            "Epoch 271: Train: 0.5962, Validation: 0.5751. Test: 0.5952, Loss: 0.8585\n",
            "Epoch 272: Train: 0.5960, Validation: 0.5751. Test: 0.5952, Loss: 0.8581\n",
            "Epoch 273: Train: 0.5964, Validation: 0.5751. Test: 0.5952, Loss: 0.8577\n",
            "Epoch 274: Train: 0.5966, Validation: 0.5751. Test: 0.5952, Loss: 0.8573\n",
            "Epoch 275: Train: 0.5968, Validation: 0.5751. Test: 0.5952, Loss: 0.8570\n",
            "Epoch 276: Train: 0.5970, Validation: 0.5751. Test: 0.5952, Loss: 0.8566\n",
            "Epoch 277: Train: 0.5978, Validation: 0.5767. Test: 0.5952, Loss: 0.8562\n",
            "Epoch 278: Train: 0.5976, Validation: 0.5767. Test: 0.5952, Loss: 0.8558\n",
            "Epoch 279: Train: 0.5970, Validation: 0.5767. Test: 0.5935, Loss: 0.8555\n",
            "Epoch 280: Train: 0.5970, Validation: 0.5767. Test: 0.5935, Loss: 0.8551\n",
            "Epoch 281: Train: 0.5968, Validation: 0.5751. Test: 0.5952, Loss: 0.8547\n",
            "Epoch 282: Train: 0.5974, Validation: 0.5751. Test: 0.5935, Loss: 0.8544\n",
            "Epoch 283: Train: 0.5982, Validation: 0.5735. Test: 0.5935, Loss: 0.8540\n",
            "Epoch 284: Train: 0.5988, Validation: 0.5735. Test: 0.5952, Loss: 0.8536\n",
            "Epoch 285: Train: 0.5986, Validation: 0.5735. Test: 0.5952, Loss: 0.8533\n",
            "Epoch 286: Train: 0.5988, Validation: 0.5735. Test: 0.5952, Loss: 0.8529\n",
            "Epoch 287: Train: 0.6004, Validation: 0.5751. Test: 0.5952, Loss: 0.8526\n",
            "Epoch 288: Train: 0.6002, Validation: 0.5751. Test: 0.5952, Loss: 0.8522\n",
            "Epoch 289: Train: 0.6004, Validation: 0.5751. Test: 0.5968, Loss: 0.8519\n",
            "Epoch 290: Train: 0.6008, Validation: 0.5751. Test: 0.5968, Loss: 0.8515\n",
            "Epoch 291: Train: 0.6008, Validation: 0.5751. Test: 0.6000, Loss: 0.8512\n",
            "Epoch 292: Train: 0.6004, Validation: 0.5751. Test: 0.6016, Loss: 0.8509\n",
            "Epoch 293: Train: 0.6006, Validation: 0.5751. Test: 0.6016, Loss: 0.8505\n",
            "Epoch 294: Train: 0.6008, Validation: 0.5767. Test: 0.6000, Loss: 0.8502\n",
            "Epoch 295: Train: 0.6006, Validation: 0.5784. Test: 0.6016, Loss: 0.8499\n",
            "Epoch 296: Train: 0.6010, Validation: 0.5751. Test: 0.6016, Loss: 0.8495\n",
            "Epoch 297: Train: 0.6010, Validation: 0.5735. Test: 0.6000, Loss: 0.8492\n",
            "Epoch 298: Train: 0.6008, Validation: 0.5767. Test: 0.6000, Loss: 0.8489\n",
            "Epoch 299: Train: 0.6015, Validation: 0.5751. Test: 0.6000, Loss: 0.8486\n",
            "Epoch 300: Train: 0.6015, Validation: 0.5767. Test: 0.6000, Loss: 0.8482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5_KxhsRhMV"
      },
      "source": [
        "### Plotting training data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "aDMQdzuHRYB2",
        "outputId": "ab6bce2d-b380-4d16-8902-8cd020f89039"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training Loss\")\n",
        "plt.plot(losses,label=\"Training Loss\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "1pDjO9lYRzty",
        "outputId": "13dbd9bb-e6cb-46ed-a702-20bdae9f85b0"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Testing Accuracy during Training\")\n",
        "plt.plot(accuracies,label=\"Testing Accuracy\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xe9DLCyR88_"
      },
      "source": [
        "### Calculate F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl8NPHT0R30i"
      },
      "source": [
        "def accuracy(pred, target):\n",
        "    # Computes the accuracy of predictions.\n",
        "    return round((pred == target).sum().item() / target.numel(), 4)\n",
        "\n",
        "def true_positive(pred, target, num_classes):\n",
        "    # Computes the number of true positive predictions.\n",
        "    out = []\n",
        "    for i in range(num_classes):\n",
        "        out.append(((pred == i) & (target == i)).sum())\n",
        "\n",
        "    return torch.tensor(out)\n",
        "\n",
        "def true_negative(pred, target, num_classes):\n",
        "    # Computes the number of true negative predictions.\n",
        "    out = []\n",
        "    for i in range(num_classes):\n",
        "        out.append(((pred != i) & (target != i)).sum())\n",
        "\n",
        "    return torch.tensor(out)\n",
        "\n",
        "def false_positive(pred, target, num_classes):\n",
        "    # Computes the number of false positive predictions.\n",
        "    out = []\n",
        "    for i in range(num_classes):\n",
        "        out.append(((pred == i) & (target != i)).sum())\n",
        "\n",
        "    return torch.tensor(out)\n",
        "\n",
        "def false_negative(pred, target, num_classes):\n",
        "    # Computes the number of false negative predictions.\n",
        "    out = []\n",
        "    for i in range(num_classes):\n",
        "        out.append(((pred != i) & (target == i)).sum())\n",
        "\n",
        "    return torch.tensor(out)\n",
        "\n",
        "def precision(pred, target, num_classes):\n",
        "    # Computes the precision\n",
        "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
        "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
        "\n",
        "    out = tp / (tp + fp)\n",
        "    out[torch.isnan(out)] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "def recall(pred, target, num_classes):\n",
        "    # Computes the recall\n",
        "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
        "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
        "\n",
        "    out = tp / (tp + fn)\n",
        "    out[torch.isnan(out)] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "def f1_score(pred, target, num_classes):\n",
        "    # Computes the F1 score\n",
        "    prec = precision(pred, target, num_classes)\n",
        "    rec = recall(pred, target, num_classes)\n",
        "\n",
        "    score = 2 * (prec * rec) / (prec + rec)\n",
        "    score[torch.isnan(score)] = 0\n",
        "\n",
        "    return score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWDSnSTjR35X",
        "outputId": "87d5116f-9005-470e-cbec-5d094ce69b2f"
      },
      "source": [
        "final_test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(), batch_size=len(dataset_test))\n",
        "\n",
        "gnn_model.eval()\n",
        "for batch in final_test_loader:\n",
        "    batch.to(\"cuda\")\n",
        "    logits = gnn_model(batch)\n",
        "    pred = logits[batch.node_label_index].max(1)[1]\n",
        "    \n",
        "    f1 = f1_score(pred, batch.node_label, num_classes)\n",
        "    print(f1)\n",
        "\n",
        "    testing_accuracy = accuracy(pred, batch.node_label)\n",
        "    print(f\"Accuracy on the whole testing set is: {testing_accuracy}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.6722, 0.4350, 0.6719])\n",
            "Accuracy on the whole testing set is: 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}