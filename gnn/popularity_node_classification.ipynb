{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "popularity_node_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qblUM5xVnXwI"
      },
      "source": [
        "# Predicting Business Popularity\n",
        "##### Pytorch Geometric implementation with NetworkX graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYqsoeYGnUQ0",
        "outputId": "16b4493d-8b25-4359-a176-56430f5d61f3"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 26.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 28.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-ABYVKzIBey"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import networkx as nx\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj04yyTQnyVy",
        "outputId": "80dbfc79-4313-48f7-bd9d-0d01175f3956"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-SbFnpP4UW5"
      },
      "source": [
        "## Creating the network graph datasets\n",
        "\n",
        "We use NetworkX to read and parse the graph into train/validate/test sets. We load these into DataLoader to use mini-batching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqrP8jo8nyeF"
      },
      "source": [
        "G = nx.read_gpickle(\"./drive/MyDrive/Colab Notebooks/restaurants_MA.gpickle\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DejVQEGxphK3",
        "outputId": "715bd8d2-58d3-489c-89ea-65b032eace3a"
      },
      "source": [
        "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
        "print(f\"Number of edges: {G.number_of_edges()}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of nodes: 6192\n",
            "Number of edges: 40605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJdPl2F6Jiz7"
      },
      "source": [
        "H = Graph(G)\n",
        "dataset = GraphDataset(graphs=[H], task='node')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pzvM8E_O6aW",
        "outputId": "d8da8174-db09-4893-9688-2ccd3cc89b90"
      },
      "source": [
        "print(dataset.graphs[0].node_feature)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.0000,  1.0000,  1.0000,  ...,  0.0000,  9.0000,  1.0000],\n",
            "        [ 1.0000,  1.0000,  1.0000,  ...,  0.0000, 16.0000,  0.8670],\n",
            "        [ 2.0000,  1.0000,  1.0000,  ...,  0.0000,  3.0000,  1.0000],\n",
            "        ...,\n",
            "        [ 2.0000,  1.0000,  1.0000,  ...,  0.0000, 12.0000,  0.9700],\n",
            "        [ 4.0000,  1.0000,  1.0000,  ...,  0.0000, 32.0000,  0.6390],\n",
            "        [ 2.0000,  1.0000,  1.0000,  ...,  0.0000,  4.0000,  1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl7KGLQPg5Nz",
        "outputId": "5065dbe0-53ff-4445-cef5-8acb299421c5"
      },
      "source": [
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(), batch_size=16)\n",
        "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(), batch_size=16)\n",
        "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(), batch_size=16)\n",
        "\n",
        "num_node_features = dataset_train.num_node_features\n",
        "num_classes = dataset_train.num_node_labels\n",
        "print(f\"There are {num_node_features} features and {num_classes} labels.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 59 features and 3 labels.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRbKKXJA9Rgu"
      },
      "source": [
        "## Training a Multi-layer Perception Network (MLP)\n",
        "\n",
        "In theory, we should be able to infer the popularity category, without taking any relational information into account.\n",
        "\n",
        "Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCG2l3CeovKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e35fb2f-4e62-4b61-a175-2939d8928412"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(MLP, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = nn.Linear(num_node_features, hidden_channels)\n",
        "        self.lin2 = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "print(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=59, out_features=16, bias=True)\n",
            "  (lin2): Linear(in_features=16, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3f2Gdnj_kys"
      },
      "source": [
        "Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity and [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout).\n",
        "Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding (`hidden_channels=16`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes.\n",
        "\n",
        "Let's train our simple MLP by following a similar procedure as described in [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8).\n",
        "We again make use of the **cross entropy loss** and **Adam optimizer**.\n",
        "This time, we also define a **`test` function** to evaluate how well our final model performs on the test node set (which labels have not been observed during training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1RXBrSGMRUq",
        "outputId": "7c1455b5-ff60-445c-800b-2313ce806c27"
      },
      "source": [
        "x = dataset.graphs[0].node_feature\n",
        "y = dataset.graphs[0].node_label\n",
        "\n",
        "print(x.size())\n",
        "print(y.size())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6192, 59])\n",
            "torch.Size([6192])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49RWYzgAPPNo",
        "outputId": "9381123c-c960-47ec-d63e-f533af890847"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "mask = np.random.rand(1, y.size()[0])\n",
        "\n",
        "train_mask = (mask < 0.9)[0]\n",
        "test_mask = (mask >= 0.9)[0]\n",
        "\n",
        "print(train_mask)\n",
        "print(test_mask)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False  True  True ...  True  True  True]\n",
            "[ True False False ... False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "qD6iAX_e9bz4",
        "outputId": "53c4a947-582f-4039-e810-d9e14ab0c00d"
      },
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(x)  # Perform a single forward pass.\n",
        "      loss = criterion(out[train_mask], y[train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(x)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[test_mask] == y[test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 500):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 1.2156\n",
            "Epoch: 002, Loss: 1.1261\n",
            "Epoch: 003, Loss: 1.0973\n",
            "Epoch: 004, Loss: 1.0853\n",
            "Epoch: 005, Loss: 1.0721\n",
            "Epoch: 006, Loss: 1.0678\n",
            "Epoch: 007, Loss: 1.0523\n",
            "Epoch: 008, Loss: 1.0483\n",
            "Epoch: 009, Loss: 1.0417\n",
            "Epoch: 010, Loss: 1.0384\n",
            "Epoch: 011, Loss: 1.0327\n",
            "Epoch: 012, Loss: 1.0325\n",
            "Epoch: 013, Loss: 1.0244\n",
            "Epoch: 014, Loss: 1.0203\n",
            "Epoch: 015, Loss: 1.0110\n",
            "Epoch: 016, Loss: 1.0077\n",
            "Epoch: 017, Loss: 1.0037\n",
            "Epoch: 018, Loss: 0.9966\n",
            "Epoch: 019, Loss: 0.9950\n",
            "Epoch: 020, Loss: 0.9855\n",
            "Epoch: 021, Loss: 0.9852\n",
            "Epoch: 022, Loss: 0.9781\n",
            "Epoch: 023, Loss: 0.9811\n",
            "Epoch: 024, Loss: 0.9701\n",
            "Epoch: 025, Loss: 0.9681\n",
            "Epoch: 026, Loss: 0.9655\n",
            "Epoch: 027, Loss: 0.9627\n",
            "Epoch: 028, Loss: 0.9616\n",
            "Epoch: 029, Loss: 0.9554\n",
            "Epoch: 030, Loss: 0.9508\n",
            "Epoch: 031, Loss: 0.9516\n",
            "Epoch: 032, Loss: 0.9550\n",
            "Epoch: 033, Loss: 0.9541\n",
            "Epoch: 034, Loss: 0.9452\n",
            "Epoch: 035, Loss: 0.9491\n",
            "Epoch: 036, Loss: 0.9475\n",
            "Epoch: 037, Loss: 0.9447\n",
            "Epoch: 038, Loss: 0.9394\n",
            "Epoch: 039, Loss: 0.9449\n",
            "Epoch: 040, Loss: 0.9325\n",
            "Epoch: 041, Loss: 0.9341\n",
            "Epoch: 042, Loss: 0.9365\n",
            "Epoch: 043, Loss: 0.9335\n",
            "Epoch: 044, Loss: 0.9274\n",
            "Epoch: 045, Loss: 0.9314\n",
            "Epoch: 046, Loss: 0.9233\n",
            "Epoch: 047, Loss: 0.9257\n",
            "Epoch: 048, Loss: 0.9245\n",
            "Epoch: 049, Loss: 0.9190\n",
            "Epoch: 050, Loss: 0.9247\n",
            "Epoch: 051, Loss: 0.9264\n",
            "Epoch: 052, Loss: 0.9237\n",
            "Epoch: 053, Loss: 0.9220\n",
            "Epoch: 054, Loss: 0.9175\n",
            "Epoch: 055, Loss: 0.9200\n",
            "Epoch: 056, Loss: 0.9167\n",
            "Epoch: 057, Loss: 0.9148\n",
            "Epoch: 058, Loss: 0.9192\n",
            "Epoch: 059, Loss: 0.9077\n",
            "Epoch: 060, Loss: 0.9135\n",
            "Epoch: 061, Loss: 0.9195\n",
            "Epoch: 062, Loss: 0.9132\n",
            "Epoch: 063, Loss: 0.9168\n",
            "Epoch: 064, Loss: 0.9146\n",
            "Epoch: 065, Loss: 0.9096\n",
            "Epoch: 066, Loss: 0.9109\n",
            "Epoch: 067, Loss: 0.9094\n",
            "Epoch: 068, Loss: 0.9067\n",
            "Epoch: 069, Loss: 0.9110\n",
            "Epoch: 070, Loss: 0.9090\n",
            "Epoch: 071, Loss: 0.9055\n",
            "Epoch: 072, Loss: 0.8997\n",
            "Epoch: 073, Loss: 0.9112\n",
            "Epoch: 074, Loss: 0.9094\n",
            "Epoch: 075, Loss: 0.9091\n",
            "Epoch: 076, Loss: 0.9024\n",
            "Epoch: 077, Loss: 0.9005\n",
            "Epoch: 078, Loss: 0.9014\n",
            "Epoch: 079, Loss: 0.9002\n",
            "Epoch: 080, Loss: 0.8971\n",
            "Epoch: 081, Loss: 0.9028\n",
            "Epoch: 082, Loss: 0.9041\n",
            "Epoch: 083, Loss: 0.8962\n",
            "Epoch: 084, Loss: 0.8950\n",
            "Epoch: 085, Loss: 0.8974\n",
            "Epoch: 086, Loss: 0.9026\n",
            "Epoch: 087, Loss: 0.8994\n",
            "Epoch: 088, Loss: 0.9014\n",
            "Epoch: 089, Loss: 0.8964\n",
            "Epoch: 090, Loss: 0.8930\n",
            "Epoch: 091, Loss: 0.9010\n",
            "Epoch: 092, Loss: 0.8966\n",
            "Epoch: 093, Loss: 0.8979\n",
            "Epoch: 094, Loss: 0.8981\n",
            "Epoch: 095, Loss: 0.8974\n",
            "Epoch: 096, Loss: 0.8957\n",
            "Epoch: 097, Loss: 0.8982\n",
            "Epoch: 098, Loss: 0.8943\n",
            "Epoch: 099, Loss: 0.8857\n",
            "Epoch: 100, Loss: 0.8915\n",
            "Epoch: 101, Loss: 0.8960\n",
            "Epoch: 102, Loss: 0.8890\n",
            "Epoch: 103, Loss: 0.8897\n",
            "Epoch: 104, Loss: 0.8927\n",
            "Epoch: 105, Loss: 0.8987\n",
            "Epoch: 106, Loss: 0.8864\n",
            "Epoch: 107, Loss: 0.8913\n",
            "Epoch: 108, Loss: 0.8950\n",
            "Epoch: 109, Loss: 0.8995\n",
            "Epoch: 110, Loss: 0.8959\n",
            "Epoch: 111, Loss: 0.8936\n",
            "Epoch: 112, Loss: 0.8882\n",
            "Epoch: 113, Loss: 0.8937\n",
            "Epoch: 114, Loss: 0.8910\n",
            "Epoch: 115, Loss: 0.8870\n",
            "Epoch: 116, Loss: 0.8908\n",
            "Epoch: 117, Loss: 0.8972\n",
            "Epoch: 118, Loss: 0.8879\n",
            "Epoch: 119, Loss: 0.8943\n",
            "Epoch: 120, Loss: 0.8889\n",
            "Epoch: 121, Loss: 0.8913\n",
            "Epoch: 122, Loss: 0.8852\n",
            "Epoch: 123, Loss: 0.8850\n",
            "Epoch: 124, Loss: 0.8905\n",
            "Epoch: 125, Loss: 0.8907\n",
            "Epoch: 126, Loss: 0.8920\n",
            "Epoch: 127, Loss: 0.8870\n",
            "Epoch: 128, Loss: 0.8871\n",
            "Epoch: 129, Loss: 0.8903\n",
            "Epoch: 130, Loss: 0.8858\n",
            "Epoch: 131, Loss: 0.8878\n",
            "Epoch: 132, Loss: 0.8969\n",
            "Epoch: 133, Loss: 0.8905\n",
            "Epoch: 134, Loss: 0.8972\n",
            "Epoch: 135, Loss: 0.8858\n",
            "Epoch: 136, Loss: 0.8847\n",
            "Epoch: 137, Loss: 0.8861\n",
            "Epoch: 138, Loss: 0.8879\n",
            "Epoch: 139, Loss: 0.8810\n",
            "Epoch: 140, Loss: 0.8844\n",
            "Epoch: 141, Loss: 0.8874\n",
            "Epoch: 142, Loss: 0.8844\n",
            "Epoch: 143, Loss: 0.8843\n",
            "Epoch: 144, Loss: 0.8838\n",
            "Epoch: 145, Loss: 0.8838\n",
            "Epoch: 146, Loss: 0.8849\n",
            "Epoch: 147, Loss: 0.8813\n",
            "Epoch: 148, Loss: 0.8807\n",
            "Epoch: 149, Loss: 0.8915\n",
            "Epoch: 150, Loss: 0.8885\n",
            "Epoch: 151, Loss: 0.8873\n",
            "Epoch: 152, Loss: 0.8800\n",
            "Epoch: 153, Loss: 0.8799\n",
            "Epoch: 154, Loss: 0.8866\n",
            "Epoch: 155, Loss: 0.8887\n",
            "Epoch: 156, Loss: 0.8830\n",
            "Epoch: 157, Loss: 0.8797\n",
            "Epoch: 158, Loss: 0.8851\n",
            "Epoch: 159, Loss: 0.8893\n",
            "Epoch: 160, Loss: 0.8845\n",
            "Epoch: 161, Loss: 0.8858\n",
            "Epoch: 162, Loss: 0.8921\n",
            "Epoch: 163, Loss: 0.8852\n",
            "Epoch: 164, Loss: 0.8871\n",
            "Epoch: 165, Loss: 0.8786\n",
            "Epoch: 166, Loss: 0.8768\n",
            "Epoch: 167, Loss: 0.8843\n",
            "Epoch: 168, Loss: 0.8847\n",
            "Epoch: 169, Loss: 0.8769\n",
            "Epoch: 170, Loss: 0.8829\n",
            "Epoch: 171, Loss: 0.8794\n",
            "Epoch: 172, Loss: 0.8853\n",
            "Epoch: 173, Loss: 0.8821\n",
            "Epoch: 174, Loss: 0.8858\n",
            "Epoch: 175, Loss: 0.8788\n",
            "Epoch: 176, Loss: 0.8888\n",
            "Epoch: 177, Loss: 0.8855\n",
            "Epoch: 178, Loss: 0.8797\n",
            "Epoch: 179, Loss: 0.8851\n",
            "Epoch: 180, Loss: 0.8839\n",
            "Epoch: 181, Loss: 0.8819\n",
            "Epoch: 182, Loss: 0.8819\n",
            "Epoch: 183, Loss: 0.8817\n",
            "Epoch: 184, Loss: 0.8810\n",
            "Epoch: 185, Loss: 0.8862\n",
            "Epoch: 186, Loss: 0.8891\n",
            "Epoch: 187, Loss: 0.8812\n",
            "Epoch: 188, Loss: 0.8813\n",
            "Epoch: 189, Loss: 0.8807\n",
            "Epoch: 190, Loss: 0.8797\n",
            "Epoch: 191, Loss: 0.8806\n",
            "Epoch: 192, Loss: 0.8791\n",
            "Epoch: 193, Loss: 0.8856\n",
            "Epoch: 194, Loss: 0.8791\n",
            "Epoch: 195, Loss: 0.8760\n",
            "Epoch: 196, Loss: 0.8774\n",
            "Epoch: 197, Loss: 0.8801\n",
            "Epoch: 198, Loss: 0.8825\n",
            "Epoch: 199, Loss: 0.8910\n",
            "Epoch: 200, Loss: 0.8777\n",
            "Epoch: 201, Loss: 0.8747\n",
            "Epoch: 202, Loss: 0.8859\n",
            "Epoch: 203, Loss: 0.8862\n",
            "Epoch: 204, Loss: 0.8843\n",
            "Epoch: 205, Loss: 0.8832\n",
            "Epoch: 206, Loss: 0.8828\n",
            "Epoch: 207, Loss: 0.8851\n",
            "Epoch: 208, Loss: 0.8782\n",
            "Epoch: 209, Loss: 0.8859\n",
            "Epoch: 210, Loss: 0.8836\n",
            "Epoch: 211, Loss: 0.8773\n",
            "Epoch: 212, Loss: 0.8717\n",
            "Epoch: 213, Loss: 0.8811\n",
            "Epoch: 214, Loss: 0.8847\n",
            "Epoch: 215, Loss: 0.8759\n",
            "Epoch: 216, Loss: 0.8780\n",
            "Epoch: 217, Loss: 0.8780\n",
            "Epoch: 218, Loss: 0.8825\n",
            "Epoch: 219, Loss: 0.8791\n",
            "Epoch: 220, Loss: 0.8792\n",
            "Epoch: 221, Loss: 0.8722\n",
            "Epoch: 222, Loss: 0.8800\n",
            "Epoch: 223, Loss: 0.8775\n",
            "Epoch: 224, Loss: 0.8784\n",
            "Epoch: 225, Loss: 0.8722\n",
            "Epoch: 226, Loss: 0.8764\n",
            "Epoch: 227, Loss: 0.8730\n",
            "Epoch: 228, Loss: 0.8754\n",
            "Epoch: 229, Loss: 0.8740\n",
            "Epoch: 230, Loss: 0.8772\n",
            "Epoch: 231, Loss: 0.8806\n",
            "Epoch: 232, Loss: 0.8738\n",
            "Epoch: 233, Loss: 0.8737\n",
            "Epoch: 234, Loss: 0.8695\n",
            "Epoch: 235, Loss: 0.8775\n",
            "Epoch: 236, Loss: 0.8755\n",
            "Epoch: 237, Loss: 0.8783\n",
            "Epoch: 238, Loss: 0.8765\n",
            "Epoch: 239, Loss: 0.8766\n",
            "Epoch: 240, Loss: 0.8760\n",
            "Epoch: 241, Loss: 0.8725\n",
            "Epoch: 242, Loss: 0.8733\n",
            "Epoch: 243, Loss: 0.8784\n",
            "Epoch: 244, Loss: 0.8772\n",
            "Epoch: 245, Loss: 0.8748\n",
            "Epoch: 246, Loss: 0.8699\n",
            "Epoch: 247, Loss: 0.8737\n",
            "Epoch: 248, Loss: 0.8685\n",
            "Epoch: 249, Loss: 0.8706\n",
            "Epoch: 250, Loss: 0.8749\n",
            "Epoch: 251, Loss: 0.8777\n",
            "Epoch: 252, Loss: 0.8723\n",
            "Epoch: 253, Loss: 0.8792\n",
            "Epoch: 254, Loss: 0.8831\n",
            "Epoch: 255, Loss: 0.8769\n",
            "Epoch: 256, Loss: 0.8796\n",
            "Epoch: 257, Loss: 0.8791\n",
            "Epoch: 258, Loss: 0.8773\n",
            "Epoch: 259, Loss: 0.8780\n",
            "Epoch: 260, Loss: 0.8722\n",
            "Epoch: 261, Loss: 0.8739\n",
            "Epoch: 262, Loss: 0.8748\n",
            "Epoch: 263, Loss: 0.8731\n",
            "Epoch: 264, Loss: 0.8727\n",
            "Epoch: 265, Loss: 0.8697\n",
            "Epoch: 266, Loss: 0.8728\n",
            "Epoch: 267, Loss: 0.8740\n",
            "Epoch: 268, Loss: 0.8727\n",
            "Epoch: 269, Loss: 0.8752\n",
            "Epoch: 270, Loss: 0.8765\n",
            "Epoch: 271, Loss: 0.8729\n",
            "Epoch: 272, Loss: 0.8702\n",
            "Epoch: 273, Loss: 0.8754\n",
            "Epoch: 274, Loss: 0.8752\n",
            "Epoch: 275, Loss: 0.8682\n",
            "Epoch: 276, Loss: 0.8703\n",
            "Epoch: 277, Loss: 0.8703\n",
            "Epoch: 278, Loss: 0.8739\n",
            "Epoch: 279, Loss: 0.8707\n",
            "Epoch: 280, Loss: 0.8746\n",
            "Epoch: 281, Loss: 0.8662\n",
            "Epoch: 282, Loss: 0.8757\n",
            "Epoch: 283, Loss: 0.8727\n",
            "Epoch: 284, Loss: 0.8724\n",
            "Epoch: 285, Loss: 0.8666\n",
            "Epoch: 286, Loss: 0.8723\n",
            "Epoch: 287, Loss: 0.8786\n",
            "Epoch: 288, Loss: 0.8640\n",
            "Epoch: 289, Loss: 0.8714\n",
            "Epoch: 290, Loss: 0.8682\n",
            "Epoch: 291, Loss: 0.8737\n",
            "Epoch: 292, Loss: 0.8736\n",
            "Epoch: 293, Loss: 0.8727\n",
            "Epoch: 294, Loss: 0.8665\n",
            "Epoch: 295, Loss: 0.8694\n",
            "Epoch: 296, Loss: 0.8747\n",
            "Epoch: 297, Loss: 0.8681\n",
            "Epoch: 298, Loss: 0.8708\n",
            "Epoch: 299, Loss: 0.8762\n",
            "Epoch: 300, Loss: 0.8753\n",
            "Epoch: 301, Loss: 0.8640\n",
            "Epoch: 302, Loss: 0.8788\n",
            "Epoch: 303, Loss: 0.8770\n",
            "Epoch: 304, Loss: 0.8657\n",
            "Epoch: 305, Loss: 0.8733\n",
            "Epoch: 306, Loss: 0.8709\n",
            "Epoch: 307, Loss: 0.8707\n",
            "Epoch: 308, Loss: 0.8668\n",
            "Epoch: 309, Loss: 0.8707\n",
            "Epoch: 310, Loss: 0.8682\n",
            "Epoch: 311, Loss: 0.8656\n",
            "Epoch: 312, Loss: 0.8746\n",
            "Epoch: 313, Loss: 0.8732\n",
            "Epoch: 314, Loss: 0.8663\n",
            "Epoch: 315, Loss: 0.8670\n",
            "Epoch: 316, Loss: 0.8690\n",
            "Epoch: 317, Loss: 0.8756\n",
            "Epoch: 318, Loss: 0.8693\n",
            "Epoch: 319, Loss: 0.8713\n",
            "Epoch: 320, Loss: 0.8681\n",
            "Epoch: 321, Loss: 0.8741\n",
            "Epoch: 322, Loss: 0.8622\n",
            "Epoch: 323, Loss: 0.8664\n",
            "Epoch: 324, Loss: 0.8701\n",
            "Epoch: 325, Loss: 0.8723\n",
            "Epoch: 326, Loss: 0.8723\n",
            "Epoch: 327, Loss: 0.8636\n",
            "Epoch: 328, Loss: 0.8690\n",
            "Epoch: 329, Loss: 0.8713\n",
            "Epoch: 330, Loss: 0.8661\n",
            "Epoch: 331, Loss: 0.8680\n",
            "Epoch: 332, Loss: 0.8648\n",
            "Epoch: 333, Loss: 0.8649\n",
            "Epoch: 334, Loss: 0.8721\n",
            "Epoch: 335, Loss: 0.8711\n",
            "Epoch: 336, Loss: 0.8709\n",
            "Epoch: 337, Loss: 0.8698\n",
            "Epoch: 338, Loss: 0.8723\n",
            "Epoch: 339, Loss: 0.8652\n",
            "Epoch: 340, Loss: 0.8704\n",
            "Epoch: 341, Loss: 0.8624\n",
            "Epoch: 342, Loss: 0.8710\n",
            "Epoch: 343, Loss: 0.8754\n",
            "Epoch: 344, Loss: 0.8631\n",
            "Epoch: 345, Loss: 0.8681\n",
            "Epoch: 346, Loss: 0.8684\n",
            "Epoch: 347, Loss: 0.8704\n",
            "Epoch: 348, Loss: 0.8689\n",
            "Epoch: 349, Loss: 0.8727\n",
            "Epoch: 350, Loss: 0.8648\n",
            "Epoch: 351, Loss: 0.8690\n",
            "Epoch: 352, Loss: 0.8683\n",
            "Epoch: 353, Loss: 0.8659\n",
            "Epoch: 354, Loss: 0.8683\n",
            "Epoch: 355, Loss: 0.8710\n",
            "Epoch: 356, Loss: 0.8599\n",
            "Epoch: 357, Loss: 0.8791\n",
            "Epoch: 358, Loss: 0.8753\n",
            "Epoch: 359, Loss: 0.8699\n",
            "Epoch: 360, Loss: 0.8665\n",
            "Epoch: 361, Loss: 0.8721\n",
            "Epoch: 362, Loss: 0.8693\n",
            "Epoch: 363, Loss: 0.8599\n",
            "Epoch: 364, Loss: 0.8730\n",
            "Epoch: 365, Loss: 0.8706\n",
            "Epoch: 366, Loss: 0.8694\n",
            "Epoch: 367, Loss: 0.8631\n",
            "Epoch: 368, Loss: 0.8674\n",
            "Epoch: 369, Loss: 0.8639\n",
            "Epoch: 370, Loss: 0.8724\n",
            "Epoch: 371, Loss: 0.8662\n",
            "Epoch: 372, Loss: 0.8677\n",
            "Epoch: 373, Loss: 0.8720\n",
            "Epoch: 374, Loss: 0.8714\n",
            "Epoch: 375, Loss: 0.8718\n",
            "Epoch: 376, Loss: 0.8661\n",
            "Epoch: 377, Loss: 0.8664\n",
            "Epoch: 378, Loss: 0.8616\n",
            "Epoch: 379, Loss: 0.8654\n",
            "Epoch: 380, Loss: 0.8681\n",
            "Epoch: 381, Loss: 0.8711\n",
            "Epoch: 382, Loss: 0.8661\n",
            "Epoch: 383, Loss: 0.8696\n",
            "Epoch: 384, Loss: 0.8741\n",
            "Epoch: 385, Loss: 0.8636\n",
            "Epoch: 386, Loss: 0.8603\n",
            "Epoch: 387, Loss: 0.8596\n",
            "Epoch: 388, Loss: 0.8632\n",
            "Epoch: 389, Loss: 0.8714\n",
            "Epoch: 390, Loss: 0.8724\n",
            "Epoch: 391, Loss: 0.8640\n",
            "Epoch: 392, Loss: 0.8673\n",
            "Epoch: 393, Loss: 0.8684\n",
            "Epoch: 394, Loss: 0.8654\n",
            "Epoch: 395, Loss: 0.8679\n",
            "Epoch: 396, Loss: 0.8649\n",
            "Epoch: 397, Loss: 0.8667\n",
            "Epoch: 398, Loss: 0.8675\n",
            "Epoch: 399, Loss: 0.8625\n",
            "Epoch: 400, Loss: 0.8715\n",
            "Epoch: 401, Loss: 0.8704\n",
            "Epoch: 402, Loss: 0.8697\n",
            "Epoch: 403, Loss: 0.8705\n",
            "Epoch: 404, Loss: 0.8697\n",
            "Epoch: 405, Loss: 0.8665\n",
            "Epoch: 406, Loss: 0.8693\n",
            "Epoch: 407, Loss: 0.8698\n",
            "Epoch: 408, Loss: 0.8648\n",
            "Epoch: 409, Loss: 0.8670\n",
            "Epoch: 410, Loss: 0.8691\n",
            "Epoch: 411, Loss: 0.8675\n",
            "Epoch: 412, Loss: 0.8577\n",
            "Epoch: 413, Loss: 0.8623\n",
            "Epoch: 414, Loss: 0.8641\n",
            "Epoch: 415, Loss: 0.8602\n",
            "Epoch: 416, Loss: 0.8680\n",
            "Epoch: 417, Loss: 0.8678\n",
            "Epoch: 418, Loss: 0.8640\n",
            "Epoch: 419, Loss: 0.8726\n",
            "Epoch: 420, Loss: 0.8641\n",
            "Epoch: 421, Loss: 0.8610\n",
            "Epoch: 422, Loss: 0.8635\n",
            "Epoch: 423, Loss: 0.8709\n",
            "Epoch: 424, Loss: 0.8678\n",
            "Epoch: 425, Loss: 0.8614\n",
            "Epoch: 426, Loss: 0.8638\n",
            "Epoch: 427, Loss: 0.8610\n",
            "Epoch: 428, Loss: 0.8650\n",
            "Epoch: 429, Loss: 0.8747\n",
            "Epoch: 430, Loss: 0.8646\n",
            "Epoch: 431, Loss: 0.8698\n",
            "Epoch: 432, Loss: 0.8648\n",
            "Epoch: 433, Loss: 0.8607\n",
            "Epoch: 434, Loss: 0.8625\n",
            "Epoch: 435, Loss: 0.8711\n",
            "Epoch: 436, Loss: 0.8691\n",
            "Epoch: 437, Loss: 0.8717\n",
            "Epoch: 438, Loss: 0.8615\n",
            "Epoch: 439, Loss: 0.8645\n",
            "Epoch: 440, Loss: 0.8590\n",
            "Epoch: 441, Loss: 0.8673\n",
            "Epoch: 442, Loss: 0.8650\n",
            "Epoch: 443, Loss: 0.8596\n",
            "Epoch: 444, Loss: 0.8687\n",
            "Epoch: 445, Loss: 0.8721\n",
            "Epoch: 446, Loss: 0.8755\n",
            "Epoch: 447, Loss: 0.8668\n",
            "Epoch: 448, Loss: 0.8637\n",
            "Epoch: 449, Loss: 0.8635\n",
            "Epoch: 450, Loss: 0.8676\n",
            "Epoch: 451, Loss: 0.8658\n",
            "Epoch: 452, Loss: 0.8724\n",
            "Epoch: 453, Loss: 0.8647\n",
            "Epoch: 454, Loss: 0.8639\n",
            "Epoch: 455, Loss: 0.8651\n",
            "Epoch: 456, Loss: 0.8634\n",
            "Epoch: 457, Loss: 0.8616\n",
            "Epoch: 458, Loss: 0.8656\n",
            "Epoch: 459, Loss: 0.8617\n",
            "Epoch: 460, Loss: 0.8622\n",
            "Epoch: 461, Loss: 0.8595\n",
            "Epoch: 462, Loss: 0.8607\n",
            "Epoch: 463, Loss: 0.8660\n",
            "Epoch: 464, Loss: 0.8685\n",
            "Epoch: 465, Loss: 0.8637\n",
            "Epoch: 466, Loss: 0.8673\n",
            "Epoch: 467, Loss: 0.8655\n",
            "Epoch: 468, Loss: 0.8660\n",
            "Epoch: 469, Loss: 0.8603\n",
            "Epoch: 470, Loss: 0.8669\n",
            "Epoch: 471, Loss: 0.8668\n",
            "Epoch: 472, Loss: 0.8700\n",
            "Epoch: 473, Loss: 0.8672\n",
            "Epoch: 474, Loss: 0.8593\n",
            "Epoch: 475, Loss: 0.8664\n",
            "Epoch: 476, Loss: 0.8677\n",
            "Epoch: 477, Loss: 0.8613\n",
            "Epoch: 478, Loss: 0.8683\n",
            "Epoch: 479, Loss: 0.8608\n",
            "Epoch: 480, Loss: 0.8724\n",
            "Epoch: 481, Loss: 0.8667\n",
            "Epoch: 482, Loss: 0.8665\n",
            "Epoch: 483, Loss: 0.8630\n",
            "Epoch: 484, Loss: 0.8635\n",
            "Epoch: 485, Loss: 0.8697\n",
            "Epoch: 486, Loss: 0.8631\n",
            "Epoch: 487, Loss: 0.8658\n",
            "Epoch: 488, Loss: 0.8646\n",
            "Epoch: 489, Loss: 0.8673\n",
            "Epoch: 490, Loss: 0.8650\n",
            "Epoch: 491, Loss: 0.8643\n",
            "Epoch: 492, Loss: 0.8742\n",
            "Epoch: 493, Loss: 0.8629\n",
            "Epoch: 494, Loss: 0.8664\n",
            "Epoch: 495, Loss: 0.8718\n",
            "Epoch: 496, Loss: 0.8650\n",
            "Epoch: 497, Loss: 0.8620\n",
            "Epoch: 498, Loss: 0.8680\n",
            "Epoch: 499, Loss: 0.8620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsnhGADw_v8N"
      },
      "source": [
        "After training the model, we can call the `test` function to see how well our model performs on unseen labels.\n",
        "Here, we are interested in the accuracy of the model, *i.e.*, the ratio of correctly classified nodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wId7_-jW_ehn",
        "outputId": "3e6205b2-fa77-4b0c-f47c-62beeed4df65"
      },
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yzxxbjcACGM"
      },
      "source": [
        "## Training a Graph Neural Network (GNN)\n",
        "\n",
        "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCLoB6j6_wt8"
      },
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, args):\n",
        "        super(GNN, self).__init__()\n",
        "        self.num_layers = args[\"num_layers\"]\n",
        "\n",
        "        conv_model = self.build_conv_model(args[\"model\"])\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_size, hidden_size))\n",
        "\n",
        "        for l in range(self.num_layers - 1):\n",
        "            self.convs.append(conv_model(hidden_size, hidden_size))\n",
        "        self.post_mp = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.node_feature, data.edge_index, data.batch\n",
        "\n",
        "        for i in range(len(self.convs) - 1):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.leaky_relu(x)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GCN':\n",
        "            return pyg_nn.GCNConv\n",
        "        elif model_type == 'GAT':\n",
        "            return pyg_nn.GATConv\n",
        "        elif model_type == \"GraphSage\":\n",
        "            return pyg_nn.SAGEConv\n",
        "        elif model_type == \"TransformerConv\":\n",
        "            return pyg_nn.TransformerConv\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Model {} unavailable, please add it to GNN.build_conv_model.\".format(model_type))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYPyeWfCCugm"
      },
      "source": [
        "def train_gnn(train_loader, val_loader, test_loader, args, num_node_features, num_classes,\n",
        "          device=\"cpu\"):\n",
        "    model = GNN(num_node_features, args['hidden_size'], num_classes, args).to(device)\n",
        "    print(model)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=5e-4)\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for epoch in range(args['epochs']):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.node_label\n",
        "            loss = model.loss(pred[batch.node_label_index], label)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc = test_gnn(train_loader, model, device)\n",
        "        val_acc = test_gnn(val_loader, model, device)\n",
        "        test_acc = test_gnn(test_loader, model, device)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Train: {train_acc:.4f}, Validation: {val_acc:.4f}. Test: {test_acc:.4f}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        losses.append(round(total_loss, 4))\n",
        "        accuracies.append(test_acc)\n",
        "\n",
        "    return (model, losses, accuracies)\n",
        "\n",
        "def test_gnn(loader, model, device='cuda'):\n",
        "    model.eval()\n",
        "    for batch in loader:\n",
        "        batch.to(device)\n",
        "        logits = model(batch)\n",
        "        pred = logits[batch.node_label_index].max(1)[1]\n",
        "        acc = pred.eq(batch.node_label).sum().item()\n",
        "        total = batch.node_label_index.shape[0]\n",
        "        acc /= total\n",
        "    return acc"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "iRg7AX6nEjc2",
        "outputId": "0271eb0c-9b30-444e-9d4c-73eb0a7fa064"
      },
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 250})'''))\n",
        "\n",
        "args = {\n",
        "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"hidden_size\" : 64,\n",
        "    \"epochs\" : 300,\n",
        "    \"lr\" : 0.01,\n",
        "    \"num_layers\": 2,\n",
        "    \"model\": \"GraphSage\" # [GraphSage, GAT, GCN, TransformerConv]\n",
        "}\n",
        "\n",
        "gnn_model, losses, accuracies = train_gnn(train_loader, val_loader, test_loader, args, num_node_features, num_classes, args[\"device\"])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 250})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "GNN(\n",
            "  (convs): ModuleList(\n",
            "    (0): SAGEConv(59, 64)\n",
            "    (1): SAGEConv(64, 64)\n",
            "  )\n",
            "  (post_mp): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n",
            "Epoch 1: Train: 0.1407, Validation: 0.1389. Test: 0.1500, Loss: 5.3578\n",
            "Epoch 2: Train: 0.4454, Validation: 0.4782. Test: 0.4565, Loss: 3.4861\n",
            "Epoch 3: Train: 0.4642, Validation: 0.4960. Test: 0.4468, Loss: 2.4246\n",
            "Epoch 4: Train: 0.4432, Validation: 0.4701. Test: 0.4355, Loss: 1.7550\n",
            "Epoch 5: Train: 0.4121, Validation: 0.4346. Test: 0.4226, Loss: 1.3918\n",
            "Epoch 6: Train: 0.4589, Validation: 0.5089. Test: 0.4565, Loss: 1.2757\n",
            "Epoch 7: Train: 0.4783, Validation: 0.5105. Test: 0.4677, Loss: 1.1904\n",
            "Epoch 8: Train: 0.4195, Validation: 0.4556. Test: 0.4194, Loss: 1.1675\n",
            "Epoch 9: Train: 0.4741, Validation: 0.5121. Test: 0.4645, Loss: 1.1562\n",
            "Epoch 10: Train: 0.4692, Validation: 0.5153. Test: 0.4581, Loss: 1.1026\n",
            "Epoch 11: Train: 0.4430, Validation: 0.4895. Test: 0.4387, Loss: 1.0789\n",
            "Epoch 12: Train: 0.4928, Validation: 0.5218. Test: 0.4806, Loss: 1.0731\n",
            "Epoch 13: Train: 0.4783, Validation: 0.5073. Test: 0.4661, Loss: 1.0411\n",
            "Epoch 14: Train: 0.4878, Validation: 0.5315. Test: 0.4774, Loss: 1.0540\n",
            "Epoch 15: Train: 0.4991, Validation: 0.5477. Test: 0.4903, Loss: 1.0273\n",
            "Epoch 16: Train: 0.4955, Validation: 0.5363. Test: 0.4919, Loss: 1.0163\n",
            "Epoch 17: Train: 0.5045, Validation: 0.5493. Test: 0.4952, Loss: 1.0169\n",
            "Epoch 18: Train: 0.4957, Validation: 0.5250. Test: 0.4871, Loss: 0.9966\n",
            "Epoch 19: Train: 0.5108, Validation: 0.5557. Test: 0.5065, Loss: 1.0074\n",
            "Epoch 20: Train: 0.5140, Validation: 0.5590. Test: 0.5032, Loss: 0.9872\n",
            "Epoch 21: Train: 0.5201, Validation: 0.5574. Test: 0.5113, Loss: 0.9867\n",
            "Epoch 22: Train: 0.5096, Validation: 0.5444. Test: 0.5048, Loss: 0.9761\n",
            "Epoch 23: Train: 0.5183, Validation: 0.5509. Test: 0.5048, Loss: 0.9712\n",
            "Epoch 24: Train: 0.5326, Validation: 0.5670. Test: 0.5145, Loss: 0.9680\n",
            "Epoch 25: Train: 0.5288, Validation: 0.5670. Test: 0.5210, Loss: 0.9560\n",
            "Epoch 26: Train: 0.5401, Validation: 0.5687. Test: 0.5371, Loss: 0.9589\n",
            "Epoch 27: Train: 0.5330, Validation: 0.5428. Test: 0.5081, Loss: 0.9437\n",
            "Epoch 28: Train: 0.5496, Validation: 0.5703. Test: 0.5452, Loss: 0.9474\n",
            "Epoch 29: Train: 0.5461, Validation: 0.5687. Test: 0.5290, Loss: 0.9355\n",
            "Epoch 30: Train: 0.5570, Validation: 0.5622. Test: 0.5355, Loss: 0.9373\n",
            "Epoch 31: Train: 0.5473, Validation: 0.5590. Test: 0.5226, Loss: 0.9232\n",
            "Epoch 32: Train: 0.5566, Validation: 0.5654. Test: 0.5371, Loss: 0.9252\n",
            "Epoch 33: Train: 0.5524, Validation: 0.5670. Test: 0.5371, Loss: 0.9150\n",
            "Epoch 34: Train: 0.5645, Validation: 0.5751. Test: 0.5435, Loss: 0.9161\n",
            "Epoch 35: Train: 0.5645, Validation: 0.5670. Test: 0.5355, Loss: 0.9042\n",
            "Epoch 36: Train: 0.5728, Validation: 0.5880. Test: 0.5500, Loss: 0.9060\n",
            "Epoch 37: Train: 0.5720, Validation: 0.5897. Test: 0.5355, Loss: 0.8986\n",
            "Epoch 38: Train: 0.5762, Validation: 0.5961. Test: 0.5565, Loss: 0.9013\n",
            "Epoch 39: Train: 0.5788, Validation: 0.5929. Test: 0.5452, Loss: 0.8923\n",
            "Epoch 40: Train: 0.5744, Validation: 0.5880. Test: 0.5452, Loss: 0.8889\n",
            "Epoch 41: Train: 0.5796, Validation: 0.5864. Test: 0.5500, Loss: 0.8866\n",
            "Epoch 42: Train: 0.5813, Validation: 0.5913. Test: 0.5548, Loss: 0.8793\n",
            "Epoch 43: Train: 0.5794, Validation: 0.5929. Test: 0.5484, Loss: 0.8800\n",
            "Epoch 44: Train: 0.5788, Validation: 0.5897. Test: 0.5532, Loss: 0.8744\n",
            "Epoch 45: Train: 0.5859, Validation: 0.5977. Test: 0.5629, Loss: 0.8741\n",
            "Epoch 46: Train: 0.5875, Validation: 0.5897. Test: 0.5565, Loss: 0.8696\n",
            "Epoch 47: Train: 0.5869, Validation: 0.5994. Test: 0.5484, Loss: 0.8674\n",
            "Epoch 48: Train: 0.5893, Validation: 0.6010. Test: 0.5484, Loss: 0.8650\n",
            "Epoch 49: Train: 0.5928, Validation: 0.5848. Test: 0.5645, Loss: 0.8618\n",
            "Epoch 50: Train: 0.5895, Validation: 0.6026. Test: 0.5548, Loss: 0.8608\n",
            "Epoch 51: Train: 0.5889, Validation: 0.5977. Test: 0.5629, Loss: 0.8574\n",
            "Epoch 52: Train: 0.5928, Validation: 0.6107. Test: 0.5613, Loss: 0.8574\n",
            "Epoch 53: Train: 0.5958, Validation: 0.5880. Test: 0.5661, Loss: 0.8542\n",
            "Epoch 54: Train: 0.5946, Validation: 0.6171. Test: 0.5613, Loss: 0.8535\n",
            "Epoch 55: Train: 0.5962, Validation: 0.5864. Test: 0.5645, Loss: 0.8535\n",
            "Epoch 56: Train: 0.5928, Validation: 0.6074. Test: 0.5516, Loss: 0.8509\n",
            "Epoch 57: Train: 0.5972, Validation: 0.5880. Test: 0.5645, Loss: 0.8505\n",
            "Epoch 58: Train: 0.5960, Validation: 0.6090. Test: 0.5694, Loss: 0.8473\n",
            "Epoch 59: Train: 0.6010, Validation: 0.6187. Test: 0.5597, Loss: 0.8438\n",
            "Epoch 60: Train: 0.5988, Validation: 0.6107. Test: 0.5661, Loss: 0.8410\n",
            "Epoch 61: Train: 0.6013, Validation: 0.6074. Test: 0.5677, Loss: 0.8384\n",
            "Epoch 62: Train: 0.6031, Validation: 0.6026. Test: 0.5597, Loss: 0.8378\n",
            "Epoch 63: Train: 0.6008, Validation: 0.6058. Test: 0.5516, Loss: 0.8369\n",
            "Epoch 64: Train: 0.6057, Validation: 0.5913. Test: 0.5565, Loss: 0.8361\n",
            "Epoch 65: Train: 0.6017, Validation: 0.6123. Test: 0.5581, Loss: 0.8357\n",
            "Epoch 66: Train: 0.6093, Validation: 0.6026. Test: 0.5565, Loss: 0.8344\n",
            "Epoch 67: Train: 0.6059, Validation: 0.6058. Test: 0.5613, Loss: 0.8312\n",
            "Epoch 68: Train: 0.6093, Validation: 0.6220. Test: 0.5500, Loss: 0.8296\n",
            "Epoch 69: Train: 0.6111, Validation: 0.6155. Test: 0.5661, Loss: 0.8277\n",
            "Epoch 70: Train: 0.6095, Validation: 0.6042. Test: 0.5597, Loss: 0.8254\n",
            "Epoch 71: Train: 0.6138, Validation: 0.5994. Test: 0.5548, Loss: 0.8252\n",
            "Epoch 72: Train: 0.6120, Validation: 0.6058. Test: 0.5500, Loss: 0.8246\n",
            "Epoch 73: Train: 0.6188, Validation: 0.5961. Test: 0.5597, Loss: 0.8234\n",
            "Epoch 74: Train: 0.6120, Validation: 0.6058. Test: 0.5532, Loss: 0.8233\n",
            "Epoch 75: Train: 0.6174, Validation: 0.6010. Test: 0.5661, Loss: 0.8245\n",
            "Epoch 76: Train: 0.6118, Validation: 0.6010. Test: 0.5532, Loss: 0.8232\n",
            "Epoch 77: Train: 0.6196, Validation: 0.6010. Test: 0.5710, Loss: 0.8232\n",
            "Epoch 78: Train: 0.6140, Validation: 0.6026. Test: 0.5548, Loss: 0.8212\n",
            "Epoch 79: Train: 0.6206, Validation: 0.5977. Test: 0.5581, Loss: 0.8190\n",
            "Epoch 80: Train: 0.6196, Validation: 0.6171. Test: 0.5597, Loss: 0.8156\n",
            "Epoch 81: Train: 0.6188, Validation: 0.6090. Test: 0.5645, Loss: 0.8141\n",
            "Epoch 82: Train: 0.6212, Validation: 0.5977. Test: 0.5532, Loss: 0.8123\n",
            "Epoch 83: Train: 0.6216, Validation: 0.6090. Test: 0.5597, Loss: 0.8109\n",
            "Epoch 84: Train: 0.6229, Validation: 0.5945. Test: 0.5613, Loss: 0.8109\n",
            "Epoch 85: Train: 0.6198, Validation: 0.5994. Test: 0.5581, Loss: 0.8112\n",
            "Epoch 86: Train: 0.6212, Validation: 0.6010. Test: 0.5645, Loss: 0.8118\n",
            "Epoch 87: Train: 0.6154, Validation: 0.6058. Test: 0.5613, Loss: 0.8122\n",
            "Epoch 88: Train: 0.6235, Validation: 0.6058. Test: 0.5613, Loss: 0.8136\n",
            "Epoch 89: Train: 0.6196, Validation: 0.6058. Test: 0.5565, Loss: 0.8113\n",
            "Epoch 90: Train: 0.6259, Validation: 0.6058. Test: 0.5597, Loss: 0.8086\n",
            "Epoch 91: Train: 0.6269, Validation: 0.6107. Test: 0.5629, Loss: 0.8055\n",
            "Epoch 92: Train: 0.6275, Validation: 0.6123. Test: 0.5677, Loss: 0.8034\n",
            "Epoch 93: Train: 0.6253, Validation: 0.6026. Test: 0.5629, Loss: 0.8014\n",
            "Epoch 94: Train: 0.6239, Validation: 0.6026. Test: 0.5565, Loss: 0.8014\n",
            "Epoch 95: Train: 0.6281, Validation: 0.6010. Test: 0.5645, Loss: 0.8025\n",
            "Epoch 96: Train: 0.6227, Validation: 0.5977. Test: 0.5677, Loss: 0.8025\n",
            "Epoch 97: Train: 0.6273, Validation: 0.6042. Test: 0.5677, Loss: 0.8026\n",
            "Epoch 98: Train: 0.6251, Validation: 0.6058. Test: 0.5629, Loss: 0.8015\n",
            "Epoch 99: Train: 0.6309, Validation: 0.6058. Test: 0.5677, Loss: 0.8000\n",
            "Epoch 100: Train: 0.6305, Validation: 0.6123. Test: 0.5613, Loss: 0.7971\n",
            "Epoch 101: Train: 0.6295, Validation: 0.6042. Test: 0.5613, Loss: 0.7955\n",
            "Epoch 102: Train: 0.6315, Validation: 0.6026. Test: 0.5629, Loss: 0.7943\n",
            "Epoch 103: Train: 0.6293, Validation: 0.6042. Test: 0.5677, Loss: 0.7935\n",
            "Epoch 104: Train: 0.6291, Validation: 0.6042. Test: 0.5629, Loss: 0.7934\n",
            "Epoch 105: Train: 0.6271, Validation: 0.6074. Test: 0.5726, Loss: 0.7941\n",
            "Epoch 106: Train: 0.6303, Validation: 0.6010. Test: 0.5726, Loss: 0.7952\n",
            "Epoch 107: Train: 0.6267, Validation: 0.6026. Test: 0.5645, Loss: 0.7954\n",
            "Epoch 108: Train: 0.6291, Validation: 0.6107. Test: 0.5774, Loss: 0.7965\n",
            "Epoch 109: Train: 0.6269, Validation: 0.5994. Test: 0.5710, Loss: 0.7952\n",
            "Epoch 110: Train: 0.6299, Validation: 0.5994. Test: 0.5677, Loss: 0.7934\n",
            "Epoch 111: Train: 0.6342, Validation: 0.6010. Test: 0.5726, Loss: 0.7903\n",
            "Epoch 112: Train: 0.6340, Validation: 0.5961. Test: 0.5661, Loss: 0.7881\n",
            "Epoch 113: Train: 0.6315, Validation: 0.6026. Test: 0.5677, Loss: 0.7862\n",
            "Epoch 114: Train: 0.6327, Validation: 0.5977. Test: 0.5774, Loss: 0.7855\n",
            "Epoch 115: Train: 0.6311, Validation: 0.6042. Test: 0.5613, Loss: 0.7859\n",
            "Epoch 116: Train: 0.6336, Validation: 0.6010. Test: 0.5774, Loss: 0.7865\n",
            "Epoch 117: Train: 0.6329, Validation: 0.5977. Test: 0.5694, Loss: 0.7873\n",
            "Epoch 118: Train: 0.6297, Validation: 0.5994. Test: 0.5742, Loss: 0.7874\n",
            "Epoch 119: Train: 0.6356, Validation: 0.6026. Test: 0.5726, Loss: 0.7875\n",
            "Epoch 120: Train: 0.6334, Validation: 0.6042. Test: 0.5758, Loss: 0.7859\n",
            "Epoch 121: Train: 0.6332, Validation: 0.6058. Test: 0.5629, Loss: 0.7840\n",
            "Epoch 122: Train: 0.6372, Validation: 0.6010. Test: 0.5726, Loss: 0.7818\n",
            "Epoch 123: Train: 0.6362, Validation: 0.6042. Test: 0.5677, Loss: 0.7803\n",
            "Epoch 124: Train: 0.6382, Validation: 0.5961. Test: 0.5710, Loss: 0.7793\n",
            "Epoch 125: Train: 0.6394, Validation: 0.6026. Test: 0.5710, Loss: 0.7786\n",
            "Epoch 126: Train: 0.6360, Validation: 0.6090. Test: 0.5613, Loss: 0.7782\n",
            "Epoch 127: Train: 0.6372, Validation: 0.5961. Test: 0.5790, Loss: 0.7782\n",
            "Epoch 128: Train: 0.6374, Validation: 0.5994. Test: 0.5581, Loss: 0.7789\n",
            "Epoch 129: Train: 0.6336, Validation: 0.5961. Test: 0.5774, Loss: 0.7799\n",
            "Epoch 130: Train: 0.6360, Validation: 0.5994. Test: 0.5726, Loss: 0.7825\n",
            "Epoch 131: Train: 0.6299, Validation: 0.5913. Test: 0.5677, Loss: 0.7844\n",
            "Epoch 132: Train: 0.6352, Validation: 0.6026. Test: 0.5694, Loss: 0.7870\n",
            "Epoch 133: Train: 0.6374, Validation: 0.6010. Test: 0.5758, Loss: 0.7840\n",
            "Epoch 134: Train: 0.6394, Validation: 0.6074. Test: 0.5629, Loss: 0.7798\n",
            "Epoch 135: Train: 0.6453, Validation: 0.6074. Test: 0.5661, Loss: 0.7748\n",
            "Epoch 136: Train: 0.6424, Validation: 0.6058. Test: 0.5742, Loss: 0.7729\n",
            "Epoch 137: Train: 0.6410, Validation: 0.6010. Test: 0.5597, Loss: 0.7738\n",
            "Epoch 138: Train: 0.6372, Validation: 0.5945. Test: 0.5823, Loss: 0.7760\n",
            "Epoch 139: Train: 0.6422, Validation: 0.6026. Test: 0.5629, Loss: 0.7789\n",
            "Epoch 140: Train: 0.6420, Validation: 0.6026. Test: 0.5758, Loss: 0.7776\n",
            "Epoch 141: Train: 0.6420, Validation: 0.6058. Test: 0.5645, Loss: 0.7745\n",
            "Epoch 142: Train: 0.6477, Validation: 0.6107. Test: 0.5661, Loss: 0.7708\n",
            "Epoch 143: Train: 0.6459, Validation: 0.6042. Test: 0.5774, Loss: 0.7697\n",
            "Epoch 144: Train: 0.6443, Validation: 0.5994. Test: 0.5613, Loss: 0.7708\n",
            "Epoch 145: Train: 0.6394, Validation: 0.5961. Test: 0.5726, Loss: 0.7727\n",
            "Epoch 146: Train: 0.6437, Validation: 0.6090. Test: 0.5694, Loss: 0.7747\n",
            "Epoch 147: Train: 0.6426, Validation: 0.6026. Test: 0.5742, Loss: 0.7733\n",
            "Epoch 148: Train: 0.6487, Validation: 0.6058. Test: 0.5532, Loss: 0.7707\n",
            "Epoch 149: Train: 0.6511, Validation: 0.6074. Test: 0.5694, Loss: 0.7671\n",
            "Epoch 150: Train: 0.6471, Validation: 0.6074. Test: 0.5710, Loss: 0.7659\n",
            "Epoch 151: Train: 0.6479, Validation: 0.6026. Test: 0.5629, Loss: 0.7665\n",
            "Epoch 152: Train: 0.6459, Validation: 0.6026. Test: 0.5823, Loss: 0.7675\n",
            "Epoch 153: Train: 0.6465, Validation: 0.6042. Test: 0.5581, Loss: 0.7686\n",
            "Epoch 154: Train: 0.6503, Validation: 0.6042. Test: 0.5855, Loss: 0.7683\n",
            "Epoch 155: Train: 0.6461, Validation: 0.6123. Test: 0.5597, Loss: 0.7677\n",
            "Epoch 156: Train: 0.6525, Validation: 0.6090. Test: 0.5710, Loss: 0.7642\n",
            "Epoch 157: Train: 0.6539, Validation: 0.6090. Test: 0.5742, Loss: 0.7621\n",
            "Epoch 158: Train: 0.6495, Validation: 0.6155. Test: 0.5661, Loss: 0.7617\n",
            "Epoch 159: Train: 0.6491, Validation: 0.6010. Test: 0.5887, Loss: 0.7625\n",
            "Epoch 160: Train: 0.6495, Validation: 0.6042. Test: 0.5613, Loss: 0.7640\n",
            "Epoch 161: Train: 0.6479, Validation: 0.5994. Test: 0.5855, Loss: 0.7645\n",
            "Epoch 162: Train: 0.6507, Validation: 0.6042. Test: 0.5661, Loss: 0.7653\n",
            "Epoch 163: Train: 0.6483, Validation: 0.6042. Test: 0.5790, Loss: 0.7638\n",
            "Epoch 164: Train: 0.6519, Validation: 0.6074. Test: 0.5597, Loss: 0.7616\n",
            "Epoch 165: Train: 0.6558, Validation: 0.6026. Test: 0.5677, Loss: 0.7586\n",
            "Epoch 166: Train: 0.6560, Validation: 0.6074. Test: 0.5694, Loss: 0.7567\n",
            "Epoch 167: Train: 0.6548, Validation: 0.6107. Test: 0.5645, Loss: 0.7559\n",
            "Epoch 168: Train: 0.6548, Validation: 0.6074. Test: 0.5790, Loss: 0.7560\n",
            "Epoch 169: Train: 0.6539, Validation: 0.6058. Test: 0.5726, Loss: 0.7574\n",
            "Epoch 170: Train: 0.6495, Validation: 0.6026. Test: 0.5774, Loss: 0.7587\n",
            "Epoch 171: Train: 0.6570, Validation: 0.6090. Test: 0.5629, Loss: 0.7600\n",
            "Epoch 172: Train: 0.6554, Validation: 0.5977. Test: 0.5839, Loss: 0.7582\n",
            "Epoch 173: Train: 0.6566, Validation: 0.6058. Test: 0.5645, Loss: 0.7570\n",
            "Epoch 174: Train: 0.6562, Validation: 0.6042. Test: 0.5839, Loss: 0.7549\n",
            "Epoch 175: Train: 0.6580, Validation: 0.6107. Test: 0.5677, Loss: 0.7528\n",
            "Epoch 176: Train: 0.6612, Validation: 0.6074. Test: 0.5726, Loss: 0.7503\n",
            "Epoch 177: Train: 0.6630, Validation: 0.6123. Test: 0.5677, Loss: 0.7484\n",
            "Epoch 178: Train: 0.6646, Validation: 0.6090. Test: 0.5629, Loss: 0.7468\n",
            "Epoch 179: Train: 0.6642, Validation: 0.6090. Test: 0.5726, Loss: 0.7456\n",
            "Epoch 180: Train: 0.6636, Validation: 0.6171. Test: 0.5677, Loss: 0.7451\n",
            "Epoch 181: Train: 0.6618, Validation: 0.6058. Test: 0.5823, Loss: 0.7447\n",
            "Epoch 182: Train: 0.6604, Validation: 0.6074. Test: 0.5661, Loss: 0.7452\n",
            "Epoch 183: Train: 0.6556, Validation: 0.5977. Test: 0.5839, Loss: 0.7468\n",
            "Epoch 184: Train: 0.6578, Validation: 0.5961. Test: 0.5597, Loss: 0.7509\n",
            "Epoch 185: Train: 0.6491, Validation: 0.5945. Test: 0.5871, Loss: 0.7539\n",
            "Epoch 186: Train: 0.6598, Validation: 0.5929. Test: 0.5694, Loss: 0.7570\n",
            "Epoch 187: Train: 0.6612, Validation: 0.5961. Test: 0.5823, Loss: 0.7515\n",
            "Epoch 188: Train: 0.6634, Validation: 0.6139. Test: 0.5710, Loss: 0.7446\n",
            "Epoch 189: Train: 0.6677, Validation: 0.6123. Test: 0.5613, Loss: 0.7381\n",
            "Epoch 190: Train: 0.6679, Validation: 0.6010. Test: 0.5726, Loss: 0.7358\n",
            "Epoch 191: Train: 0.6675, Validation: 0.6058. Test: 0.5629, Loss: 0.7369\n",
            "Epoch 192: Train: 0.6640, Validation: 0.5945. Test: 0.5806, Loss: 0.7393\n",
            "Epoch 193: Train: 0.6679, Validation: 0.5977. Test: 0.5726, Loss: 0.7414\n",
            "Epoch 194: Train: 0.6703, Validation: 0.5994. Test: 0.5774, Loss: 0.7390\n",
            "Epoch 195: Train: 0.6683, Validation: 0.6123. Test: 0.5726, Loss: 0.7354\n",
            "Epoch 196: Train: 0.6733, Validation: 0.6042. Test: 0.5677, Loss: 0.7311\n",
            "Epoch 197: Train: 0.6727, Validation: 0.5994. Test: 0.5694, Loss: 0.7290\n",
            "Epoch 198: Train: 0.6739, Validation: 0.6058. Test: 0.5613, Loss: 0.7297\n",
            "Epoch 199: Train: 0.6689, Validation: 0.5945. Test: 0.5758, Loss: 0.7314\n",
            "Epoch 200: Train: 0.6727, Validation: 0.6026. Test: 0.5790, Loss: 0.7336\n",
            "Epoch 201: Train: 0.6689, Validation: 0.5977. Test: 0.5758, Loss: 0.7334\n",
            "Epoch 202: Train: 0.6756, Validation: 0.6058. Test: 0.5758, Loss: 0.7314\n",
            "Epoch 203: Train: 0.6780, Validation: 0.6010. Test: 0.5661, Loss: 0.7271\n",
            "Epoch 204: Train: 0.6780, Validation: 0.6042. Test: 0.5710, Loss: 0.7235\n",
            "Epoch 205: Train: 0.6768, Validation: 0.6107. Test: 0.5677, Loss: 0.7227\n",
            "Epoch 206: Train: 0.6772, Validation: 0.5994. Test: 0.5677, Loss: 0.7228\n",
            "Epoch 207: Train: 0.6792, Validation: 0.6058. Test: 0.5661, Loss: 0.7247\n",
            "Epoch 208: Train: 0.6753, Validation: 0.5961. Test: 0.5726, Loss: 0.7256\n",
            "Epoch 209: Train: 0.6784, Validation: 0.6074. Test: 0.5710, Loss: 0.7265\n",
            "Epoch 210: Train: 0.6812, Validation: 0.5945. Test: 0.5629, Loss: 0.7237\n",
            "Epoch 211: Train: 0.6798, Validation: 0.6026. Test: 0.5742, Loss: 0.7218\n",
            "Epoch 212: Train: 0.6778, Validation: 0.6010. Test: 0.5597, Loss: 0.7187\n",
            "Epoch 213: Train: 0.6782, Validation: 0.5994. Test: 0.5710, Loss: 0.7176\n",
            "Epoch 214: Train: 0.6838, Validation: 0.6058. Test: 0.5613, Loss: 0.7174\n",
            "Epoch 215: Train: 0.6818, Validation: 0.5961. Test: 0.5645, Loss: 0.7179\n",
            "Epoch 216: Train: 0.6812, Validation: 0.6026. Test: 0.5661, Loss: 0.7196\n",
            "Epoch 217: Train: 0.6822, Validation: 0.5977. Test: 0.5661, Loss: 0.7182\n",
            "Epoch 218: Train: 0.6846, Validation: 0.6026. Test: 0.5742, Loss: 0.7157\n",
            "Epoch 219: Train: 0.6836, Validation: 0.5945. Test: 0.5532, Loss: 0.7130\n",
            "Epoch 220: Train: 0.6808, Validation: 0.5929. Test: 0.5726, Loss: 0.7121\n",
            "Epoch 221: Train: 0.6858, Validation: 0.6090. Test: 0.5694, Loss: 0.7118\n",
            "Epoch 222: Train: 0.6844, Validation: 0.5897. Test: 0.5694, Loss: 0.7113\n",
            "Epoch 223: Train: 0.6895, Validation: 0.6010. Test: 0.5565, Loss: 0.7120\n",
            "Epoch 224: Train: 0.6836, Validation: 0.6010. Test: 0.5677, Loss: 0.7106\n",
            "Epoch 225: Train: 0.6848, Validation: 0.5977. Test: 0.5694, Loss: 0.7099\n",
            "Epoch 226: Train: 0.6883, Validation: 0.5929. Test: 0.5581, Loss: 0.7090\n",
            "Epoch 227: Train: 0.6879, Validation: 0.5994. Test: 0.5661, Loss: 0.7099\n",
            "Epoch 228: Train: 0.6909, Validation: 0.5977. Test: 0.5548, Loss: 0.7090\n",
            "Epoch 229: Train: 0.6867, Validation: 0.5977. Test: 0.5677, Loss: 0.7073\n",
            "Epoch 230: Train: 0.6909, Validation: 0.5929. Test: 0.5613, Loss: 0.7043\n",
            "Epoch 231: Train: 0.6899, Validation: 0.5929. Test: 0.5677, Loss: 0.7029\n",
            "Epoch 232: Train: 0.6887, Validation: 0.6026. Test: 0.5710, Loss: 0.7024\n",
            "Epoch 233: Train: 0.6899, Validation: 0.5929. Test: 0.5581, Loss: 0.7034\n",
            "Epoch 234: Train: 0.6887, Validation: 0.5994. Test: 0.5710, Loss: 0.7040\n",
            "Epoch 235: Train: 0.6891, Validation: 0.5929. Test: 0.5645, Loss: 0.7038\n",
            "Epoch 236: Train: 0.6881, Validation: 0.6042. Test: 0.5645, Loss: 0.7044\n",
            "Epoch 237: Train: 0.6836, Validation: 0.5897. Test: 0.5710, Loss: 0.7057\n",
            "Epoch 238: Train: 0.6889, Validation: 0.6026. Test: 0.5629, Loss: 0.7079\n",
            "Epoch 239: Train: 0.6881, Validation: 0.5913. Test: 0.5661, Loss: 0.7072\n",
            "Epoch 240: Train: 0.6939, Validation: 0.5961. Test: 0.5597, Loss: 0.7059\n",
            "Epoch 241: Train: 0.6935, Validation: 0.6010. Test: 0.5661, Loss: 0.7002\n",
            "Epoch 242: Train: 0.6903, Validation: 0.5977. Test: 0.5726, Loss: 0.6959\n",
            "Epoch 243: Train: 0.6943, Validation: 0.5994. Test: 0.5629, Loss: 0.6959\n",
            "Epoch 244: Train: 0.6923, Validation: 0.5913. Test: 0.5597, Loss: 0.6969\n",
            "Epoch 245: Train: 0.6925, Validation: 0.5994. Test: 0.5661, Loss: 0.7002\n",
            "Epoch 246: Train: 0.6891, Validation: 0.5913. Test: 0.5710, Loss: 0.7009\n",
            "Epoch 247: Train: 0.6911, Validation: 0.5897. Test: 0.5661, Loss: 0.7017\n",
            "Epoch 248: Train: 0.6923, Validation: 0.5913. Test: 0.5661, Loss: 0.6997\n",
            "Epoch 249: Train: 0.6915, Validation: 0.6090. Test: 0.5694, Loss: 0.6981\n",
            "Epoch 250: Train: 0.6961, Validation: 0.5994. Test: 0.5677, Loss: 0.6958\n",
            "Epoch 251: Train: 0.6947, Validation: 0.5994. Test: 0.5677, Loss: 0.6937\n",
            "Epoch 252: Train: 0.7024, Validation: 0.5961. Test: 0.5613, Loss: 0.6915\n",
            "Epoch 253: Train: 0.6970, Validation: 0.5929. Test: 0.5677, Loss: 0.6904\n",
            "Epoch 254: Train: 0.6937, Validation: 0.5994. Test: 0.5645, Loss: 0.6916\n",
            "Epoch 255: Train: 0.6943, Validation: 0.5864. Test: 0.5661, Loss: 0.6930\n",
            "Epoch 256: Train: 0.6927, Validation: 0.5977. Test: 0.5661, Loss: 0.6932\n",
            "Epoch 257: Train: 0.6961, Validation: 0.5913. Test: 0.5661, Loss: 0.6922\n",
            "Epoch 258: Train: 0.6984, Validation: 0.5977. Test: 0.5661, Loss: 0.6897\n",
            "Epoch 259: Train: 0.6959, Validation: 0.5961. Test: 0.5790, Loss: 0.6873\n",
            "Epoch 260: Train: 0.7006, Validation: 0.6010. Test: 0.5645, Loss: 0.6858\n",
            "Epoch 261: Train: 0.6974, Validation: 0.6042. Test: 0.5726, Loss: 0.6849\n",
            "Epoch 262: Train: 0.7004, Validation: 0.5994. Test: 0.5597, Loss: 0.6848\n",
            "Epoch 263: Train: 0.6998, Validation: 0.5961. Test: 0.5806, Loss: 0.6842\n",
            "Epoch 264: Train: 0.7006, Validation: 0.5977. Test: 0.5629, Loss: 0.6833\n",
            "Epoch 265: Train: 0.6996, Validation: 0.5977. Test: 0.5790, Loss: 0.6827\n",
            "Epoch 266: Train: 0.7010, Validation: 0.5961. Test: 0.5710, Loss: 0.6820\n",
            "Epoch 267: Train: 0.7000, Validation: 0.5945. Test: 0.5677, Loss: 0.6813\n",
            "Epoch 268: Train: 0.7016, Validation: 0.5945. Test: 0.5694, Loss: 0.6806\n",
            "Epoch 269: Train: 0.7048, Validation: 0.5945. Test: 0.5613, Loss: 0.6801\n",
            "Epoch 270: Train: 0.7008, Validation: 0.5913. Test: 0.5774, Loss: 0.6796\n",
            "Epoch 271: Train: 0.7030, Validation: 0.5961. Test: 0.5661, Loss: 0.6804\n",
            "Epoch 272: Train: 0.6967, Validation: 0.5832. Test: 0.5726, Loss: 0.6812\n",
            "Epoch 273: Train: 0.6965, Validation: 0.6042. Test: 0.5629, Loss: 0.6847\n",
            "Epoch 274: Train: 0.6919, Validation: 0.5864. Test: 0.5758, Loss: 0.6883\n",
            "Epoch 275: Train: 0.6965, Validation: 0.6123. Test: 0.5661, Loss: 0.6932\n",
            "Epoch 276: Train: 0.7004, Validation: 0.5929. Test: 0.5677, Loss: 0.6881\n",
            "Epoch 277: Train: 0.7091, Validation: 0.6074. Test: 0.5613, Loss: 0.6821\n",
            "Epoch 278: Train: 0.7032, Validation: 0.6090. Test: 0.5726, Loss: 0.6757\n",
            "Epoch 279: Train: 0.7046, Validation: 0.6139. Test: 0.5790, Loss: 0.6742\n",
            "Epoch 280: Train: 0.7020, Validation: 0.6058. Test: 0.5597, Loss: 0.6764\n",
            "Epoch 281: Train: 0.7002, Validation: 0.6042. Test: 0.5758, Loss: 0.6792\n",
            "Epoch 282: Train: 0.7075, Validation: 0.6123. Test: 0.5613, Loss: 0.6850\n",
            "Epoch 283: Train: 0.7054, Validation: 0.6090. Test: 0.5758, Loss: 0.6778\n",
            "Epoch 284: Train: 0.7052, Validation: 0.6042. Test: 0.5661, Loss: 0.6726\n",
            "Epoch 285: Train: 0.7052, Validation: 0.6010. Test: 0.5548, Loss: 0.6706\n",
            "Epoch 286: Train: 0.7014, Validation: 0.6171. Test: 0.5726, Loss: 0.6733\n",
            "Epoch 287: Train: 0.7052, Validation: 0.6058. Test: 0.5597, Loss: 0.6783\n",
            "Epoch 288: Train: 0.7079, Validation: 0.6042. Test: 0.5774, Loss: 0.6732\n",
            "Epoch 289: Train: 0.7097, Validation: 0.6058. Test: 0.5645, Loss: 0.6691\n",
            "Epoch 290: Train: 0.7066, Validation: 0.6090. Test: 0.5661, Loss: 0.6673\n",
            "Epoch 291: Train: 0.7044, Validation: 0.6042. Test: 0.5790, Loss: 0.6686\n",
            "Epoch 292: Train: 0.7064, Validation: 0.6090. Test: 0.5629, Loss: 0.6709\n",
            "Epoch 293: Train: 0.7107, Validation: 0.6058. Test: 0.5726, Loss: 0.6701\n",
            "Epoch 294: Train: 0.7077, Validation: 0.6058. Test: 0.5677, Loss: 0.6678\n",
            "Epoch 295: Train: 0.7129, Validation: 0.6155. Test: 0.5581, Loss: 0.6673\n",
            "Epoch 296: Train: 0.7064, Validation: 0.5961. Test: 0.5710, Loss: 0.6663\n",
            "Epoch 297: Train: 0.7137, Validation: 0.6171. Test: 0.5613, Loss: 0.6684\n",
            "Epoch 298: Train: 0.7095, Validation: 0.6058. Test: 0.5726, Loss: 0.6672\n",
            "Epoch 299: Train: 0.7182, Validation: 0.6090. Test: 0.5677, Loss: 0.6660\n",
            "Epoch 300: Train: 0.7109, Validation: 0.6107. Test: 0.5726, Loss: 0.6643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5_KxhsRhMV"
      },
      "source": [
        "### Plotting training data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "aDMQdzuHRYB2",
        "outputId": "ba41cc7d-53bd-4f86-dfc3-9c5f966f58b3"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training Loss\")\n",
        "plt.plot(losses,label=\"Training Loss\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "1pDjO9lYRzty",
        "outputId": "df29adcf-9732-4044-a226-10c919c50ee7"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Testing Accuracy during Training\")\n",
        "plt.plot(accuracies,label=\"Testing Accuracy\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFNCAYAAABIc7ibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xb1d3H8c/PO/GIE2fH2duZkJCwNyVswgq0lNlSSoECpeMplKa09HmgpS2UsNpSoIWwWjaUssIeCZC9F7ETJ3ES7y3pPH/o2rETO5ZtyfL4vl+vvCJdXV39jq5if3PO0bnmnENERERE2lZMtAsQERER6YoUwkRERESiQCFMREREJAoUwkRERESiQCFMREREJAoUwkRERESiQCFMpAswsxIzGxHtOjoTM3vUzH7Tiuc/aGa/CGdN4dacz40+YyLNpxAmEmXeL6+aPwEzK69z/1stON4CM/tO3W3OuRTn3MbwVb3fa15mZs7M5kTqNTob59zVzrlfh/OYZnZUnc9OqXdO6n6+hjSzxpA/N5H+jIl0RgphIlHm/fJKcc6lAFuAM+pseyLa9YXoUmAPcElbvqiZxbXl64WLmcVG4rjOuQ/qfJYmeJvT63yettSpoUO+dyKdiUKYSDtlZjFm9jMz22Bmu83sGTPr5T2WZGb/9LYXmNlCM+tnZncARwH3eT0f93n7OzMb5d1+1MzmmdmrZlZsZp+Z2cg6r/sNM1tjZoVmdr+Zvbdvz9o+dQ4FjgGuAk42s/51Hos1s597bSg2sy/MbLD32AQze9PM9pjZDjP7eZ36flPnGMeaWU6d+5vN7KdmthQoNbO4Ou9TsZmtNLPZ+9T4XTNbVefxg83sx2b2r332u9fM7mmknQeZ2ZfeMZ4Gkuo8dpmZfbjP/vu+5w+Y2WtmVgocV7edNW00sx+Z2U4zyzWzy+scK8PMXjazIu9c/2bf12uKmc01s+e8z00RcJmZzTCzT7zPUK6Z3WdmCQdow4E+NxH7jIl0VgphIu3XdcDZBAPOQCAfmOc9dinQAxgMZABXA+XOuVuAD4BrvZ6Paxs59oXAr4CewHrgDgAz6w08B/yPd9w1wOFN1HkJsMg59y9gFVB3CPUm4CLgVCANuAIoM7NU4C3gP17bRgFvN/E6dV0EnEawl8cHbCAYPnt47fqnmQ3w2nQ+MNerMw04E9gN/BOYZWbp3n5x3vvy+L4v5gWTF4B/AL2AZ4Fzm1EvwDcJvs+pQEMBqr9X/yDgSmCemfX0HpsHlHr7XOr9aYmzCJ7fdOAJwA/cCPQGDgNOAK45wPMb/Nw0Z98WfsZEOiWFMJH262rgFudcjnOukmCQOM8LC9UEf4GNcs75nXNfOOeKmnHs551zn3sB5glgqrf9VGCFc+7f3mP3AtubONYlwJPe7SepPyT5HeBW59waF7TEObcbOB3Y7py72zlX4Zwrds591oz673XOZTvnygGcc88657Y55wLOuaeBdcCMOjXc5Zxb6NWw3jn3tXMuF3gfON/bbxawyzn3RQOvdygQD/zJOVftnHsOWNiMegFedM595NVY0cDj1cDt3vFfA0qAsRYcujwX+KVzrsw5txJ4rJmvXeMT59wLXg3l3ufmU+eczzm3GXiIYOhvTGOfm+bs25LPmEinpBAm0n4NBZ73hooKCPYy+YF+BHtk3gCeMrNtZnaXmcU349h1f+mVASne7YFAds0DzjkH5NAIMzsCGA485W16EphkZjW/cAcT7KXaV2PbQ5Vd946ZXWJmi+u8VxMJ9u409VqPARd7ty8m+L42ZCCw1Xs/anzdmpobsNsLJTVqzksfIG6f5zd1rJBqMLMxZvaKmW33hih/y973rSGNfW6as2+zPmMinZlCmEj7lQ2c4pxLr/MnyTm31est+ZVzLovgUM7p7O2Bco0esWm5QGbNHTOzuvcbcClgwGIz2w58Vmd7TRtGNvC8bKCx5QxKge517vdvYJ/aNnpz0v4CXAtkOOfSgeVeXQeqAYJDjJPNbCLB97CxL0LkAoO896NG3W8a1qu57ry4hmpupjzAR/3zMLiFx9q3hgeA1cBo51wa8HP2vm+R0tzPmEinpRAm0n49CNzhhQzMrI+ZneXdPs7MJnlDVUUEh7IC3vN20HjAacqrBHuyzvaGPX9AwyEIM0sCLiA4IX9qnT/XAd/0nv9X4NdmNtqCJptZBvAKMMDMbjCzRDNLNbOZ3qEXA6eaWS8vzNzQRM3JBMNFnlfX5QR7wmr8FbjZzKZ5NYyqeU+9YcHnCPbgfV7324P7+IRgELrezOLN7Bz2DncCLAEmmNlU732Z20TNIXPO+YF/A3PNrLuZjSN830JNJfj5KfGO+/0wHfdAQv6MiXR2CmEi7dc9wEvAf82sGPgUqAkq/QmGhyKCw5TvsXco7R6Cc8fyzeze5rygc24XwTlSdxGcvJ4FLAIqG9j9bKAceNw5t73mD/AIweGzWcAfgGeA/3q1/g3o5pwrBk4CziA4bLUOOM477j8IhprN3vOebqLmlcDdBIPSDmAS8FGdx58lOCn8SaCYYO9XrzqHeMx7TmNDkTjnqoBzgMsILsUxh2Awqnl8LXA7wS8brKPhifetcS3BSfvbvTrn0/A5aa6bCX5hoJhgb+IB3+twaOZnTKRTs/pTHERE9jKzGILzdb7lnHs32vVEggUXMF0N9G/mlxuixszuJFhvS78l2W50hc+YSGPUEyYi9ZjZyWaWbmaJ7J0j9GmUy4oILwDcBDzVngOYmY3zhnLNzGYQXMLi+WjX1VJd6TMmciBaMVlE9nUYwaG7BGAlcHbNUhCdiZklExy+/Jrg0Gl7lkpwCHIgwZrvBl6MakWt0yU+YyJN0XCkiIiISBRoOFJEREQkChTCRERERKKgw80J6927txs2bFi0yxARERFp0hdffLHLOdenocc6XAgbNmwYixYtinYZIiIiIk0ys0YvcabhSBEREZEoUAgTERERiQKFMBEREZEo6HBzwhpSXV1NTk4OFRUV0S5FwiwpKYnMzEzi4+OjXYqIiEhYdYoQlpOTQ2pqKsOGDcPMol2OhIlzjt27d5OTk8Pw4cOjXY6IiEhYdYrhyIqKCjIyMhTAOhkzIyMjQz2cIiLSKXWKEAYogHVSOq8iItJZdZoQFk27d+9m6tSpTJ06lf79+zNo0KDa+1VVVU0+f8GCBXz88ce19x988EEef/zxsNW3a9cu4uPjefDBB8N2TBEREWmdTjEnLNoyMjJYvHgxAHPnziUlJYWbb7455OcvWLCAlJQUDj/8cACuvvrqsNb37LPPcuihhzJ//vywH7sun89HXJw+UiIiIqFQT1iEfPHFFxxzzDFMmzaNk08+mdzcXADuvfdesrKymDx5MhdeeCGbN2/mwQcf5I9//CNTp07lgw8+YO7cufz+978H4Nhjj+WnP/0pM2bMYMyYMXzwwQcAlJWVccEFF5CVlcXs2bOZOXNmo1cSmD9/PnfffTdbt24lJyendvvjjz/O5MmTmTJlCt/+9rcB2LFjB7Nnz2bKlClMmTKFjz/+mM2bNzNx4sTa5/3+979n7ty5tfXdcMMNTJ8+nXvuuYeXX36ZmTNnctBBB3HiiSeyY8cOAEpKSrj88suZNGkSkydP5l//+hePPPIIN9xwQ+1x//KXv3DjjTeG6QyISFsor/Lz7pqdFFdUR7sUkQ5H3RYR4Jzjuuuu48UXX6RPnz48/fTT3HLLLTzyyCP83//9H5s2bSIxMZGCggLS09O5+uqr6/Wevf322/WO5/P5+Pzzz3nttdf41a9+xVtvvcX9999Pz549WblyJcuXL2fq1KkN1pKdnU1ubi4zZszgggsu4Omnn+ZHP/oRK1as4De/+Q0ff/wxvXv3Zs+ePQBcf/31HHPMMTz//PP4/X5KSkrIz88/YHurqqpqA2B+fj6ffvopZsZf//pX7rrrLu6++25+/etf06NHD5YtW1a7X3x8PHfccQe/+93viI+P5+9//zsPPfRQq957kY7OOcdbq3by6tJtxMQYFx4yhBnDe0W7rFq5heU89N5GdpcGp1p8+XU+WwvK6ZOayMzhvfabx3n21IGcML5fNEoVafc6XQj71csrWLmtKKzHzBqYxi/PmBDy/pWVlSxfvpyTTjoJAL/fz4ABAwCYPHky3/rWtzj77LM5++yzQzreOeecA8C0adPYvHkzAB9++CE//OEPAZg4cSKTJ09u8LlPP/00F1xwAQAXXnghV1xxBT/60Y945513OP/88+nduzcAvXoFf8i/8847tfPRYmNj6dGjR5MhbM6cObW3c3JymDNnDrm5uVRVVdUuLfHWW2/x1FNP1e7Xs2dPAI4//nheeeUVxo8fT3V1NZMmTQrpPRHpbLYWlLO9sJx73l7P+2vz6J2SiC8Q4N9fbuWMKQO56qgRrM8r5r531lNYXs1BQ3ry/WNH0iclkcG9uoelhqKKar7/zy9Ys714v8cOGdaLsf1Teei9jfidIzO9GwCD0rtx40ljeOGrrfv97C2qqOY/y3O5aMYQ3ly5g2p/AIBJg3owfVgvXly8lV+fNZGZIzIIBBwbd5USYzC8d7K+lCNdQqcLYe2Bc44JEybwySef7PfYq6++yvvvv8/LL7/MHXfcUdszdCCJiYlAMBT5fL5m1TJ//ny2b9/OE088AcC2bdtYt25ds44RFxdHIBCovb/vkhHJycm1t6+77jpuuukmzjzzTBYsWFA7bNmY73znO/z2t79l3LhxXH755c2qS6SzePSjTcx9eSUAqUlx/OL0LC45bCg+v+OB9zbw0HsbeHnJNgAmDkpj2tCevLZsO2+uDA73nzi+L7eelsWw3skNHr/S5+e1ZbkUlh14yPA/K7azaHM+5x6cSVzs3hBU5Qvw6rJcXl++nVkT+nPLaeP3C37nTcvc73iF5dWc+8DHPP7J1xw3tg8D07vhDzheW5bLu2vySE6I5ap/fMF1x4/ixcXbWLa1EIDpQ3sy98wJTBzU44D1LssppNLnZ/qw6PcUOud4Y8V2Dh7Sk6KKarYXVnLk6N61j2fvKWP9zhKOGt2buNi9M4Eqqv28vzaPqUPSKSoPPm/miF68tXIHM4b3IiMlMSz1LckuIOAcBw3pGZbjSXh0uhDWnB6rSElMTCQvL49PPvmEww47jOrqatauXcv48ePJzs7muOOO48gjj+Spp56ipKSE1NRUioqa13t3xBFH8Mwzz3DcccexcuXKBsPc2rVrKSkpYevWrbXbfvnLXzJ//nzOPfdcZs+ezU033URGRgZ79uyhV69enHDCCTzwwAPccMMNtcOR/fr1Y+fOnezevZuUlBReeeUVZs2a1WBdhYWFDBo0CIDHHnusdvtJJ53EvHnz+NOf/gQEhyN79uzJzJkzyc7O5ssvv2Tp0qXNeg9EOoO3V+3g9ldWcvy4vpw3LZOZdX7xxsfCTSeN4cJDBvPVlgJSk+I4clRvYmKMH588joWb97BhZwkPvreBb/zxfY4a3ZvYmP17kNbsKObr3WVN1hIbY/x29kTmHDJkv8d+Mmsce0qrGNs/NeS29egWzzPfO4xtBeX1AtVPZ41jW2E5qYnxnP/Qx/zm1VX0T0vi9rMmUO133P/ues6470OOHNWbbvGxtc+bMjidK48cTn5ZFf/72mpe8oLp9KE96ZeWxJxDBnP0mD4h1xdO97y9jj+9tY7uCbFU+gL4A465Z2SxdmcJO4sqeH/dLqp8AUb1TWFEnbC8ansR2XvK6z0vNSmO4gofWQPSuOyIYWzMK+UnJ48lpoFzu6+SSh8Pv7+R1blF9bZ9vGE3CXEx3HH2RL7cUsCVRw5jVN/Qz6VERqcLYe1BTEwMzz33HNdffz2FhYX4fD5uuOEGxowZw8UXX0xhYSHOOa6//nrS09M544wzOO+883jxxRf585//HNJrXHPNNVx66aVkZWUxbtw4JkyYQI8e9f/XOH/+fGbPnl1v27nnnsucOXO47bbbuOWWWzjmmGOIjY3loIMO4tFHH+Wee+7hqquu4m9/+xuxsbE88MADHHbYYdx2223MmDGDQYMGMW7cuEbrmjt3Lueffz49e/bk+OOPZ9OmTQDceuut/OAHP2DixInExsbyy1/+snaY9YILLmDx4sW1Q5QSfdl7ysgv27u8SkpiHCP6pESxoo5tY14JJZX792LnFVdy3fyvmDCwB/d98yC6JzT8I3lgejcGesN/NfqkJnLqpOA0hzmHDObu/65lSU5Bg8/vl5bE3DMnMDUz/YB1JsTFkJzYcA19UhPpk9r8XpleyQn0Sk6ot61ncgI9vW0f/OR4Sit9pCbF1fYQnTctk/veWccH63bVPscXcPx35Q7ueXsd/oAjNsa4/vhRJCfG8dKSbXy+eQ+vLsvlxPH9uOroEfROSaj3mXXOsW5nCRXV/gbrHN03lW4JsQ0+1pTnv8rhT2+t4/TJA0iIiyEtKZ7lWwuZ+/JKkuJjGJaRzJlTBnLYiAye+OxrtuzZG4gH9ujGzd8Yy3tr8ujRPZ5RfVN4e9VOpmSmc+876/jJc8H/nPZOSeD15ds5dkwfvn/sSOZ/voX7F2yguKL+56rKF6DKH2BMvxRivCFdM+OaY0fy+vLt/Ng73vtr8/jThVMZ1Sel9lxI2zPnXLRraJbp06e7fb8FuGrVKsaPHx+liqLD7/dTXV1NUlISGzZs4MQTT2TNmjUkJHS8f0ynn346N954IyeccEKDj3fF8xsOlT4/b6/ayYSBaQzNaHiYal87iiq48/XV/Purrfs9dutp4/nOUSPYsruM5dsKOWF8XxLjWvZLq7NbsGYnq715VYs25/PWqh2N7juwRxLP/+AI+qUltVV5HdanG3fz1sodJMbHcOEhQ+oNiVb6/Dzy4Wbue2cdpVXBoHVSVj+mDQ3+5+69NXl8snF3o8ful5bIN2cMJTG+eYsGVFT7uf/dDRw8NJ3Hr5hJQlzw+fmlVcxfuIXZBw1iQI9uTRylYW+t3EFplY+/f7SZxdl7A/aQXt3ZsqeMmcN77TdkGxtjnDKxf4PDjtl7ynh1WS4TBqbx3ccXUVEdIC0pjn9fczgj+6SwYG0ea7YXM7x3MidP6N+imjsS51ybzD00sy+cc9MbfEwhrGMqLi7muOOOo7q6Guccd955J6ecckq0y2qWgoICZsyYwZQpU3j22Wcb3a8rnt+G/O3DTby3No8+KYlcd/yoRuf/AHy9u5TL/r6QTbtKSYiN4Yojh3PoiF7M/3wL5dXB+X1j+6Xwg+NGkd49od4vsGq/48qjhjN96N4f4s8syua/K3dw+MgMFm7Kp8ofYFB6N0b2TeHIURlcevgwNu8q4/4F68nfZ95RjME3svoz55DBDQ6VdTZfbcln9v17F19OTojl6mNGkjUwrcH9pw5OD9u8Hwn2Li7NKWDltiIefG9DbSDr2T2ea48fzbCM/b/EUF7t5y/vb2RJTmGLXnP8gDSe+u6h9Oge36raG/PVlny++/gibj0ti5eXbGPtzmJ+fsp4Zk3s3+IQ8fXuUlZuK+IXLy7HzOifllQ7Jw/gf8+ZxEUzhhAIOF5cspVFm/O57PBhjO7XOYYwy6v8XPnYQr5z1HCOHxfZb+8qhEmHpvML//z0a259YTmj+qaQW1BOaZWfeG/i9Mg+Kdx6Wla9ScBXPLqQhZv2cOd5k3l71U7+9WVwfbi+qYkMTO+Gc45lWwtxQFyMEXDgDzhOyurHraeN36/nrKLaz8//vYyNu0oZ1z+Vo8f04cnPtrCrpJLV24uJjTH8AUda0v7DlsUV1WzIKyVrQBo3nDi6duhjXP9UUpMi80urNSqq/ewurWJQevN7L5xznPPAx+Tkl/OfHx5F94Q44mOt3kRsaTs+f4Bqf/B3XEJczAH/E+Cco6I60OjjB5IYFxPSfK3WqOm1qfmdHa4enKU5Bfzva6up8gc4Y/IAzp2WyXXzv2LBmjziYw3ngkPBNf/Ga37uDOnVnR+eOIYBPZrfg5uaFMe4/g3/p6QlqnwBrn3yS8zg0sOGcd38ryg6wLp1mT27M6BHEp9s3M1DF0/jGxHu9VMIkw6tK5zfFdsK2bK7jJMn9K/3w7yooppfvLCcFxdv4/hxfXn429PYU1rF0wuzKa/2E3Dw2rJctuwp46Ssfkwc2IOC8ir+/tFmfn7qOK46eiQAi7MLWLu9mDOnDiTJm+i8ensRry7NxR8I/gw4fGTvekEuVB+sy+OTDbtJSYrjokOG7De/xDnHq8ty+e2rq9hWuPebtRnJCcw5ZDC9UxI5f3pmmweyd1fvpFtCLEMzuvP8V1up9jn8gQDPfZHDzuJKHr9yBoePDP392LSrlB8/u4RFX+dz13mTuWD64AhWLxI5JZU+Hv9kMyXefLOx/VM5YlRvnl6YTWmlDwf8d8V2NuSVtvg1jh3bh4MGNzwPOGtgGieO71svaH69u5RXlubi80L1IcN6MqJPCs9/tZVFm/fw9uqdAJjBsIxkTpnYcLBywDurdrJmRzG/OD2LK48c3uI2hKpLhLBx48ZpXZlOyDnH6tWrO1UI8/kDPPn5FpblFPKrsyZgGCfcvYBthRVMGZzO3DOymJKZTnUgwBWPLuSzjXv4/rEj+cFxo2oDVF0V1X7+9uEmHliwoXby99TB6Tz9vUPb1Zyt8io/X2XnEwjsHf75fHNwkeCM5ARG9d3bg5bWLZ5fnpFFZs+m179yzuFc8IdvKD8DAgHHm6t2cPU/v8A5SIiNocq/twdkwsA0yqv97Cqu5M5zJ7N0ayFffl1/rbxDR2TwvWNG1E6kzyuu5Ox5H1Fa5eN/ThnHBdMH6+eRdGrV/gBffJ1fG4qaY/m2Qua9u36/LxXUNa5/Kj26Bf9j5oDFWwrq/TuFvf92Yyz4LWLn4Lkvc/jHFTMZ0sCwcw2fP8D6vBLG9kvVnLDmaiiEbdq0idTUVDIyMvSDrxNxzrF7926Ki4trF33t6Cp9fi57ZGHtBOETx/cjs2c3Hv14M9ccO5Jnv8ghr7iy3nPuPn8K5zawBtO+asIIhB5Ioi0QCA6Lzns3uABpjRXbiujfI4nkhFj8znH1MSNJ77b/l07ySiq49+31bNpVyui+KTzx3Zn0Td1/eGT9zhK2FZTzwldbeWHxVgIuuNzBN7L6sa2gnKuPGVk7/BgTY2TvKePyRxeyfmcJAAcPSSfeG1Ks9AVYnF3AiD7JPHf14cFvwb20gm2F5TzzvcOY3MQ3EEWk/s+ruvzO8dTCbF5bmkugzg4j+qTwwxNG0zc1kSp/gEc/3syWPWV87+gRDO7ZvXYEoa0m2zdHpw9h1dXV5OTk7LeIqHR8SUlJZGZmEh/f/uYONUdheTX//PRrPt24mw/W7eL/zplEWZWf218JLtB52uQBzPvmwZRU+nh2UTZF5XuHAWY10q3emX28fheXPPI5/dKSiImB7D3lje47qm8Kp0zsz18/2MTw3skcO7b+OlFb9pTxytLgtVsTYmO44JBMMnt254Lpg/dbOqGuan+A/yzfzrCMZCZl1v8G2gfr8rjysUWkJsaxu7SKoRnd+d9zJjVr+FJEuoZOH8JE2rMqX4BLHvmMTzfuISk+hhtPHMP3jgnO1Vq7o5jiimqmZKZr8vY+thaUk+GFpBXbCgk08KMqNsaYNKgH8bExvLlyBzc/u4SyqvpDHAmxMVx82FBOGNePIb26078FE4kb8urSXH79ykouPXwYVxw5rF0N/YpI+6EQJhIFX27JZ0dhBX/7cBOLvs7nj3OmMPugpocVRUSk8zhQCNOK+SIRsHxrIed4a0X1Sk7g7vMVwEREpD6FMJEIePTjzXRPiOUfV85gbP80Uhq5FIyIiHRd+s0gEmZ5xZW8tGQbc6YPZtrQXtEuR0RE2imFMJFWKqn08eCCDRSUV+EcvLVqB4GA49LDh0W7NBERaccUwkRayB9w5BVX8rN/L+W9tXn06h78Jt/w3snc/61p9RYfFRER2ZdCmHQqFdV+3l61kyp/8KK904f2YnCvplddP5BnFmVz5+urSU6M41dnTuC4cX0prfTxzb9+xpLsAgDumD2Rb80c2ur6RUSk61AIk07lly+u4OlF2bX3E2Jj+M5RwzlvWiYvL8nlkGE9OXxU6AtqPvrRJua+vJJpQ3tSVF7NlY8t5HfnTeGlJdtYllPAj08ey8RBPThmTJ+mDyYiIlKHQph0WP6AY2t+OY7gWndb9pTxzBfZXHLYUK44YjgVPj8Pv7eR+xds4P4FG2qfN2tCf6YN7ckbK7Zz9kGDuGjGEGK9S17sLK6gvMqPc/Dykm388a21nJTVjwcvnkalz8+chz7lR88uIS7G+M3Zk/jmzCFRabuIiHR8WqxVOqTF2QX87F9LWb29uN72XskJvHvzsbUXfoXgoqkLVu/kjCkDeWPFdua9u4Hyaj8DeiSRW1jBWVMH8vNTx3Pnf1bz7y+31jveqZP68/vzp9ReqHlncXDx1fOnZTKqb2rkGyoiIh2aVsyXTqXKF+DIO98hNsa4+piRpCbt7dCdPrQXQzIOPAdsR1EF2wrKmTo4nT+/s54/vLmW+FjDMC4/Yhhj+wfD1dCMZKYN7RnRtoiISOemFfOlQ6mo9uMPOJIbWeD09eW57Cyu5O+XH8JxY/s2+/j90pLolxa8fuB1x4+ipNLH1oJyfnLyWIZmJLeqdhERkVAphEm74pzj8r8vZNnWQi4/YljtBZxnDM8ga2Aa1f4Aj3y0meG9kzlmdOsnw5sZPz91fKuPIyIi0lwKYdKuvL58O59s3M2Yfin8+Z31tdvN4MhRvdmaX87GXaX8dvYkYrzJ9CIiIh2RQpi0G3tKq7jj1VWM65/Kq9cfRUmlj0DAUeHz87cPNvHh+l30Sk7g56eO54TxzR+GFBERaU8UwiSi8kureHfNTk6dNICk+NhG9yssq+aqxxeRV1LJvG8dTGyM1fuG462nZ7VFuSIiIm0moiHMzGYB9wCxwF+dc/+3z+OXAb8DatYFuM8599dI1iRtp7zKz2WPLmRJdgF/eHMtD148jYmDevDO6h08+Vk2/kCA0yYPpMoX4Pf/XUNBWRX3XnQQUwenR7t0ERGRiItYCDOzWGAecBKQAyw0s5eccyv32fVp59y1kapDoiMQcNz0zGKW5hRw812RR3oAACAASURBVDfG8ORnW7ji0YVkDUxjwZo8BvZIIiEuhpufXQLAjOG9+OUZWUwY2CPKlYuIiLSNSPaEzQDWO+c2ApjZU8BZwL4hTDqh3/13Da8v386tp43nO0eN4KSs/pz7wMcs2pzPraeN55LDhhEXY7yxYjtxsTGcOL4vZppoLyIiXUckQ9ggILvO/RxgZgP7nWtmRwNrgRudc9kN7CMdyK6SSh5+fyPnTcvkyiOHAzC2fypv3Hg03eJj6eUtOwFwyqQB0SpTREQkqmKi/PovA8Occ5OBN4HHGtrJzK4ys0VmtigvL69NC5Tme31ZLv6A47tHjajXuzUovVu9ACYiItKVRTKEbQUG17mfyd4J+AA453Y75yq9u38FpjV0IOfcw8656c656X36tH6BTomsl5fkMqZfSu3lf0RERGR/kQxhC4HRZjbczBKAC4GX6u5gZnXHos4EVkWwHmmFmmuMNnWt0Zz8MhZ+vYczJg9si7JEREQ6rIiFMOecD7gWeINguHrGObfCzG43szO93a43sxVmtgS4HrgsUvVIy/gDjmue+IIz7vuQ99fmccgdb/HoR5sa3f8P/11LfGwMsw8e1IZVioiIdDzWVM9GezN9+nS3aNGiaJfRZdz+8koe+WgTMQYBBzEGDvjLt6dzYla/evsuzi7g7Hkfcc2xI/nJrHHRKVhERKQdMbMvnHPTG3os2hPzpR37encpf/94ExcfOoQ/zpnK2H6pvHTtkYzvn8bPn19GaaWv3v73v7ueXskJXHPcqChVLCIi0nEohEmjHv/ka2LNuO740Zw1dRBv3Hg0Ewf14NdnT2RncSW/enkFH63fhXOO7D1lvLVqBxfNGExKoq6GJSIi0hT9tpQGlVT6eGZhNqdOGkC/tKR6j00b2pPzpmXyzKIcnlmUw3XHj2JHUQVmxsWHDo1SxSIiIh2LQpg06OH3NlBc6atdbHVfd507me8dPYKH3t/In99ZD8DFhw5hQI9ubVmmiIhIh6UQJvvZsruMh97fyJlTBjKlkYtpx8QYo/ul8tvZk+iTmsiUzB6cPKF/G1cqIiLScWlOWBdUUe3nz2+vY2tB+X6PvbJ0G6fd+wGxMcZPT2n6G44JcTH8dNY4Zk0coGs/ioiININCWBcTCDh+/NxS7n5zLT/719J6i6865/jtq6vI7NWdl687kkHpGloUERGJFIWwLuaPb63l5SXbmDa0Jx+s28U7q3fWPrZuZwnbCiu45LChjOyTEsUqRUREOj+FsC7kpSXb+PM765kzfTDzv3soI/sk89N/LSV7TxkAC9YEA9mxY3V9ThERkUjTxPwu5G8fbGRc/1R+M3si8bExPPTtacy+/2NO/tP7ZKQk4ByM7ZeqbziKiIi0AfWEdRG7SypZurWQUycNID42eNpH9U3ln1fO5JyDBzGgRzdy8ss5fnzfKFcqIiLSNagnrJN6dWkuLy3ZysD0bvzitCzeX5eHc/sPNU4ZnM6Uwek451iSU8iYfpoLJiIi0hYUwqLgw3W7+ONba+mbmsj/nDKeIRndAcgvraq3bIQZjO6bSkJc0x2W/oCjtMpHWlI82XvKuPGZxSTGxlBc6ePcgzNZsCaP3ikJTBzYo8HnmxlTG1kTTERERMJPIayNrMot4oN1eSzanM9/V+5gUHo3VucW8faqnXxz5hAS42P4xydfU1blr/e8a44dyU9mNb1e15/eWsvfPtzEk989lAcWrCfGYP5Vh3L6nz/k9eW5vLt6Jydm9SMmRmt5iYiItAcKYW3k1heW88XX+XRPiOXHJ4/lyiOHU1hezZ2vr+axTzbjHHwjqx/nTsskxlv09OH3N/Di4m3c/I2xTYanV5flUlbl5+x5HwHw45PHMnFQDyYN6sFf3t9ElT/AnOmDI91MERERCZFCWITl5JfRKzmBJdkFfPeo4fxk1rjaifFJ8bH8Yc5UfnvOJJyDbgmx9Z5bXFHNTc8s4avsfKYN7dXoa2zZXcbGvFIumjGEnUUVzDlkMCdl9QOCc8CWbS1kXP9UZgxv/BgiIiLSthTCImhZTiFn3Pchlx8xDF/Acfio3rUBrK6k+NgGng0nZfUjIS6GZxbmcPCQnvtdFsg5xwuLt7IqtxiAq44ewfDeyfX2OXF8P/78znquOHK4LiskIiLSjiiERdDGXSUA/P2jzcQYTB/as1nPT02K5+ypA3l6UTZLcgpI7x5f+9iUwelkDUjjxqeXADA0o/t+AaxmvzdvPJpRffWtRxERkfZEISyCdhZV1t6eOKgHqUnxB9i7Yf97zmSmDE7nlSW5+L3rPFb6Ajz03kbiY41x/VM5dEQGUwY3/K1HgNH9UptfvIiIiESUQlgE7SyuICE2hvhY46jRvVt0jNgY41szh/KtmUNrtznn+MWLy3nisy3cdkYWh49s2bFFREQkehTCImhHUSUD0pN4+qrD6g0ltpaZ8euzJvKD40bpEkMiIiIdlEJYBO0oqqBvaiL9eySF/dhmpgAmIiLSgenakRG0s7iSvmnhD2AiIiLS8SmERdCOogr6pSqEiYiIyP4UwiKkpNJHWZWffmmJ0S5FRERE2iGFsAjZUVQBQD8NR4qIiEgDFMIipCaE9VVPmIiIiDRAISxCahZqVU+YiIiINEQhLEI0HCkiIiIHohAWITn55aQmxZGSqKXYREREZH8KYRGyIa+EkX100WwRERFpmEJYhCiEiYiIyIEohEVASaWPHUWVjOiTHO1SREREpJ1SCIuAjXklAOoJExERkUYphEXABi+EjeqrnjARERFpmEJYBGzYWUpsjDGkl0KYiIiINEwhLAI25JUwtFd3EuL09oqIiEjDIpoSzGyWma0xs/Vm9rMD7HeumTkzmx7JetrK5t1lDOutXjARERFpXMRCmJnFAvOAU4As4CIzy2pgv1Tgh8BnkaqlrW0rKGdgulbKFxERkcZFsidsBrDeObfROVcFPAWc1cB+vwbuBCoiWEubKavyUVhezcD0btEuRURERNqxSIawQUB2nfs53rZaZnYwMNg592oE62hT2wqCWXJgD4UwERERaVzUZo6bWQzwB+BHIex7lZktMrNFeXl5kS+uFbYVlAMwoIeGI0VERKRxkQxhW4HBde5nettqpAITgQVmthk4FHipocn5zrmHnXPTnXPT+/TpE8GSWy+3MBjCNBwpIiIiBxLJELYQGG1mw80sAbgQeKnmQedcoXOut3NumHNuGPApcKZzblEEa4q4bQUVmEG/NPWEiYiISOMiFsKccz7gWuANYBXwjHNuhZndbmZnRup1oy23sJw+KYlaI0xEREQOKC6SB3fOvQa8ts+22xrZ99hI1tJWthVUMEBDkSIiItIEddeE2bbCcgZqUr6IiIg0QSEsjJxz5BZUaFK+iIiINEkhLIxeWLyV8mo/w3XJIhEREWmCQliYrN5exE+fW8ahI3pxwfTBTT9BREREujSFsDB5cfE2As4x75sH65uRIiIi0iSlhTBZsCaPg4f2JCMlMdqliIiISAegEBYGO4oqWJVbxHFj+0a7FBEREekgFMLC4L01wetZHju2fV9SSURERNoPhbAwWLB2J/3TkhjXPzXapYiIiEgHoRDWStX+AB+s3cUxY/pgZtEuR0RERDoIhbBW+vLrfIorfRqKFBERkWZRCGulBWvziIsxjhjdO9qliIiISAeiENZKH6wLLk2RlhQf7VJERESkA1EIa6Wc/HLG9tOEfBEREWkehbBW8AccheXV9OyuXjARERFpHoWwVigqr8Y5SO+eEO1SREREpINRCGuF/LIqAHomqydMREREmkchrBXyy6oB9YSJiIhI8ymEtUJ+qdcTphAmIiIizdRkCDOzM8xMYa0BtcORmpgvIiIizRRKuJoDrDOzu8xsXKQL6kgKNBwpIiIiLdRkCHPOXQwcBGwAHjWzT8zsKjPr8otj5ZdVERtjpCXFRbsUERER6WBCGmZ0zhUBzwFPAQOA2cCXZnZdBGtr9/LLqknvFq8Ld4uIiEizhTIn7Ewzex5YAMQDM5xzpwBTgB9Ftrz2raCsinTNBxMREZEWCGUc7Vzgj8659+tudM6VmdmVkSmrY8gvq9I3I0VERKRFQhmOnAt8XnPHzLqZ2TAA59zbEamqgygoq6ZnskKYiIiINF8oIexZIFDnvt/b1uUFe8I0HCkiIiLNF0oIi3POVdXc8W53+e4f5xz5ZdUajhQREZEWCSWE5ZnZmTV3zOwsYFfkSuoYyqv9VPkCWiNMREREWiSUiflXA0+Y2X2AAdnAJRGtqgOouW6khiNFRESkJZoMYc65DcChZpbi3S+JeFUdQFF5MISldVMIExERkeYLaal3MzsNmAAk1SxM6py7PYJ1tXullT4AkhO1Wr6IiIg0XyiLtT5I8PqR1xEcjjwfGBrhutq9Ei+EpSTGRrkSERER6YhCmZh/uHPuEiDfOfcr4DBgTGTLav9KK/0ApCRqOFJERESaL5QQVuH9XWZmA4FqgteP7NJKKoNzwpLVEyYiIiItEMqEppfNLB34HfAl4IC/RLSqDqCktidMc8JERESk+Q6YIMwsBnjbOVcA/MvMXgGSnHOFbVJdO6aJ+SIiItIaBxyOdM4FgHl17lc2J4CZ2SwzW2Nm683sZw08frWZLTOzxWb2oZllNav6KCqp9JEYF0N8bCgjuiIiIiL1hZIg3jazc61mbYoQmVkswQB3CpAFXNRAyHrSOTfJOTcVuAv4Q3NeI5pKKn0aihQREZEWCyWEfY/gBbsrzazIzIrNrCiE580A1jvnNnrXm3wKOKvuDs65usdJJjjfrEMorfSRkqQQJiIiIi0Tyor5qS089iCClziqkQPM3HcnM/sBcBPBi4If38LXanOllT6SExTCREREpGWaTBFmdnRD251z74ejAOfcPGCemX0TuBW4tIEargKuAhgyZEg4XrbViis0HCkiIiItF0qK+HGd20kEhxm/oOleq63A4Dr3M71tjXkKeKChB5xzDwMPA0yfPr1dDFmWVvnom5oU7TJERESkgwplOPKMuvfNbDDwpxCOvRAYbWbDCYavC4Fv7nOs0c65dd7d04B1dBCllX6Se6snTERERFqmJSkiBxjf1E7OOZ+ZXQu8AcQCjzjnVpjZ7cAi59xLwLVmdiLBVfjzaWAosr0KfjtSq+WLiIhIy4QyJ+zP7P3WYgwwleDK+U1yzr0GvLbPttvq3P5hyJW2MyUVmpgvIiIiLRdKilhU57YPmO+c+yhC9XQI/oCjvNqvJSpERESkxUJJEc8BFc45PwQXYTWz7s65ssiW1n6VVgUvWaRvR4qIiEhLhbRiPtCtzv1uwFuRKadjKKnQdSNFRESkdUIJYUnOuZKaO97t7pErqf2ruXi3esJERESkpUIJYaVmdnDNHTObBpRHrqT2r0QhTERERFoplBRxA/CsmW0DDOgPzIloVe1caaUf0HCkiIiItFwoi7UuNLNxwFhv0xrnXHVky2rfSiqDzVdPmIiIiLRUk8OR3gW2k51zy51zy4EUM7sm8qW1XyVeT5hCmIiIiLRUKHPCvuucK6i545zLB74buZLav5KKYE9YslbMFxERkRYKJYTFmpnV3DGzWCAhciW1f0XeEhVp3eKjXImIiIh0VKGMp/0HeNrMHvLufw94PXIltX9F5dV0T4glPjaUDCsiIiKyv1BC2E+Bq4CrvftLCX5Dsssqqqimh3rBREREpBWa7MpxzgWAz4DNwAzgeGBVZMtq34rKfaQlKYSJiIhIyzXaE2ZmY4CLvD+7gKcBnHPHtU1p7VdheTVp3fTNSBEREWm5AyWJ1cAHwOnOufUAZnZjm1TVzhVVVNM/LSnaZYiIiEgHdqDhyHOAXOBdM/uLmZ1AcMX8Lk9zwkRERKS1Gg1hzrkXnHMXAuOAdwlevqivmT1gZt9oqwLbo6Jyn5anEBERkVYJZWJ+qXPuSefcGUAm8BXBb0x2SYGAo6iimrQkzQkTERGRlmvWQlfOuXzn3MPOuRMiVVB7V1Llwzkt1CoiIiKto9VGm6moPHjJIoUwERERaQ2FsGYqKvcuWaR1wkRERKQVFMKaqbC2J0xzwkRERKTlFMKaqajCC2HqCRMREZFWUAhrppo5YVonTERERFpDIayZiiq8OWEKYSIiItIKCmHNVFhejRmkJmpOmIiIiLScQlgzFZVXk5IYR0yMruAkIiIiLacQ1kzB1fI1FCkiIiKtoxDWTEXlPk3KFxERkVZTCGumovJqrREmIiIiraYQ1kwajhQREZFwUAhrpmBPmEKYiIiItI5CWDMVVWhOmIiIiLSeQlgz+PwBSip9Go4UERGRVlMIa4bi2tXyNTFfREREWkchrBl08W4REREJF4WwZigqD/aEaU6YiIiItFZEQ5iZzTKzNWa23sx+1sDjN5nZSjNbamZvm9nQSNbTWoXlXk+YQpiIiIi0UsRCmJnFAvOAU4As4CIzy9pnt6+A6c65ycBzwF2RqiccaocjNSdMREREWimSPWEzgPXOuY3OuSrgKeCsujs45951zpV5dz8FMiNYT6sVlWtOmIiIiIRHJEPYICC7zv0cb1tjrgRej2A9rVbTE6Y5YSIiItJa7WJczcwuBqYDxzTy+FXAVQBDhgxpw8rqKyyvJjbG6J4QG7UaREREpHOIZE/YVmBwnfuZ3rZ6zOxE4BbgTOdcZUMHcs497Jyb7pyb3qdPn4gUG4qich9pSXGYWdRqEBERkc4hkiFsITDazIabWQJwIfBS3R3M7CDgIYIBbGcEawmLogpdN1JERETCI2IhzDnnA64F3gBWAc8451aY2e1mdqa32++AFOBZM1tsZi81crh2oai8WvPBREREJCwiOifMOfca8No+226rc/vESL5+uBWWV+ubkSIiIhIWWjG/GYoqfFojTERERMJCIawZitQTJiIiImGiENYMmpgvIiIi4aIQFiKfP0BFdYCURA1HioiISOsphIWotNIPoBAmIiIiYaEQFqLiyuAli1KSFMJERESk9RTCQlRS6QPUEyYiIiLhoRAWopIKhTAREREJH4WwENX2hGk4UkRERMJAISxEGo4UERGRcFIIC5GGI0VERCScFMJCpOFIERERCSeFsBDVhLDkBIUwERERaT2FsBCVVPjonhBLbIxFuxQRERHpBBTCQlRa5dN8MBEREQkbhbAQFVcohImIiEj4KISFqKTSp0n5IiIiEjYKYSEqrVRPmIiIiISPQliINBwpIiIi4aQQFqIS9YSJiIhIGCmEhUhzwkRERCScFMJC4JzTnDAREREJK4WwEFT6AlT7HckKYSIiIhImCmEhqLlkUaqGI0VERCRMFMJCUFpz8W71hImIiEiYKISFoLjCu3i3QpiIiIiEiUJYCIrKqwENR4qIiEj4KISFYHdpFQAZyYlRrkREREQ6C4WwEOSXBUNYr+SEKFciIiIinYVCWAh2lwRDWM/u8VGuRERERDoLhbAQ7CmtIr17PHGxertEREQkPJQqQrCntEpDkSIiIhJWCmEh2F1aSYZCmIiIiISRQlgI1BMmIiIi4aYQFoJgCNPyFCIiIhI+CmFNCAQc+WXVGo4UERGRsFIIa0JheTX+gKOnQpiIiIiEUURDmJnNMrM1ZrbezH7WwONHm9mXZuYzs/MiWUtL7SmrWS1fIUxERETCJ2IhzMxigXnAKUAWcJGZZe2z2xbgMuDJSNXRWntKtVq+iIiIhF8kr0g9A1jvnNsIYGZPAWcBK2t2cM5t9h4LRLCOVqlZLV8hTERERMIpksORg4DsOvdzvG0dSk1PWEaKQpiIiIiET4eYmG9mV5nZIjNblJeX16avvae0EoCe3RXCREREJHwiGcK2AoPr3M/0tjWbc+5h59x059z0Pn36hKW4UG0tKCcjOYGk+Ng2fV0RERHp3CIZwhYCo81suJklABcCL0Xw9SIie085mb26R7sMERER6WQiFsKccz7gWuANYBXwjHNuhZndbmZnApjZIWaWA5wPPGRmKyJVT0tl55eR2bNbtMsQERGRTiaS347EOfca8No+226rc3shwWHKdskfcGwrKOeUiQOiXYqIiIh0Mh1iYn607CiqoNrvGNxLPWEiIiISXgphB5C9pwyAwT01J0xERETCSyHsALLzywEYrIn5IiIiEmYKYQeQvacMMxiYnhTtUkRERKSTUQg7gOz8MvqnJZEYpzXCREREJLwUwg4gZ0+55oOJiIhIRCiEHUB2fhmZ+makiIiIRIBCWCMqfX62F1WoJ0xEREQiQiGsEdsKKnBO34wUERGRyFAIa8TeNcI0HCkiIiLhpxDWiOz8YAjTxbtFREQkEhTCGpG9p5z4WKN/mtYIExERkfBTCGtETn4ZA9O7ERtj0S5FREREOiGFsEZk52uNMBEREYkchbBG5OwpY7DWCBMREZEIUQhrQGmlj92lVWSqJ0xEREQiRCGsATn55YDWCBMREZHIUQhrgNYIExERkUhTCGtAzRph6gkTERGRSFEIa0D2nnK6xceSkZwQ7VJERESkk1IIa0B2fvCbkWZaI0xEREQiQyGsAdl7yrRGmIiIiESUQtg+nHPk5JeTqUn5IiIiEkEKYfsoKKumpNKnSfkiIiISUQph+6hZI0wLtYqIiEgkKYTtY+/yFBqOFBERkchRCNvH0WP68MIPjmBkn5RolyIiIiKdWFy0C2hvUhLjmDo4PdpliIiISCennjARERGRKFAIExEREYkChTARERGRKFAIExEREYkChTARERGRKFAIExEREYkChTARERGRKFAIExEREYkChTARERGRKFAIExEREYkCc85Fu4ZmMbM84OsIv0xvYFeEX6M9U/u7bvu7ctuha7e/K7cdunb7u3LbIfLtH+qc69PQAx0uhLUFM1vknJse7TqiRe3vuu3vym2Hrt3+rtx26Nrt78pth+i2X8ORIiIiIlGgECYiIiISBQphDXs42gVEmdrfdXXltkPXbn9Xbjt07fZ35bZDFNuvOWEiIiIiUaCeMBEREZEoUAjbh5nNMrM1ZrbezH4W7XragpltNrNlZrbYzBZ523qZ2Ztmts77u2e06wwHM3vEzHaa2fI62xpsqwXd630WlprZwdGrPDwaaf9cM9vqnf/FZnZqncf+x2v/GjM7OTpVh4eZDTazd81spZmtMLMfetu7xPk/QPs7/fk3syQz+9zMlnht/5W3fbiZfea18WkzS/C2J3r313uPD4tm/a11gPY/amab6pz7qd72TvXZBzCzWDP7ysxe8e63j3PvnNMf7w8QC2wARgAJwBIgK9p1tUG7NwO999l2F/Az7/bPgDujXWeY2no0cDCwvKm2AqcCrwMGHAp8Fu36I9T+ucDNDeyb5f0bSASGe/82YqPdhla0fQBwsHc7FVjrtbFLnP8DtL/Tn3/vHKZ4t+OBz7xz+gxwobf9QeD73u1rgAe92xcCT0e7DRFq/6PAeQ3s36k++16bbgKeBF7x7reLc6+esPpmAOudcxudc1XAU8BZUa4pWs4CHvNuPwacHcVawsY59z6wZ5/NjbX1LOBxF/QpkG5mA9qm0shopP2NOQt4yjlX6ZzbBKwn+G+kQ3LO5TrnvvRuFwOrgEF0kfN/gPY3ptOcf+8clnh3470/DjgeeM7bvu+5r/lMPAecYGbWRuWG3QHa35hO9dk3s0zgNOCv3n2jnZx7hbD6BgHZde7ncOAfUp2FA/5rZl+Y2VXetn7OuVzv9nagX3RKaxONtbUrfR6u9YYdHqkz9Nxp2+8NMRxEsEegy53/fdoPXeD8e8NRi4GdwJsEe/YKnHM+b5e67attu/d4IZDRthWH177td87VnPs7vHP/RzNL9LZ1qnMP/An4CRDw7mfQTs69QpgAHOmcOxg4BfiBmR1d90EX7JftEl+j7UptreMBYCQwFcgF7o5uOZFlZinAv4AbnHNFdR/rCue/gfZ3ifPvnPM756YCmQR79MZFuaQ2tW/7zWwi8D8E34dDgF7AT6NYYkSY2enATufcF9GupSEKYfVtBQbXuZ/pbevUnHNbvb93As8T/AG1o6b72ft7Z/QqjLjG2tolPg/OuR3eD+gA8Bf2Djl1uvabWTzBAPKEc+7f3uYuc/4ban9XOv8AzrkC4F3gMILDbHHeQ3XbV9t27/EewO42LjUi6rR/ljdE7ZxzlcDf6Zzn/gjgTDPbTHCK0fHAPbSTc68QVt9CYLT3rYkEgpPyXopyTRFlZslmllpzG/gGsJxguy/1drsUeDE6FbaJxtr6EnCJ902hQ4HCOsNWncY+cz1mEzz/EGz/hd63hYYDo4HP27q+cPHmdfwNWOWc+0Odh7rE+W+s/V3h/JtZHzNL9253A04iOCfuXeA8b7d9z33NZ+I84B2vl7RDaqT9q+v858MIzomqe+47xWffOfc/zrlM59wwgr/T33HOfYv2cu4jOeu/I/4h+K2QtQTnC9wS7XraoL0jCH4DagmwoqbNBMfA3wbWAW8BvaJda5jaO5/gkEs1wXkAVzbWVoLfDJrnfRaWAdOjXX+E2v8Pr31LCf4AGlBn/1u89q8BTol2/a1s+5EEhxqXAou9P6d2lfN/gPZ3+vMPTAa+8tq4HLjN2z6CYLBcDzwLJHrbk7z7673HR0S7DRFq/zveuV8O/JO936DsVJ/9Ou/Dsez9dmS7OPdaMV9EREQkCjQcKSLy/+3dz8sVVRzH8fcHlAwVN7q3hSIYWIKhpOEiXLkIN0Lt3FhggiEh/QUP2MatELgRN4rWItSVKUolyKOV5apNhCIYkkkR8m0x5+rwdMkfjzXdy/sFlztzZ86Zmbv6cM5hvpI0AEOYJEnSAAxhkiRJAzCESZIkDcAQJkmSNABDmKSJkeRS+16Z5O3n3PdH464lSf8WX1EhaeIk2Qrsr6rtT9FmQT2qFTfu+L2qWvI87k+SnoQjYZImRpJ7bXMG2JJkNsm+Vpz4YJLLrRjx7nb+1iQXknwGXG+/nWrF6r8bFaxPf+kpbwAAAdZJREFUMgO82Po72r9We2v4wSTfJvkmyc5e3+eSHE/yQ5Kj7c3jJJlJcr3dy8f/5X8kaXIsePwpkvS/c4DeSFgLU3erakOSF4CLSc62c9cDL1fVj21/V1XdaeVbLic5UVUHkuyprsDxXDvoiluvA5a3NufbsVeBtcDPwEXg9STf05X/WVNVNSoXI0lzORImaRpso6t1Nwt8RVeKaFU79nUvgAHsTXIV+JKuUO8q/tlm4Fh1Ra5vAV8AG3p9/1Rd8etZYCVwF/gd+CTJDuD+vJ9O0lQyhEmaBgHer6pX2uelqhqNhP328KRuLdmbwKaqWkdXT2/RPK77R2/7ATBad/YacBzYDpyeR/+SppghTNIk+hVY2ts/A7yXZCFAktVJFo9ptwz4paruJ1kDbOwd+3PUfo4LwM627mwF8AZdYd+xkiwBllXV58A+umlMSfob14RJmkTXgAdtWvEIcIhuKvBKWxx/G3hrTLvTwLtt3dYNuinJkcPAtSRXquqd3u8ngU3AVaCAD6vqZgtx4ywFPk2yiG6E7oNne0RJ085XVEiSJA3A6UhJkqQBGMIkSZIGYAiTJEkagCFMkiRpAIYwSZKkARjCJEmSBmAIkyRJGoAhTJIkaQB/AfuGfIld9uJ0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xe9DLCyR88_"
      },
      "source": [
        "### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWDSnSTjR35X"
      },
      "source": [
        "final_test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(), batch_size=len(dataset_test))\n",
        "\n",
        "y_pred = None\n",
        "y_true = None\n",
        "\n",
        "gnn_model.eval()\n",
        "for batch in final_test_loader:\n",
        "    batch.to(\"cuda\")\n",
        "    logits = gnn_model(batch)\n",
        "    y_pred = logits[batch.node_label_index].max(1)[1]\n",
        "    y_true = batch.node_label"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp3_kiP0Q0Jf",
        "outputId": "99f74764-4596-4330-ced3-71d39afc4d4a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "target_names = ['Low', 'Middle', 'High']\n",
        "print(classification_report(y_true.cpu(), y_pred.cpu(), target_names=target_names))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Low       0.68      0.63      0.65       235\n",
            "      Middle       0.49      0.41      0.45       209\n",
            "        High       0.54      0.69      0.61       176\n",
            "\n",
            "    accuracy                           0.57       620\n",
            "   macro avg       0.57      0.58      0.57       620\n",
            "weighted avg       0.57      0.57      0.57       620\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqB7w7GyWXbl",
        "outputId": "3bcd3db7-e7e7-42fb-8443-e6e8f289aab1"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "multilabel_confusion_matrix(y_true.cpu(), y_pred.cpu())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[316,  69],\n",
              "        [ 88, 147]],\n",
              "\n",
              "       [[320,  91],\n",
              "        [123,  86]],\n",
              "\n",
              "       [[339, 105],\n",
              "        [ 54, 122]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLm__cI7XRCN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}