{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "restaurants_gnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnzHs-cvhVEi",
        "outputId": "2f684361-d108-466e-b437-8c77cd348402"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 14.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 10.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qvEAMNSiCTc"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_geometric.nn as pyg_nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWmejSI5inim",
        "outputId": "dc634259-f0b6-4be7-eb83-67209de663bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGp07hICiCYy"
      },
      "source": [
        "# Read NetworkX graph of restaurants\n",
        "# No features: restaurants_no_features.gpickle\n",
        "# Basic features: restaurants_basic_features.gpickle\n",
        "\n",
        "G = nx.read_gpickle(\"./drive/MyDrive/Colab Notebooks/restaurants_node_degree.gpickle\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi-4KYdpn1ce",
        "outputId": "bc2df6a3-7637-485e-dca5-d7c877bf9568"
      },
      "source": [
        "G.number_of_nodes()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9pIILdRn3qV",
        "outputId": "9090ea60-6956-49c8-fb9a-e197f1494031"
      },
      "source": [
        "G.number_of_edges()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "491464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbGQz6NixeH"
      },
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, args):\n",
        "        super(GNN, self).__init__()\n",
        "        self.num_layers = args[\"num_layers\"]\n",
        "\n",
        "        conv_model = self.build_conv_model(args[\"model\"])\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_size, hidden_size))\n",
        "\n",
        "        for l in range(self.num_layers - 1):\n",
        "            self.convs.append(conv_model(hidden_size, hidden_size))\n",
        "        self.post_mp = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.node_feature, data.edge_index, data.batch\n",
        "\n",
        "        for i in range(len(self.convs) - 1):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.leaky_relu(x)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GCN':\n",
        "            return pyg_nn.GCNConv\n",
        "        elif model_type == 'GAT':\n",
        "            return pyg_nn.GATConv\n",
        "        elif model_type == \"GraphSage\":\n",
        "            return pyg_nn.SAGEConv\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Model {} unavailable, please add it to GNN.build_conv_model.\".format(model_type))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXuVFQCCixiA"
      },
      "source": [
        "def train(train_loader, val_loader, test_loader, args, num_node_features, num_classes,\n",
        "          device=\"cpu\"):\n",
        "    model = GNN(num_node_features, args['hidden_size'], num_classes, args).to(device)\n",
        "    print(model)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(args['epochs']):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.node_label\n",
        "            loss = model.loss(pred[batch.node_label_index], label)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc = round(test(train_loader, model, device), 4)\n",
        "        val_acc = round(test(val_loader, model, device), 4)\n",
        "        test_acc = round(test(test_loader, model, device), 4)\n",
        "        print(f\"Epoch {epoch + 1}: Train: {train_acc}, Validation: {val_acc}. Test: {test_acc}, Loss: {round(total_loss, 4)}\")\n",
        "\n",
        "def test(loader, model, device='cuda'):\n",
        "    model.eval()\n",
        "    for batch in loader:\n",
        "        batch.to(device)\n",
        "        logits = model(batch)\n",
        "        pred = logits[batch.node_label_index].max(1)[1]\n",
        "        acc = pred.eq(batch.node_label).sum().item()\n",
        "        total = batch.node_label_index.shape[0]\n",
        "        acc /= total\n",
        "    return acc"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-cyNHzf0Tt1",
        "outputId": "355588c4-0001-4693-aaa4-de77b44ccaad"
      },
      "source": [
        "args = {\n",
        "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"hidden_size\" : 128,\n",
        "    \"epochs\" : 250,\n",
        "    \"lr\" : 0.01,\n",
        "    \"num_layers\": 3,\n",
        "    \"model\": \"GraphSage\" # [GraphSage, GAT, GCN]\n",
        "}\n",
        "\n",
        "H = Graph(G)\n",
        "dataset = GraphDataset(graphs=[H], task='node')\n",
        "\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(), batch_size=10)\n",
        "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(), batch_size=10)\n",
        "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(), batch_size=10)\n",
        "\n",
        "num_node_features = dataset_train.num_node_features\n",
        "num_classes = dataset_train.num_node_labels\n",
        "print(f\"There are {num_node_features} features and {num_classes} labels.\")\n",
        "\n",
        "train(train_loader, val_loader, test_loader, args, num_node_features, num_classes, args[\"device\"])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 8 features and 3 labels.\n",
            "GNN(\n",
            "  (convs): ModuleList(\n",
            "    (0): SAGEConv(8, 128)\n",
            "    (1): SAGEConv(128, 128)\n",
            "    (2): SAGEConv(128, 128)\n",
            "  )\n",
            "  (post_mp): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n",
            "Epoch 1: Train: 0.338, Validation: 0.3281. Test: 0.3397, Loss: 11.2614\n",
            "Epoch 2: Train: 0.341, Validation: 0.3418. Test: 0.331, Loss: 6.5609\n",
            "Epoch 3: Train: 0.3272, Validation: 0.3328. Test: 0.3373, Loss: 15.0303\n",
            "Epoch 4: Train: 0.3246, Validation: 0.3281. Test: 0.337, Loss: 18.0754\n",
            "Epoch 5: Train: 0.3629, Validation: 0.3585. Test: 0.3664, Loss: 9.4994\n",
            "Epoch 6: Train: 0.3469, Validation: 0.3351. Test: 0.3427, Loss: 3.4237\n",
            "Epoch 7: Train: 0.3413, Validation: 0.3415. Test: 0.331, Loss: 4.662\n",
            "Epoch 8: Train: 0.3413, Validation: 0.3431. Test: 0.3293, Loss: 5.8553\n",
            "Epoch 9: Train: 0.364, Validation: 0.3585. Test: 0.356, Loss: 5.3081\n",
            "Epoch 10: Train: 0.361, Validation: 0.3581. Test: 0.361, Loss: 4.4751\n",
            "Epoch 11: Train: 0.3719, Validation: 0.3702. Test: 0.3737, Loss: 4.1595\n",
            "Epoch 12: Train: 0.4037, Validation: 0.4129. Test: 0.4061, Loss: 3.2009\n",
            "Epoch 13: Train: 0.37, Validation: 0.3825. Test: 0.359, Loss: 2.1442\n",
            "Epoch 14: Train: 0.3748, Validation: 0.3832. Test: 0.373, Loss: 1.9506\n",
            "Epoch 15: Train: 0.3251, Validation: 0.3268. Test: 0.338, Loss: 1.5879\n",
            "Epoch 16: Train: 0.3121, Validation: 0.3074. Test: 0.3297, Loss: 1.2737\n",
            "Epoch 17: Train: 0.3025, Validation: 0.2994. Test: 0.3146, Loss: 1.546\n",
            "Epoch 18: Train: 0.3001, Validation: 0.2961. Test: 0.3113, Loss: 1.5077\n",
            "Epoch 19: Train: 0.324, Validation: 0.3138. Test: 0.3377, Loss: 1.3533\n",
            "Epoch 20: Train: 0.4363, Validation: 0.4236. Test: 0.4428, Loss: 1.2443\n",
            "Epoch 21: Train: 0.4209, Validation: 0.4172. Test: 0.4251, Loss: 1.1564\n",
            "Epoch 22: Train: 0.3955, Validation: 0.3909. Test: 0.3974, Loss: 1.1096\n",
            "Epoch 23: Train: 0.3768, Validation: 0.3865. Test: 0.3664, Loss: 1.1245\n",
            "Epoch 24: Train: 0.3772, Validation: 0.3868. Test: 0.3667, Loss: 1.1553\n",
            "Epoch 25: Train: 0.3932, Validation: 0.3915. Test: 0.3837, Loss: 1.1217\n",
            "Epoch 26: Train: 0.4343, Validation: 0.4319. Test: 0.4278, Loss: 1.0951\n",
            "Epoch 27: Train: 0.4257, Validation: 0.4196. Test: 0.4218, Loss: 1.0866\n",
            "Epoch 28: Train: 0.4384, Validation: 0.4326. Test: 0.4384, Loss: 1.0826\n",
            "Epoch 29: Train: 0.4467, Validation: 0.4282. Test: 0.4531, Loss: 1.0748\n",
            "Epoch 30: Train: 0.4438, Validation: 0.4352. Test: 0.4528, Loss: 1.0687\n",
            "Epoch 31: Train: 0.4364, Validation: 0.4316. Test: 0.4498, Loss: 1.069\n",
            "Epoch 32: Train: 0.4341, Validation: 0.4322. Test: 0.4528, Loss: 1.0693\n",
            "Epoch 33: Train: 0.442, Validation: 0.4369. Test: 0.4581, Loss: 1.065\n",
            "Epoch 34: Train: 0.4543, Validation: 0.4399. Test: 0.4635, Loss: 1.0588\n",
            "Epoch 35: Train: 0.4564, Validation: 0.4496. Test: 0.4608, Loss: 1.0516\n",
            "Epoch 36: Train: 0.4522, Validation: 0.4486. Test: 0.4531, Loss: 1.0505\n",
            "Epoch 37: Train: 0.4598, Validation: 0.4596. Test: 0.4618, Loss: 1.0495\n",
            "Epoch 38: Train: 0.458, Validation: 0.4586. Test: 0.4578, Loss: 1.0407\n",
            "Epoch 39: Train: 0.4538, Validation: 0.4556. Test: 0.4588, Loss: 1.0354\n",
            "Epoch 40: Train: 0.4547, Validation: 0.4519. Test: 0.4565, Loss: 1.0348\n",
            "Epoch 41: Train: 0.4606, Validation: 0.4463. Test: 0.4608, Loss: 1.0311\n",
            "Epoch 42: Train: 0.4672, Validation: 0.4589. Test: 0.4688, Loss: 1.0242\n",
            "Epoch 43: Train: 0.4784, Validation: 0.475. Test: 0.4751, Loss: 1.0231\n",
            "Epoch 44: Train: 0.4776, Validation: 0.477. Test: 0.4821, Loss: 1.0182\n",
            "Epoch 45: Train: 0.4803, Validation: 0.47. Test: 0.4835, Loss: 1.0107\n",
            "Epoch 46: Train: 0.4825, Validation: 0.4579. Test: 0.4855, Loss: 1.0089\n",
            "Epoch 47: Train: 0.4786, Validation: 0.4676. Test: 0.4801, Loss: 1.0017\n",
            "Epoch 48: Train: 0.4732, Validation: 0.4606. Test: 0.4705, Loss: 1.0033\n",
            "Epoch 49: Train: 0.4749, Validation: 0.473. Test: 0.4711, Loss: 1.001\n",
            "Epoch 50: Train: 0.4418, Validation: 0.4483. Test: 0.4248, Loss: 1.0128\n",
            "Epoch 51: Train: 0.4803, Validation: 0.481. Test: 0.4785, Loss: 1.0207\n",
            "Epoch 52: Train: 0.4876, Validation: 0.487. Test: 0.4815, Loss: 0.9942\n",
            "Epoch 53: Train: 0.4827, Validation: 0.489. Test: 0.4668, Loss: 0.9922\n",
            "Epoch 54: Train: 0.4942, Validation: 0.489. Test: 0.4915, Loss: 0.9941\n",
            "Epoch 55: Train: 0.482, Validation: 0.4927. Test: 0.4768, Loss: 0.9829\n",
            "Epoch 56: Train: 0.499, Validation: 0.4967. Test: 0.4978, Loss: 0.989\n",
            "Epoch 57: Train: 0.4975, Validation: 0.493. Test: 0.4811, Loss: 0.9794\n",
            "Epoch 58: Train: 0.4945, Validation: 0.486. Test: 0.4848, Loss: 0.9807\n",
            "Epoch 59: Train: 0.498, Validation: 0.485. Test: 0.4968, Loss: 0.9767\n",
            "Epoch 60: Train: 0.4958, Validation: 0.4723. Test: 0.4862, Loss: 0.978\n",
            "Epoch 61: Train: 0.5045, Validation: 0.4903. Test: 0.4925, Loss: 0.9755\n",
            "Epoch 62: Train: 0.4988, Validation: 0.5033. Test: 0.4955, Loss: 0.9669\n",
            "Epoch 63: Train: 0.5043, Validation: 0.4913. Test: 0.4928, Loss: 0.973\n",
            "Epoch 64: Train: 0.5023, Validation: 0.498. Test: 0.4885, Loss: 0.9688\n",
            "Epoch 65: Train: 0.5032, Validation: 0.4977. Test: 0.4935, Loss: 0.9712\n",
            "Epoch 66: Train: 0.5084, Validation: 0.4963. Test: 0.5028, Loss: 0.9644\n",
            "Epoch 67: Train: 0.5103, Validation: 0.4933. Test: 0.4975, Loss: 0.9627\n",
            "Epoch 68: Train: 0.5126, Validation: 0.5073. Test: 0.5098, Loss: 0.9615\n",
            "Epoch 69: Train: 0.5076, Validation: 0.5037. Test: 0.4992, Loss: 0.9575\n",
            "Epoch 70: Train: 0.5175, Validation: 0.5057. Test: 0.5032, Loss: 0.9588\n",
            "Epoch 71: Train: 0.5079, Validation: 0.4937. Test: 0.4988, Loss: 0.9595\n",
            "Epoch 72: Train: 0.5001, Validation: 0.4967. Test: 0.5072, Loss: 0.9659\n",
            "Epoch 73: Train: 0.4963, Validation: 0.4863. Test: 0.4945, Loss: 0.9742\n",
            "Epoch 74: Train: 0.512, Validation: 0.5. Test: 0.4905, Loss: 0.9873\n",
            "Epoch 75: Train: 0.5076, Validation: 0.5007. Test: 0.5048, Loss: 0.9601\n",
            "Epoch 76: Train: 0.4919, Validation: 0.4833. Test: 0.4831, Loss: 0.9629\n",
            "Epoch 77: Train: 0.5128, Validation: 0.4943. Test: 0.5002, Loss: 0.9804\n",
            "Epoch 78: Train: 0.5062, Validation: 0.5027. Test: 0.4992, Loss: 0.9544\n",
            "Epoch 79: Train: 0.508, Validation: 0.495. Test: 0.5008, Loss: 0.9686\n",
            "Epoch 80: Train: 0.514, Validation: 0.5047. Test: 0.5072, Loss: 0.9639\n",
            "Epoch 81: Train: 0.5066, Validation: 0.5033. Test: 0.4955, Loss: 0.9554\n",
            "Epoch 82: Train: 0.5181, Validation: 0.507. Test: 0.5038, Loss: 0.9642\n",
            "Epoch 83: Train: 0.5105, Validation: 0.4993. Test: 0.5038, Loss: 0.951\n",
            "Epoch 84: Train: 0.5215, Validation: 0.512. Test: 0.5155, Loss: 0.9601\n",
            "Epoch 85: Train: 0.517, Validation: 0.506. Test: 0.5078, Loss: 0.9473\n",
            "Epoch 86: Train: 0.5169, Validation: 0.4997. Test: 0.5088, Loss: 0.9566\n",
            "Epoch 87: Train: 0.5242, Validation: 0.5073. Test: 0.5105, Loss: 0.9535\n",
            "Epoch 88: Train: 0.5148, Validation: 0.508. Test: 0.5102, Loss: 0.9461\n",
            "Epoch 89: Train: 0.5202, Validation: 0.5083. Test: 0.5128, Loss: 0.9597\n",
            "Epoch 90: Train: 0.522, Validation: 0.5037. Test: 0.5102, Loss: 0.9495\n",
            "Epoch 91: Train: 0.5237, Validation: 0.5144. Test: 0.5142, Loss: 0.9468\n",
            "Epoch 92: Train: 0.5217, Validation: 0.5073. Test: 0.5048, Loss: 0.9538\n",
            "Epoch 93: Train: 0.5242, Validation: 0.5047. Test: 0.5092, Loss: 0.9444\n",
            "Epoch 94: Train: 0.5256, Validation: 0.5167. Test: 0.5105, Loss: 0.9421\n",
            "Epoch 95: Train: 0.5249, Validation: 0.507. Test: 0.5135, Loss: 0.9507\n",
            "Epoch 96: Train: 0.5281, Validation: 0.5197. Test: 0.5169, Loss: 0.9443\n",
            "Epoch 97: Train: 0.526, Validation: 0.524. Test: 0.5142, Loss: 0.9395\n",
            "Epoch 98: Train: 0.5268, Validation: 0.5057. Test: 0.5125, Loss: 0.9468\n",
            "Epoch 99: Train: 0.5317, Validation: 0.5123. Test: 0.5169, Loss: 0.943\n",
            "Epoch 100: Train: 0.5264, Validation: 0.5237. Test: 0.5169, Loss: 0.9375\n",
            "Epoch 101: Train: 0.5215, Validation: 0.5027. Test: 0.5135, Loss: 0.947\n",
            "Epoch 102: Train: 0.5309, Validation: 0.5134. Test: 0.5098, Loss: 0.9494\n",
            "Epoch 103: Train: 0.5314, Validation: 0.5297. Test: 0.5199, Loss: 0.9361\n",
            "Epoch 104: Train: 0.5174, Validation: 0.51. Test: 0.5102, Loss: 0.9413\n",
            "Epoch 105: Train: 0.5326, Validation: 0.5244. Test: 0.5172, Loss: 0.951\n",
            "Epoch 106: Train: 0.5312, Validation: 0.5224. Test: 0.5108, Loss: 0.9359\n",
            "Epoch 107: Train: 0.5221, Validation: 0.5083. Test: 0.5172, Loss: 0.9384\n",
            "Epoch 108: Train: 0.533, Validation: 0.5267. Test: 0.5225, Loss: 0.9472\n",
            "Epoch 109: Train: 0.5355, Validation: 0.5307. Test: 0.5212, Loss: 0.9353\n",
            "Epoch 110: Train: 0.5273, Validation: 0.5083. Test: 0.5182, Loss: 0.9344\n",
            "Epoch 111: Train: 0.5348, Validation: 0.529. Test: 0.5192, Loss: 0.9419\n",
            "Epoch 112: Train: 0.5357, Validation: 0.5297. Test: 0.5225, Loss: 0.9348\n",
            "Epoch 113: Train: 0.5312, Validation: 0.5164. Test: 0.5172, Loss: 0.9322\n",
            "Epoch 114: Train: 0.5359, Validation: 0.535. Test: 0.5229, Loss: 0.9375\n",
            "Epoch 115: Train: 0.5349, Validation: 0.5244. Test: 0.5189, Loss: 0.9356\n",
            "Epoch 116: Train: 0.5368, Validation: 0.528. Test: 0.5225, Loss: 0.9319\n",
            "Epoch 117: Train: 0.5371, Validation: 0.534. Test: 0.5269, Loss: 0.93\n",
            "Epoch 118: Train: 0.5319, Validation: 0.5147. Test: 0.5205, Loss: 0.9324\n",
            "Epoch 119: Train: 0.5367, Validation: 0.534. Test: 0.5202, Loss: 0.9371\n",
            "Epoch 120: Train: 0.5319, Validation: 0.519. Test: 0.5199, Loss: 0.9361\n",
            "Epoch 121: Train: 0.5362, Validation: 0.5347. Test: 0.5249, Loss: 0.936\n",
            "Epoch 122: Train: 0.5381, Validation: 0.524. Test: 0.5259, Loss: 0.9308\n",
            "Epoch 123: Train: 0.5359, Validation: 0.5264. Test: 0.5182, Loss: 0.9286\n",
            "Epoch 124: Train: 0.5403, Validation: 0.537. Test: 0.5302, Loss: 0.9313\n",
            "Epoch 125: Train: 0.5254, Validation: 0.5197. Test: 0.5205, Loss: 0.9349\n",
            "Epoch 126: Train: 0.5305, Validation: 0.524. Test: 0.5172, Loss: 0.944\n",
            "Epoch 127: Train: 0.5373, Validation: 0.5267. Test: 0.5255, Loss: 0.9386\n",
            "Epoch 128: Train: 0.5357, Validation: 0.5227. Test: 0.5215, Loss: 0.9303\n",
            "Epoch 129: Train: 0.5386, Validation: 0.532. Test: 0.5339, Loss: 0.9308\n",
            "Epoch 130: Train: 0.5317, Validation: 0.517. Test: 0.5185, Loss: 0.9356\n",
            "Epoch 131: Train: 0.5394, Validation: 0.5244. Test: 0.5265, Loss: 0.9359\n",
            "Epoch 132: Train: 0.5413, Validation: 0.5294. Test: 0.5359, Loss: 0.9284\n",
            "Epoch 133: Train: 0.5308, Validation: 0.5214. Test: 0.5235, Loss: 0.9297\n",
            "Epoch 134: Train: 0.5388, Validation: 0.532. Test: 0.5205, Loss: 0.9365\n",
            "Epoch 135: Train: 0.5386, Validation: 0.5237. Test: 0.5252, Loss: 0.9315\n",
            "Epoch 136: Train: 0.536, Validation: 0.522. Test: 0.5259, Loss: 0.9271\n",
            "Epoch 137: Train: 0.5385, Validation: 0.532. Test: 0.5319, Loss: 0.9299\n",
            "Epoch 138: Train: 0.533, Validation: 0.52. Test: 0.5215, Loss: 0.9323\n",
            "Epoch 139: Train: 0.5429, Validation: 0.5334. Test: 0.5305, Loss: 0.9334\n",
            "Epoch 140: Train: 0.5398, Validation: 0.5314. Test: 0.5225, Loss: 0.9283\n",
            "Epoch 141: Train: 0.5396, Validation: 0.5324. Test: 0.5239, Loss: 0.9255\n",
            "Epoch 142: Train: 0.541, Validation: 0.53. Test: 0.5272, Loss: 0.9264\n",
            "Epoch 143: Train: 0.5367, Validation: 0.528. Test: 0.5222, Loss: 0.9278\n",
            "Epoch 144: Train: 0.5421, Validation: 0.534. Test: 0.5289, Loss: 0.9287\n",
            "Epoch 145: Train: 0.5376, Validation: 0.533. Test: 0.5232, Loss: 0.9268\n",
            "Epoch 146: Train: 0.5438, Validation: 0.5304. Test: 0.5282, Loss: 0.9262\n",
            "Epoch 147: Train: 0.5389, Validation: 0.529. Test: 0.5225, Loss: 0.9254\n",
            "Epoch 148: Train: 0.5431, Validation: 0.533. Test: 0.5332, Loss: 0.9257\n",
            "Epoch 149: Train: 0.5361, Validation: 0.5207. Test: 0.5259, Loss: 0.9265\n",
            "Epoch 150: Train: 0.5357, Validation: 0.5294. Test: 0.5152, Loss: 0.931\n",
            "Epoch 151: Train: 0.5247, Validation: 0.5147. Test: 0.5222, Loss: 0.9356\n",
            "Epoch 152: Train: 0.537, Validation: 0.5267. Test: 0.5225, Loss: 0.9473\n",
            "Epoch 153: Train: 0.5426, Validation: 0.533. Test: 0.5305, Loss: 0.9335\n",
            "Epoch 154: Train: 0.5334, Validation: 0.5207. Test: 0.5209, Loss: 0.9256\n",
            "Epoch 155: Train: 0.5419, Validation: 0.5274. Test: 0.5299, Loss: 0.9351\n",
            "Epoch 156: Train: 0.5398, Validation: 0.531. Test: 0.5275, Loss: 0.933\n",
            "Epoch 157: Train: 0.539, Validation: 0.5304. Test: 0.5265, Loss: 0.9255\n",
            "Epoch 158: Train: 0.5439, Validation: 0.533. Test: 0.5299, Loss: 0.926\n",
            "Epoch 159: Train: 0.5399, Validation: 0.528. Test: 0.5235, Loss: 0.9293\n",
            "Epoch 160: Train: 0.5422, Validation: 0.5277. Test: 0.5259, Loss: 0.9256\n",
            "Epoch 161: Train: 0.542, Validation: 0.5294. Test: 0.5285, Loss: 0.9245\n",
            "Epoch 162: Train: 0.5377, Validation: 0.528. Test: 0.5239, Loss: 0.928\n",
            "Epoch 163: Train: 0.5447, Validation: 0.5304. Test: 0.5352, Loss: 0.9283\n",
            "Epoch 164: Train: 0.5444, Validation: 0.537. Test: 0.5285, Loss: 0.9239\n",
            "Epoch 165: Train: 0.5414, Validation: 0.534. Test: 0.5245, Loss: 0.9222\n",
            "Epoch 166: Train: 0.544, Validation: 0.5344. Test: 0.5279, Loss: 0.924\n",
            "Epoch 167: Train: 0.5338, Validation: 0.5244. Test: 0.5239, Loss: 0.9268\n",
            "Epoch 168: Train: 0.5414, Validation: 0.537. Test: 0.5232, Loss: 0.932\n",
            "Epoch 169: Train: 0.5378, Validation: 0.5247. Test: 0.5235, Loss: 0.9299\n",
            "Epoch 170: Train: 0.5448, Validation: 0.533. Test: 0.5299, Loss: 0.9274\n",
            "Epoch 171: Train: 0.5439, Validation: 0.5327. Test: 0.5289, Loss: 0.9222\n",
            "Epoch 172: Train: 0.5375, Validation: 0.5294. Test: 0.5222, Loss: 0.9229\n",
            "Epoch 173: Train: 0.5444, Validation: 0.5354. Test: 0.5315, Loss: 0.9277\n",
            "Epoch 174: Train: 0.5366, Validation: 0.5234. Test: 0.5239, Loss: 0.9289\n",
            "Epoch 175: Train: 0.5442, Validation: 0.5257. Test: 0.5249, Loss: 0.9293\n",
            "Epoch 176: Train: 0.5468, Validation: 0.531. Test: 0.5279, Loss: 0.9236\n",
            "Epoch 177: Train: 0.5378, Validation: 0.5287. Test: 0.5245, Loss: 0.9233\n",
            "Epoch 178: Train: 0.5453, Validation: 0.5357. Test: 0.5352, Loss: 0.9271\n",
            "Epoch 179: Train: 0.5417, Validation: 0.533. Test: 0.5235, Loss: 0.9261\n",
            "Epoch 180: Train: 0.5452, Validation: 0.5324. Test: 0.5309, Loss: 0.9239\n",
            "Epoch 181: Train: 0.5461, Validation: 0.5367. Test: 0.5299, Loss: 0.921\n",
            "Epoch 182: Train: 0.542, Validation: 0.5327. Test: 0.5265, Loss: 0.9207\n",
            "Epoch 183: Train: 0.547, Validation: 0.5314. Test: 0.5289, Loss: 0.9224\n",
            "Epoch 184: Train: 0.5405, Validation: 0.527. Test: 0.5275, Loss: 0.9232\n",
            "Epoch 185: Train: 0.5467, Validation: 0.5354. Test: 0.5315, Loss: 0.9244\n",
            "Epoch 186: Train: 0.5404, Validation: 0.5297. Test: 0.5262, Loss: 0.9237\n",
            "Epoch 187: Train: 0.5471, Validation: 0.533. Test: 0.5315, Loss: 0.9243\n",
            "Epoch 188: Train: 0.5413, Validation: 0.5314. Test: 0.5255, Loss: 0.9232\n",
            "Epoch 189: Train: 0.546, Validation: 0.5327. Test: 0.5319, Loss: 0.9226\n",
            "Epoch 190: Train: 0.5447, Validation: 0.534. Test: 0.5272, Loss: 0.9207\n",
            "Epoch 191: Train: 0.5447, Validation: 0.5354. Test: 0.5325, Loss: 0.9194\n",
            "Epoch 192: Train: 0.5464, Validation: 0.534. Test: 0.5315, Loss: 0.9196\n",
            "Epoch 193: Train: 0.5396, Validation: 0.5287. Test: 0.5282, Loss: 0.9207\n",
            "Epoch 194: Train: 0.5408, Validation: 0.5364. Test: 0.5252, Loss: 0.9243\n",
            "Epoch 195: Train: 0.5263, Validation: 0.5177. Test: 0.5192, Loss: 0.9299\n",
            "Epoch 196: Train: 0.535, Validation: 0.525. Test: 0.5128, Loss: 0.947\n",
            "Epoch 197: Train: 0.5437, Validation: 0.531. Test: 0.5305, Loss: 0.9379\n",
            "Epoch 198: Train: 0.5384, Validation: 0.524. Test: 0.5292, Loss: 0.9221\n",
            "Epoch 199: Train: 0.5407, Validation: 0.5377. Test: 0.5295, Loss: 0.9277\n",
            "Epoch 200: Train: 0.5348, Validation: 0.524. Test: 0.5249, Loss: 0.9361\n",
            "Epoch 201: Train: 0.5431, Validation: 0.5304. Test: 0.5292, Loss: 0.9312\n",
            "Epoch 202: Train: 0.5433, Validation: 0.5314. Test: 0.5249, Loss: 0.9211\n",
            "Epoch 203: Train: 0.5383, Validation: 0.5287. Test: 0.5272, Loss: 0.9308\n",
            "Epoch 204: Train: 0.5423, Validation: 0.5294. Test: 0.5272, Loss: 0.9267\n",
            "Epoch 205: Train: 0.5425, Validation: 0.534. Test: 0.5255, Loss: 0.922\n",
            "Epoch 206: Train: 0.5393, Validation: 0.5284. Test: 0.5292, Loss: 0.9305\n",
            "Epoch 207: Train: 0.5452, Validation: 0.5337. Test: 0.5259, Loss: 0.9259\n",
            "Epoch 208: Train: 0.5426, Validation: 0.534. Test: 0.5255, Loss: 0.92\n",
            "Epoch 209: Train: 0.5367, Validation: 0.5214. Test: 0.5265, Loss: 0.9284\n",
            "Epoch 210: Train: 0.5465, Validation: 0.5304. Test: 0.5265, Loss: 0.9283\n",
            "Epoch 211: Train: 0.5455, Validation: 0.534. Test: 0.5275, Loss: 0.9193\n",
            "Epoch 212: Train: 0.5361, Validation: 0.5224. Test: 0.5222, Loss: 0.9266\n",
            "Epoch 213: Train: 0.5464, Validation: 0.535. Test: 0.5292, Loss: 0.9308\n",
            "Epoch 214: Train: 0.5461, Validation: 0.5377. Test: 0.5335, Loss: 0.9197\n",
            "Epoch 215: Train: 0.5362, Validation: 0.52. Test: 0.5249, Loss: 0.9227\n",
            "Epoch 216: Train: 0.5452, Validation: 0.5317. Test: 0.5235, Loss: 0.9318\n",
            "Epoch 217: Train: 0.5474, Validation: 0.5357. Test: 0.5279, Loss: 0.9233\n",
            "Epoch 218: Train: 0.5406, Validation: 0.525. Test: 0.5259, Loss: 0.9189\n",
            "Epoch 219: Train: 0.5469, Validation: 0.5317. Test: 0.5279, Loss: 0.9246\n",
            "Epoch 220: Train: 0.5446, Validation: 0.529. Test: 0.5302, Loss: 0.9231\n",
            "Epoch 221: Train: 0.5439, Validation: 0.5344. Test: 0.5302, Loss: 0.9187\n",
            "Epoch 222: Train: 0.5461, Validation: 0.5374. Test: 0.5355, Loss: 0.9189\n",
            "Epoch 223: Train: 0.5413, Validation: 0.5264. Test: 0.5285, Loss: 0.9218\n",
            "Epoch 224: Train: 0.5486, Validation: 0.5324. Test: 0.5339, Loss: 0.9217\n",
            "Epoch 225: Train: 0.5479, Validation: 0.536. Test: 0.5349, Loss: 0.918\n",
            "Epoch 226: Train: 0.5418, Validation: 0.5284. Test: 0.5289, Loss: 0.9188\n",
            "Epoch 227: Train: 0.5485, Validation: 0.5344. Test: 0.5285, Loss: 0.9221\n",
            "Epoch 228: Train: 0.5423, Validation: 0.5297. Test: 0.5285, Loss: 0.9213\n",
            "Epoch 229: Train: 0.5474, Validation: 0.5364. Test: 0.5295, Loss: 0.9194\n",
            "Epoch 230: Train: 0.5489, Validation: 0.534. Test: 0.5299, Loss: 0.9174\n",
            "Epoch 231: Train: 0.5431, Validation: 0.5314. Test: 0.5319, Loss: 0.9175\n",
            "Epoch 232: Train: 0.548, Validation: 0.5354. Test: 0.5339, Loss: 0.9193\n",
            "Epoch 233: Train: 0.5431, Validation: 0.5314. Test: 0.5292, Loss: 0.9198\n",
            "Epoch 234: Train: 0.5491, Validation: 0.5327. Test: 0.5319, Loss: 0.9205\n",
            "Epoch 235: Train: 0.5453, Validation: 0.5314. Test: 0.5332, Loss: 0.9187\n",
            "Epoch 236: Train: 0.5474, Validation: 0.5391. Test: 0.5295, Loss: 0.9174\n",
            "Epoch 237: Train: 0.5482, Validation: 0.537. Test: 0.5352, Loss: 0.9165\n",
            "Epoch 238: Train: 0.5426, Validation: 0.533. Test: 0.5312, Loss: 0.9174\n",
            "Epoch 239: Train: 0.5483, Validation: 0.5347. Test: 0.5279, Loss: 0.9198\n",
            "Epoch 240: Train: 0.538, Validation: 0.523. Test: 0.5302, Loss: 0.9222\n",
            "Epoch 241: Train: 0.5442, Validation: 0.5377. Test: 0.5259, Loss: 0.929\n",
            "Epoch 242: Train: 0.5471, Validation: 0.5324. Test: 0.5339, Loss: 0.9252\n",
            "Epoch 243: Train: 0.5434, Validation: 0.53. Test: 0.5322, Loss: 0.9181\n",
            "Epoch 244: Train: 0.543, Validation: 0.5414. Test: 0.5289, Loss: 0.9198\n",
            "Epoch 245: Train: 0.5341, Validation: 0.5217. Test: 0.5255, Loss: 0.9274\n",
            "Epoch 246: Train: 0.5454, Validation: 0.5327. Test: 0.5219, Loss: 0.9358\n",
            "Epoch 247: Train: 0.5479, Validation: 0.5317. Test: 0.5262, Loss: 0.924\n",
            "Epoch 248: Train: 0.5383, Validation: 0.522. Test: 0.5322, Loss: 0.9195\n",
            "Epoch 249: Train: 0.5469, Validation: 0.5367. Test: 0.5299, Loss: 0.9277\n",
            "Epoch 250: Train: 0.5483, Validation: 0.5367. Test: 0.5309, Loss: 0.9209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlsPif913e_S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}