{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "restaurants_gnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnzHs-cvhVEi",
        "outputId": "5717f270-efda-400e-8c3e-7726445c7d32"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qvEAMNSiCTc"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.nn import SAGEConv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWmejSI5inim",
        "outputId": "a44fddb7-ba16-4a7f-c4dc-1ede83f52e28"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGp07hICiCYy"
      },
      "source": [
        "# Read NetworkX graph of restaurants\n",
        "G = nx.read_gpickle(\"./drive/MyDrive/Colab Notebooks/restaurants.gpickle\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi-4KYdpn1ce",
        "outputId": "18dd3e24-3be7-42e1-9264-8518438bac1d"
      },
      "source": [
        "G.number_of_nodes()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9pIILdRn3qV",
        "outputId": "2ddb4b7f-9a8b-42b6-cedd-e65053226c0f"
      },
      "source": [
        "G.number_of_edges()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "491464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbGQz6NixeH"
      },
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, args):\n",
        "        super(GNN, self).__init__()\n",
        "        self.num_layers = args[\"num_layers\"]\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(input_size, hidden_size))\n",
        "        for l in range(self.num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_size, hidden_size))\n",
        "        self.post_mp = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.node_feature, data.edge_index, data.batch\n",
        "\n",
        "        for i in range(len(self.convs) - 1):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.leaky_relu(x)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXuVFQCCixiA"
      },
      "source": [
        "def train(train_loader, val_loader, test_loader, args, num_node_features, num_classes,\n",
        "          device=\"cpu\"):\n",
        "    model = GNN(num_node_features, args['hidden_size'], num_classes, args).to(device)\n",
        "    print(model)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(args['epochs']):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.node_label\n",
        "            loss = model.loss(pred[batch.node_label_index], label)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc = round(test(train_loader, model, device), 4)\n",
        "        val_acc = round(test(val_loader, model, device), 4)\n",
        "        test_acc = round(test(test_loader, model, device), 4)\n",
        "        print(f\"Epoch {epoch + 1}: Train: {train_acc}, Validation: {val_acc}. Test: {test_acc}, Loss: {round(total_loss, 4)}\")\n",
        "\n",
        "def test(loader, model, device='cuda'):\n",
        "    model.eval()\n",
        "    for batch in loader:\n",
        "        batch.to(device)\n",
        "        logits = model(batch)\n",
        "        pred = logits[batch.node_label_index].max(1)[1]\n",
        "        acc = pred.eq(batch.node_label).sum().item()\n",
        "        total = batch.node_label_index.shape[0]\n",
        "        acc /= total\n",
        "    return acc"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-cyNHzf0Tt1",
        "outputId": "ceb27282-e025-42c6-f279-da59c1ed63e2"
      },
      "source": [
        "args = {\n",
        "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"hidden_size\" : 128,\n",
        "    \"epochs\" : 100,\n",
        "    \"lr\" : 0.01,\n",
        "    \"num_layers\": 5,\n",
        "}\n",
        "\n",
        "H = Graph(G)\n",
        "dataset = GraphDataset(graphs=[H], task='node')\n",
        "\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "print(dataset_train, dataset_val, dataset_test)\n",
        "\n",
        "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(), batch_size=1)\n",
        "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(), batch_size=1)\n",
        "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(), batch_size=1)\n",
        "\n",
        "num_node_features = dataset_train.num_node_features\n",
        "num_classes = dataset_train.num_node_labels\n",
        "\n",
        "train(train_loader, val_loader,test_loader, args, num_node_features, num_classes, args[\"device\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GraphDataset(1) GraphDataset(1) GraphDataset(1)\n",
            "GNN(\n",
            "  (convs): ModuleList(\n",
            "    (0): SAGEConv(2, 128)\n",
            "    (1): SAGEConv(128, 128)\n",
            "    (2): SAGEConv(128, 128)\n",
            "    (3): SAGEConv(128, 128)\n",
            "    (4): SAGEConv(128, 128)\n",
            "  )\n",
            "  (post_mp): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n",
            "Epoch 1: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 4.8515\n",
            "Epoch 2: Train: 0.2578, Validation: 0.2627. Test: 0.2626, Loss: 3.1253\n",
            "Epoch 3: Train: 0.2501, Validation: 0.25. Test: 0.2472, Loss: 5.0447\n",
            "Epoch 4: Train: 0.2501, Validation: 0.25. Test: 0.2472, Loss: 3.4888\n",
            "Epoch 5: Train: 0.2607, Validation: 0.2623. Test: 0.2623, Loss: 2.4435\n",
            "Epoch 6: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 3.1811\n",
            "Epoch 7: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 4.8266\n",
            "Epoch 8: Train: 0.4981, Validation: 0.4977. Test: 0.5018, Loss: 2.4932\n",
            "Epoch 9: Train: 0.2501, Validation: 0.25. Test: 0.2472, Loss: 1.2026\n",
            "Epoch 10: Train: 0.2501, Validation: 0.25. Test: 0.2472, Loss: 1.5634\n",
            "Epoch 11: Train: 0.2501, Validation: 0.25. Test: 0.2472, Loss: 1.57\n",
            "Epoch 12: Train: 0.2501, Validation: 0.25. Test: 0.2472, Loss: 1.6955\n",
            "Epoch 13: Train: 0.2501, Validation: 0.25. Test: 0.2472, Loss: 1.5217\n",
            "Epoch 14: Train: 0.2579, Validation: 0.263. Test: 0.2626, Loss: 1.3673\n",
            "Epoch 15: Train: 0.2579, Validation: 0.2627. Test: 0.2626, Loss: 1.2757\n",
            "Epoch 16: Train: 0.2579, Validation: 0.2627. Test: 0.2626, Loss: 1.1928\n",
            "Epoch 17: Train: 0.4981, Validation: 0.4977. Test: 0.5018, Loss: 1.0964\n",
            "Epoch 18: Train: 0.4981, Validation: 0.4977. Test: 0.5018, Loss: 1.0485\n",
            "Epoch 19: Train: 0.4981, Validation: 0.4977. Test: 0.5018, Loss: 1.1303\n",
            "Epoch 20: Train: 0.4981, Validation: 0.4977. Test: 0.5018, Loss: 1.144\n",
            "Epoch 21: Train: 0.498, Validation: 0.4977. Test: 0.5018, Loss: 1.0763\n",
            "Epoch 22: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 1.0445\n",
            "Epoch 23: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 1.0469\n",
            "Epoch 24: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 1.0587\n",
            "Epoch 25: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 1.0648\n",
            "Epoch 26: Train: 0.5005, Validation: 0.4967. Test: 0.5012, Loss: 1.0607\n",
            "Epoch 27: Train: 0.5008, Validation: 0.497. Test: 0.5012, Loss: 1.0482\n",
            "Epoch 28: Train: 0.501, Validation: 0.4983. Test: 0.5015, Loss: 1.0379\n",
            "Epoch 29: Train: 0.5014, Validation: 0.4983. Test: 0.5015, Loss: 1.0369\n",
            "Epoch 30: Train: 0.5016, Validation: 0.499. Test: 0.5015, Loss: 1.0422\n",
            "Epoch 31: Train: 0.5016, Validation: 0.499. Test: 0.5018, Loss: 1.0454\n",
            "Epoch 32: Train: 0.5015, Validation: 0.4993. Test: 0.5018, Loss: 1.0431\n",
            "Epoch 33: Train: 0.5024, Validation: 0.5007. Test: 0.5018, Loss: 1.0362\n",
            "Epoch 34: Train: 0.5043, Validation: 0.5047. Test: 0.5025, Loss: 1.0276\n",
            "Epoch 35: Train: 0.5032, Validation: 0.5037. Test: 0.5018, Loss: 1.0241\n",
            "Epoch 36: Train: 0.5038, Validation: 0.5053. Test: 0.5038, Loss: 1.0225\n",
            "Epoch 37: Train: 0.5036, Validation: 0.502. Test: 0.5022, Loss: 1.0208\n",
            "Epoch 38: Train: 0.5004, Validation: 0.501. Test: 0.5015, Loss: 1.0154\n",
            "Epoch 39: Train: 0.5059, Validation: 0.5027. Test: 0.5052, Loss: 1.0102\n",
            "Epoch 40: Train: 0.5039, Validation: 0.5017. Test: 0.5015, Loss: 1.0093\n",
            "Epoch 41: Train: 0.5065, Validation: 0.502. Test: 0.5055, Loss: 1.0015\n",
            "Epoch 42: Train: 0.5089, Validation: 0.5083. Test: 0.5072, Loss: 0.9989\n",
            "Epoch 43: Train: 0.5073, Validation: 0.5067. Test: 0.5008, Loss: 1.0006\n",
            "Epoch 44: Train: 0.5106, Validation: 0.5083. Test: 0.5055, Loss: 0.9955\n",
            "Epoch 45: Train: 0.5117, Validation: 0.5077. Test: 0.5065, Loss: 0.9884\n",
            "Epoch 46: Train: 0.5127, Validation: 0.5117. Test: 0.5102, Loss: 0.9844\n",
            "Epoch 47: Train: 0.5165, Validation: 0.5197. Test: 0.5075, Loss: 0.9838\n",
            "Epoch 48: Train: 0.523, Validation: 0.521. Test: 0.5195, Loss: 0.9807\n",
            "Epoch 49: Train: 0.5271, Validation: 0.5277. Test: 0.5215, Loss: 0.9735\n",
            "Epoch 50: Train: 0.5262, Validation: 0.5274. Test: 0.5158, Loss: 0.9714\n",
            "Epoch 51: Train: 0.5288, Validation: 0.5307. Test: 0.5242, Loss: 0.9684\n",
            "Epoch 52: Train: 0.5303, Validation: 0.5314. Test: 0.5215, Loss: 0.9622\n",
            "Epoch 53: Train: 0.5317, Validation: 0.5297. Test: 0.5082, Loss: 0.9619\n",
            "Epoch 54: Train: 0.5292, Validation: 0.5307. Test: 0.5132, Loss: 0.9615\n",
            "Epoch 55: Train: 0.5214, Validation: 0.5217. Test: 0.5008, Loss: 0.9535\n",
            "Epoch 56: Train: 0.5239, Validation: 0.533. Test: 0.5092, Loss: 0.9549\n",
            "Epoch 57: Train: 0.5253, Validation: 0.5227. Test: 0.5048, Loss: 0.9583\n",
            "Epoch 58: Train: 0.5232, Validation: 0.5227. Test: 0.5108, Loss: 0.9499\n",
            "Epoch 59: Train: 0.5334, Validation: 0.5314. Test: 0.5142, Loss: 0.9541\n",
            "Epoch 60: Train: 0.5317, Validation: 0.5294. Test: 0.5122, Loss: 0.9514\n",
            "Epoch 61: Train: 0.5302, Validation: 0.5324. Test: 0.5202, Loss: 0.949\n",
            "Epoch 62: Train: 0.5311, Validation: 0.5347. Test: 0.5158, Loss: 0.9497\n",
            "Epoch 63: Train: 0.533, Validation: 0.5314. Test: 0.5142, Loss: 0.9449\n",
            "Epoch 64: Train: 0.5326, Validation: 0.53. Test: 0.5122, Loss: 0.9467\n",
            "Epoch 65: Train: 0.5308, Validation: 0.5347. Test: 0.5202, Loss: 0.9437\n",
            "Epoch 66: Train: 0.5306, Validation: 0.5347. Test: 0.5202, Loss: 0.9451\n",
            "Epoch 67: Train: 0.5319, Validation: 0.5364. Test: 0.5175, Loss: 0.9414\n",
            "Epoch 68: Train: 0.5302, Validation: 0.5344. Test: 0.5189, Loss: 0.9432\n",
            "Epoch 69: Train: 0.5302, Validation: 0.5347. Test: 0.5202, Loss: 0.9402\n",
            "Epoch 70: Train: 0.5331, Validation: 0.5374. Test: 0.5169, Loss: 0.9427\n",
            "Epoch 71: Train: 0.5374, Validation: 0.5414. Test: 0.5215, Loss: 0.9401\n",
            "Epoch 72: Train: 0.5319, Validation: 0.5367. Test: 0.5229, Loss: 0.9399\n",
            "Epoch 73: Train: 0.5353, Validation: 0.5377. Test: 0.5215, Loss: 0.9408\n",
            "Epoch 74: Train: 0.5403, Validation: 0.5447. Test: 0.5279, Loss: 0.9381\n",
            "Epoch 75: Train: 0.5328, Validation: 0.5357. Test: 0.5219, Loss: 0.9387\n",
            "Epoch 76: Train: 0.5403, Validation: 0.5421. Test: 0.5272, Loss: 0.939\n",
            "Epoch 77: Train: 0.5401, Validation: 0.5444. Test: 0.5292, Loss: 0.9371\n",
            "Epoch 78: Train: 0.5348, Validation: 0.5344. Test: 0.5219, Loss: 0.9372\n",
            "Epoch 79: Train: 0.5408, Validation: 0.5451. Test: 0.5292, Loss: 0.9378\n",
            "Epoch 80: Train: 0.541, Validation: 0.5424. Test: 0.5245, Loss: 0.9367\n",
            "Epoch 81: Train: 0.536, Validation: 0.5347. Test: 0.5222, Loss: 0.9358\n",
            "Epoch 82: Train: 0.5421, Validation: 0.5434. Test: 0.5299, Loss: 0.9366\n",
            "Epoch 83: Train: 0.539, Validation: 0.5417. Test: 0.5212, Loss: 0.9365\n",
            "Epoch 84: Train: 0.54, Validation: 0.5421. Test: 0.5215, Loss: 0.9351\n",
            "Epoch 85: Train: 0.5426, Validation: 0.5434. Test: 0.5305, Loss: 0.9347\n",
            "Epoch 86: Train: 0.5394, Validation: 0.5387. Test: 0.5242, Loss: 0.9352\n",
            "Epoch 87: Train: 0.5423, Validation: 0.5434. Test: 0.5315, Loss: 0.9349\n",
            "Epoch 88: Train: 0.5436, Validation: 0.5444. Test: 0.5282, Loss: 0.9339\n",
            "Epoch 89: Train: 0.5429, Validation: 0.5447. Test: 0.5262, Loss: 0.9332\n",
            "Epoch 90: Train: 0.5433, Validation: 0.5427. Test: 0.5345, Loss: 0.9332\n",
            "Epoch 91: Train: 0.5411, Validation: 0.5404. Test: 0.5259, Loss: 0.9336\n",
            "Epoch 92: Train: 0.5424, Validation: 0.5364. Test: 0.5322, Loss: 0.9338\n",
            "Epoch 93: Train: 0.5404, Validation: 0.5391. Test: 0.5259, Loss: 0.9341\n",
            "Epoch 94: Train: 0.5424, Validation: 0.5364. Test: 0.5332, Loss: 0.9339\n",
            "Epoch 95: Train: 0.5419, Validation: 0.5401. Test: 0.5262, Loss: 0.9335\n",
            "Epoch 96: Train: 0.5446, Validation: 0.5424. Test: 0.5335, Loss: 0.9326\n",
            "Epoch 97: Train: 0.5452, Validation: 0.5441. Test: 0.5299, Loss: 0.9318\n",
            "Epoch 98: Train: 0.5452, Validation: 0.5427. Test: 0.5352, Loss: 0.931\n",
            "Epoch 99: Train: 0.5453, Validation: 0.5417. Test: 0.5352, Loss: 0.9304\n",
            "Epoch 100: Train: 0.5463, Validation: 0.5451. Test: 0.5309, Loss: 0.9301\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}